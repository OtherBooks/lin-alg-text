\begin{frame}{Coordinate vectors relative to a basis $B$}
\begin{definition}
Let $B=\{\boldv_1,\dots ,\boldv_n\}$ be a basis for $V$, and let $\boldv\in V$. According to Theorem \ref{th:basisunique} there is a unique choice of $c_i$ such that 
\[
\boldv=c_1\boldv_1+c_2\boldv_2\cdots +c_n\boldv_n.
\]
We define the {\bf coordinate vector of $\boldv$ relative to the basis $B$} to be the $n$-tuple 
\[
[\boldv]_B:=(c_1,c_2,\dots, c_n)
\]
\end{definition}
\pause
\begin{comments}
\bb
\ii To compute the coordinate vector of $\boldv$ relative to $B=\{\boldv_1,\boldv_2,\dots, \boldv_n\}$ we must \alert{solve} the vector equation $\boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n$ for the coefficients $c_1, c_2,\dots, c_n$. 

For any given example this computation can usually be reduced to setting up and solving a certain system of linear equations. 
\pause
\ii As usual, we will be flexible in terms of how we treat the $n$-vector $(c_1,c_2,\dots, c_n)$. For example, we will often interpret this as a $n\times 1$ column vector.  
\ee
\end{comments} 
\end{frame}
\begin{frame}{Example} 
Let $V=P_2$. 
\bb
\ii Let $B=\{1,x, x^2\}$ be the standard basis of $V$. Computing $[p(x)]_B$ in this case is easy since any polynomial $p(x)=a+bx+cx^2$ is already expressed as a linear combination of the elements of $B$ !! 

That is, we have $[a+bx+cx^2]_B=(a,b,c)$ for any polynomial $p(x)=a+bx+cx^2$. 

\pause Some explicit examples:
\begin{align*}
 [7-\pi x+\sqrt{2}x^2]_B&= (7, -\pi, \sqrt{2})\\
 [1+9x^2]_B&=(1,0,9)
\end{align*}
\pause
\ii Let $B'=\{1+x+x^2, -x+x^2, -1+x^2\}=\{p_1,p_2,p_3\}$ be the nonstandard basis we considered earlier. Now it requires more work to compute $[p(x)]_{B'}$. 
\pause Some explicit examples:
\begin{eqnarray*}
1+x+x^2=1p_1+0p_2+0p_3&\Rightarrow&[1+x+x^2]_{B'}=(1,0,0)\\
-x+x^2=0p_1+1p_2+0p_3&\Rightarrow&[-x+x^2]_{B'}=(0,1,0)\\
x^2=\frac{1}{3}p_1+\frac{1}{3}p_2+\frac{1}{3}p_3&\Rightarrow& [x^2]_{B'}=(\frac{1}{3},\frac{1}{3},\frac{1}{3})
\end{eqnarray*}
\ee
\pause The last example was computed by setting up the polynomial equation $x^2=c_1(1+x+x^2)+c_2(-x+x^2)+c_3(-1+x^2)$ and solving for the $c_i$. Verify for yourself that $c_1=c_2=c_3=1/3$ is the solution!

\end{frame}

\begin{frame}{Coordinate vector map}
\begin{theorem}\label{th:coordinates}
Let $V$ be a vector space, and suppose $B=\{\boldv_1,\boldv_2,\dots, \boldv_n\}$ be a basis for $V$. Define 
\begin{align*}
T\colon V&\rightarrow \R^n\\
\boldv&\mapsto [\boldv]_B.
\end{align*}
\bb
\ii  $T$ is a linear transformation. We will call $T$ the {\bf coordinate vector map with respect to $B$}.

\ii $T(\boldv_1)=T(\boldv_2)$ if and only if $\boldv_1=\boldv_2$: i.e., $T$ is \alert{one-to-one}. 

In particular, $T(\boldv)=\boldzero$ if and only if $\boldv=\boldzero_V$. 

\ii  $\range T=\R^n$: i.e. $T$ is \alert{onto}. 

\ii  A set $S=\{w_1,w_2,\dots, w_r\}\subseteq V$ is linear independent  if and only if $T(S)=\{T(\boldw_1), T(\boldw_2),\dots, T(\boldw_r)\}\subseteq \R^n$ is linearly independent. 

\ii A set $S=\{w_1,w_2,\dots, w_r\}\subseteq V$ spans $V$ if and only if $T(S)=\{T(\boldw_1), T(\boldw_2),\dots, T(\boldw_r)\}$ spans $\R^n$. 
\ee
\end{theorem}
 \begin{proof}
 Exercise.
 \end{proof}
\end{frame}
\begin{frame}
Fix vector space $V$ and basis $B=\{\boldv_1,\boldv_2,\dots, \boldv_n\}$. According to Theorem \ref{th:coordinates}, the coordinate vector map $\boldv\mapsto [\boldv]_B$ is not only \alert{linear}, it is also \alert{1-1 and onto}.  Such functions are called \alert{isomorphisms}.  
\pause
\begin{definition}
Let $V$ and $W$ be vector spaces. An {\bf isomorphism} is a linear transformation $T\colon V\rightarrow W$ that is 1-1 and onto. 

The spaces $V$ and $W$ are called {\bf isomorphic} in this case. 
\end{definition}
\pause
\begin{comment}
`Isomorphism' is derived from the Greek terms `isos', meaning ``equal", and `morphe', meaning ``form". 
\bspace
We do not have time in the course to study isomorphisms in detail. Instead we content ourselves with the following \alert{general principle}:  when $V$ and $W$ are isomorphic, as witnessed by an isomorphism $T\colon V\rightarrow W$, the spaces are identical as far as vector space structure is concerned, even though as sets they may look completely different. 
\bpause
Furthermore, since $T$ is 1-1 and onto, it has an inverse function $T^{-1}\colon W\rightarrow V$. Furthermore, you can show that $T^{-1}$ is also a linear transformation. The two functions $T$ and $T^{-1}$ allow us to to go back and forth between $V$ and $W$, translating statements about $V$ into corresponding statements about $W$ with no information loss. 
\end{comment}
\end{frame}
\begin{frame}
We apply the general principle described in the previous slide to the special case of a coordinate vector isomorphism 
$[\hspace{10pt}]_B\colon V\rightarrow \R^n$. 
 \begin{block}{Coordinate vector technique} Suppose we have chosen a finite basis $B$ for the vector space $V$. 
 Let $[\hspace{10pt}]_B$ denote the corresponding coordinate vector map, and let $[\hspace{10pt}]_B^{-1}$ denote the inverse of this map.
 
 Any linear algebraic questions we want to answer about $V$ (involving span, linear independence, etc.) can be answered as follows: 
\bb
\ii First apply $[\hspace{10pt}]_B$ to all vectors $\boldv$ in question. 
\ii Answer the corresponding question in $\R^n$ about the column vectors $[\boldv]_B$. 
\ii If necessary, translate your results back to the setting of $V$ by applying the inverse $[\hspace{10pt}]_B^{-1}$. 
\ee
\end{block}

\end{frame}
\begin{frame}{Example}
The set 
\[
S=\left \{ A_1=\begin{bmatrix}
2&1\\
0&-2
\end{bmatrix}, A_2=\begin{bmatrix}
1&1\\
1&-1
\end{bmatrix}, 
A_3=\begin{bmatrix}
0&1\\
2&0
\end{bmatrix},
A_4=\begin{bmatrix}
-1&0\\
1&1
\end{bmatrix}\right\}
\]
is a subset of the space $W=\{ A\in M_{22}\colon \tr A=0\}$. (Recall: the trace of a matrix, $\tr A$, is defined as the sum of its diagonal elements. )

Contract $S$ to a basis of $\Span S$, and determine whether $\Span S=W$. Use the coordinate vector map technique! 
\bpause 
We must first pick a basis for $W$. Observe that the general element of of $W$ can be described as 
\[
\begin{bmatrix}a&b\\
c&-a\end{bmatrix}
=a\underset{B_1}{\underbrace{\begin{bmatrix}
1&0\\
0&-1
\end{bmatrix}}}+b\underset{B_2}{\underbrace{\begin{bmatrix}
0&1\\
0&0
\end{bmatrix}}}+c\underset{B_3}{\underbrace{\begin{bmatrix}
0&0\\1&0
\end{bmatrix}}}.\]
It follows easily that $B=\{B_1,B_2, B_3\}$ is a basis for $W$. 
\bpause
Apply $[\hspace{10pt}]_B$ to the elements of the given $S$ to get a corresponding set $S'\subseteq\R^3$:
\[
S'=\left\{ [A_1]_B=(2,1,0), [A_2]_B=(1,1,1), [A_3]_B=(0,1,2), [A_4]_B=(-1,0,1)\right\}
\]
\pause
Now use the ``contract to basis" algorithm on $S'$ to conclude that the set $\{(2,1,0), (1,1,1)\}$ forms a basis for $\Span S'$. Translating back to $W$, we see that $A_1$ and $A_2$ form a basis for $\Span S$, that $\dim\Span S=2$, and hence that $\Span S\ne W$, since $\dim W=3$. 
\end{frame}
