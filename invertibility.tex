\begin{frame}{\ref{s:linear systems}:\ref{ss:invertible}: executive summary}
\alert{Definitions:} none.
\bspace
\alert{Procedures:} deciding when a linear system $A\boldx=\boldb$ is consistent.
\bspace
\alert{Theorems:} invertibility theorem (expanded), $AB$ invertible if and only if $A$ and $B$ are invertible, $A$ is a left inverse of $B$ if and only if $A$ is a right inverse of $B$. 
\end{frame}
\begin{frame}{\ref{s:linear systems}:\ref{ss:invertible}: expanded invertibility theorem}
We add two more equivalent statements of invertibility to our invertibility theorem (IT). 
\begin{theorem}[Expanded invertibility theorem]
Let $A=[a_{ij}]$ be a square $n\times n$ matrix. The following statements are equivalent.
\bb[(a)]
\ii $A$ is invertible.
\ii $A\boldx=\boldzero$ has a unique solution (the trivial one). 
\ii $A$ is row equivalent to $I_n$, the $n\times n$ identity matrix.
\ii $A$ is a product of elementary matrices. 	
\ii $A\boldx=\boldb$ has a {\em unique} solution for every $n\times 1$ column vector $\boldb$. 
\ii $A\boldx=\boldb$ has a solution for every $n\times 1$ column vector $\boldb$. 
\ee
\end{theorem} 
\pause
\begin{proof}
We already know statements (a)-(d) are equivalent. Adding (e) and (f) to this list is left as an exercise. 
\end{proof}
\end{frame}
\begin{frame}\footnotesize
\begin{corollary}
Let $A$ and $B$ be $n\times n$. Then $AB$ is invertible if and only if  $A$ and $B$ are both invertible. 
\end{corollary}
\pause
\begin{proof}
We have already proved the $\Leftarrow$ direction of this equivalnce. 
\bpause
For the $\Rightarrow$ direction, assume $AB$ is invertible and let $C$ be its inverse, so that $C(AB)=(AB)C=I_n$. 

We first prove $B$ is invertible, using equivalent statement (b) of IT; that is we will prove the implication 
$
B\boldx=\boldzero\Rightarrow \boldx=\boldzero$. \pause
\begin{eqnarray*}
B\boldx=\boldzero&\Rightarrow & AB\boldx=\boldzero\\
\pause&\Rightarrow& \boldx=\boldzero \ \text{ (since $AB$ invertible)}.
\end{eqnarray*} 
This proves $B$ is invertible, and hence that $B^{-1}$ exists.
\bpause 
Next we prove $A$ is invertible, by explicitly exhibiting an inverse: namely, $A^{-1}=BC$. 
\bpause Indeed we have 
$
A(BC)=(AB)C=I_n,
$
from above. \pause For the other direction we have 
\begin{eqnarray*}
C(AB)=I&\Leftrightarrow& CA=B^{-1} \ \text{ (mult. both sides on right by $B^{-1}$)}\\
&\Leftrightarrow& (BC)A=I_n \text{ (mult. both sides on left by $B$)}.
\end{eqnarray*}
This proves $A$ and $B$ are both invertible, and completes our proof. 
\end{proof}

\end{frame}
\begin{frame}\footnotesize
\begin{corollary}
Let $A$ and $B$ be $n\times n$. 
\bb[(i)]
\ii $BA=I_n\Rightarrow AB=I_n$.
\ii $AB=I_n\Rightarrow BA=I_n$. 
\ee
\end{corollary}
In other words, to show $B$ is the inverse of $A$ it is enough to show either that it is a left-inverse ($BA=I_n$), or a right-inverse ($AB=I_n$). 
\pause
\begin{proof}
It is enough to prove the first implication: the second then follows by exchanging the roles of $A$ and $B$! 
\bpause
Suppose $BA=I_n$. I will first show that $A$ is invertible. Indeed, suppose $A\boldx=\boldzero$. Then $BA\boldx=\boldzero$. But $BA=I_n$, and $I\boldx=\boldzero$ implies $\boldx=\boldzero$. It follows from statement (b) of IT that $A$ is invertible. 
\bpause 
Now that we know $A^{-1}$ exists we have 
\begin{eqnarray*}
BA=I_n&\Rightarrow & B=A^{-1} \ \text{ (mult. both sides on right by $A^{-1}$)}\\
\pause&\Rightarrow &AB=I_n \ \text{ (mult. both sides on left by $A$)}
\end{eqnarray*}  
\end{proof}
\end{frame}
\begin{frame}{When is a linear system consistent}
Consider a general linear equation 
\[\scriptsize
\eqsys
\]
which we represent as a matrix equation 
\[
\underset{m\times n}{A}\cdot \underset{{n\times 1}}{\boldx}=\underset{m\times 1}{\boldb}.
\]
\alert{Question:} what conditions on $A$ and $\boldb$ guarantee the system is consistent; equivalently, when we can solve the matrix equation above? 
\bpause
\alert{Partial answer:} the preceding discussion gives us a partial answer. If $A$ happens to be an \alert{invertible matrix} (in particular, it must be square!), then the equation is guaranteed to have a (unique) solution. We just solve for $\boldx$ directly in this case: $\boldx=A^{-1}\boldb$.  
\bpause
\alert{Otherwise:} if $A$ is not invertible, then we resort to the theory of Gaussian elimination to answer the question. Take the augmented matrix $[A \vert \boldb]$, row reduce to row echelon form $[U\vert \boldb']$ and reason from there. The example that follows illustrates this. 
\end{frame}
\begin{frame}{Example} 
Find all $b_1,b_2,b_3$ for which the following system is consistent: 
\[
\begin{linsys}{3}
x_1&+&x_2&+&2x_3&=& b_1\\
x_1& & &+&x_3&=&b_2\\
2x_1&+&x_2&+&3x_3&=&b_3
\end{linsys}
\]
\pause
\begin{bsolution}
We consider the augmented matrix $[A \vert \boldb]$ and row reduce 
\[
\begin{bmatrix}[rrr|r]
1&1&2&b_1\\ 1&0&1&b_2\\ 2&1&3&b_3
\end{bmatrix}
\xrightarrow{\text{row red.}} \begin{bmatrix}[rrr|r]
\boxed{1}&1&2&b_1\\ 0&\boxed{1}&1&b_2-b_1\\ 0&0&0&b_3-b_2-b_1
\end{bmatrix}
\]
\pause 
Note: since the $3\times 3$ matrix on the left only 2 leading 1's, we know $A$ is not invertible. 

But of course that doesn't mean that the given system is necessarily inconsistent!  

Indeed, the theory of Gaussian elimination tells us that the system will be consistent if and only if $b_3-b_2-b_1=0$. 
\bpause
We conclude the original system is consistent if and only if $b_3=b_2+b_1$: equivalently, iff 
\[
\boldb=\begin{bmatrix}
b_1\\b_2\\b_1+b_2
\end{bmatrix}.
\]
\end{bsolution}
\end{frame}
\begin{frame}{Diagonal, triangular and symmetric matrices}
Lastly, we officially introduce some special families of square matrices, as well as a corresponding invertibility theorem. The proof of the latter will be done in class. 
 \begin{definition} Let $A=[a_{ij}]$ be $n\times n$. We say  
 \begin{enumerate}[(i)]
\ii $A$ is {\bf diagonal} if $a_{ij}=0$ for all $(i,j)$ with $i\ne j$. (``$A$ is zero off the diagonal.")
\ii $A$ is {\bf upper triangular} if $a_{ij}=0$ for all $(i,j)$ with $j>i$. (``$A$ is zero below the diagonal.")
\ii $A$ is {\bf lower triangular} if $a_{ij}=0$ for all $(i,j)$ with $i>j$. (``$A$ is zero above the diagonal.")
\ii $A$ is {\bf triangular} if $A$ is upper triangular or lower triangular. 
\ii $A$ is {\bf symmetric} if $A^T=A$.  (Equivalently, $a_{ij}=a_{ji}$ for all $1\leq i,j\leq n$.) 
\end{enumerate}
 \end{definition}
 \pause
 \begin{theorem}[Invertibility of triangular matrices]
 Let $A=[a_{ij}]$ be a triangular $n\times n$ matrix. Then $A$ is invertible if and only if $a_{ii}\ne 0$ for all $1\leq i\leq n$.
 
In other words, $A$ is invertible if and only if the diagonal entries of $A$ are all nonzero. 
 \end{theorem}
\end{frame}
