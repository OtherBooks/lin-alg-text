\begin{frame}{Linear transformations and bases}
As the next theorem articulates, a linear transformation $T\colon V\rightarrow W$ is \alert{uniquely determined} by how it acts on the elements of a basis for $V$. 
\begin{theorem}\label{th:Tbasis}
Let $B=\{\boldv_1,\dots,\boldv_n\}$ be a basis for $V$, and let $W$ be any space. 
\bb[(a)]
\ii Given any choice of $n$ elements $\boldw_1,\boldw_2,\dots, \boldw_n\in W$ there is a \alert{unique} linear transformation $T\colon V\rightarrow W$ such that $T(\boldv_i)=\boldw_i$. 
\ii Given linear transformations $T_1,T_2\colon V\rightarrow W$, $T_1=T_2$ if and only if $T_1(\boldv_i)=T_2(\boldv_i)$ for $1\leq i\leq n$.  
\ee
\end{theorem}
\pause
This is an extremely useful theorem as it allows us, given a basis $B$ of $V$, to 
\bb[(i)]
\ii easily define linear transformations simply by declaring where $\boldv_i$ gets sent for each $i$, and 
\pause \ii easily check whether two linear transformations are equal simply by checking that they agree on the basis elements $\boldv_i$. 
\ee
\end{frame}
\begin{frame}
 \begin{proof}[Proof of Theorem \ref{th:Tbasis}]
 \footnotesize
 \pause 
 First observe that given any $\boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n$, if $T$ is a linear transformation satisfying $T(\boldv_i)=\boldw_i$ for all $\boldv_i\in B$, then we must have 
 \begin{align*}
 T(\boldv)&=T(c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n)\\
 &=c_1T(\boldv_1)+c_2T(\boldv_2)+\cdots +c_nT(\boldv_n)\\
 &=c_1\boldw_1+c_2\boldw_2+\cdots +c_n\boldw_n
 \end{align*}
 This shows that there is \alert{at most} one such $T$, and furthermore that $T_1=T_2$ if and only if $T_1(\boldv_i)=T_2(\boldv_i)$ for all $\boldv_i\in B$. 
 \pause
 
 Thus we have proven (b), and the proof of (a) is completed by showing that the formula $T(\boldv)=c_1\boldw_1+c_2\boldw_2+\cdots +c_n\boldw_n$, where $\boldv=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n$, defines a linear transformation satisfying $T(\boldv_i)=\boldw_i$. 
 \pause
 
 The latter property is obvious, since $\boldv_i=1\boldv_i+\sum_{j\ne i}0\boldv_j$ implies $T(\boldv_i)=1\boldw_i+\sum_{i\ne j}0\boldw_j=\boldw_i$.
 \pause
 
 Now let's show $T$ is linear: given $\boldv=\sum_{i=1}^n c_i\boldv_i$ and $\boldv'=\sum_{i=1}^nc_i'\boldv_i$, we have $r\boldv+s\boldv'=\sum_{i=1}^n(rc_i+sc_i')\boldv_i$, in which case 
 \begin{align*}\setlength{\topsep}{0pt}
 T(r\boldv+s\boldv')&=\sum_{i=1}^n(rc_i+sc_i')\boldw_i &\text{(def. of $T$)}\\
 &=r\sum_{i=1}^n c_i\boldw_i+s\sum_{i=1}^nc_i'\boldw_i=rT(\boldv)+sT(\boldv')\ \checkmark &\text{(def. of $T$)}
 \end{align*}
 \end{proof}
\end{frame}
\begin{frame}{Transformations of $\R^n$}
\begin{corollary}
Let $T\colon \R^n\rightarrow \R^m$ be a linear transformation. Then $T=T_A$ for some $\underset{m\times n}{A}$. 

In other words, \alert{any} linear transformation whose domain is $\R^n$ for some $n$ and whose codomain is $\R^m$ for some $m$, is in fact a matrix transformation $T=T_A$. 
\end{corollary}
\pause
\begin{proof}
Our proof includes a \alert{recipe} for computing the matrix $A$ such that $T=T_A$. 

Namely, given a linear $T\colon\R^n\rightarrow \R^m$, let $A$ be the matrix whose $j$-th column is $\boldc_j=T(\bolde_j)$, where $\bolde_j$ is the $j$-th element of the standard basis of $\R^n$. 

I claim $T=T_A$. 
\bpause
According to the Theorem \ref{th:Tbasis}, I only need to show that $T$ and $T_A$ agree on a basis for $\R^n$. Let's take the standard basis $B=\{\bolde_1,\dots,\bolde_n\}$. 

\pause Then 
\begin{align*}
T_A(\bolde_j)&=A\bolde_j &\text{(def. of $T_A$) }\\
&=\boldc_j &\text{(column expansion)}\\
&=T(\bolde_j) &\text{(def. of $A$)}
\end{align*}
We've shown that $T_A(\bolde_j)=T(\bolde_j)$ for all $\bolde_j\in B$. Theorem \ref{th:Tbasis} implies $T=T_A$. 
\end{proof}
\end{frame}
\begin{frame}{Linear transformations of $\R^n$}
 Once we know a given function $T\colon \R^n\rightarrow\R^m$ is linear, the recipe provided in the previous proof allows us to quickly compute the matrix $\underset{m\times n}{A}$ such that $T=T_A$: simply set the $j$-th column of $A$ equal to $T(\bolde_j)$. 
 \pause
 \begin{example}
 Consider again rotation by $\theta$, $T_\theta\colon \R^2\rightarrow \R^2$, where $\theta$ is a fixed angle. 
 
 Once we know $T_\theta$ is linear (we do now, but this wasn't obvious), then we easily come up with the matrix $A_\theta$ that defines $T_\theta$. 
 
 \pause
 The \alert{first column} of $A_\theta$ is 
 \[
 T_\theta(\bolde_1)=T_\theta (\cos(0),\sin(0))=(\cos(\theta),\sin(\theta)).\] 
 \pause
 The \alert{second column} of $A_\theta$ is 
 \[
 T_\theta(\bolde_2)=T_\theta (\cos(\pi/2),\sin(\pi/2))=(\cos(\pi/2+\theta),\sin(\pi/2+\theta))=(-\sin(\theta), \cos(\theta)).
 \] 
 Thus $A_\theta=\begin{bmatrix}
 \cos\theta&-\sin\theta \\
 \sin\theta&\cos\theta
 \end{bmatrix}$. 
 
 \end{example} 
 
\end{frame}
