\begin{frame}{3.7: executive summary}
\alert{Definitions:} null space, column space, row space, rank, nullity.
\bspace
\alert{Procedures:} computing bases for null, column and row spaces; select a basis from a spanning set (in $\R^n$ as well as more exotic spaces). 
\bspace
\alert{Theorems:} rank-nullity theorem; extended invertibility theorem.
\end{frame}
\begin{frame}{3.7: fundamental subspaces of a matrix}
You should consider the next topic as a long example wherein we introduce some new examples of subspaces and compute their dimensions. 
\begin{definition}
Let $A$ be a an $m\times n$ matrix with rows $\boldr_1,\dots, \boldr_m$ and columns $\boldc_1,\dots \boldc_n$. The following subspaces are called the {\bf fundamental subspaces of $A$}. 
\bb
\ii The {\bf null space} of $A$ is defined as 
\[
\NS(A)=\{\boldx\in\R^n\colon A\boldx=\boldzero\}\subset \R^n.
\]
\ii The {\bf row space} of $A$ is defined as 
\[
\RS(A)=\Span(\{\boldr_1,\dots, \boldr_m\})\subset\R^n.
\]
\ii The {\bf column space} of $A$ is defined as 
\[
\CS(A)=\Span(\{\boldc_1,\dots \boldc_n\})\subset\R^m. 
\]
\ee
\end{definition}
\end{frame}
\begin{frame}{Understanding $\CS(A)$}
Let's attempt to get a better understanding of $\CS(A)$ by unwinding the definition a bit. We consider $A$ as a collection of $n$ columns $\boldc_j$, each of which lives in $\R^m$:
\[
A=\begin{bmatrix}
\vert&\vert& \cdots &\vert\\
\boldc_1&\boldc_2&\cdots &\boldc_n\\
\vert&\vert& \cdots &\vert
\end{bmatrix}
\]
\pause
Then we have 
\begin{eqnarray*}
\boldb\in\CS(A)&\Leftrightarrow& \boldb\in\Span(\{\boldc_1,\dots,\boldc_n\})\\
\pause&\Leftrightarrow & \boldb=a_1\boldc_1+a_2\boldc_2+\cdots + a_n\boldc_n \text{ for some $a_i$}\\
\pause&\Leftrightarrow & \boldb=A\begin{bmatrix}
a_1\\ a_2\\ \vdots \\ a_n
\end{bmatrix} \text{ (by column expansion!!)} \\
\pause&\Leftrightarrow & \text{ the equation } A\boldx=\boldb \text{ has a solution}.
\end{eqnarray*}
We conclude:
\[
\boxed{\CS(A)=\{\boldb\in\R^m\colon \text{the equation }Ax=\boldb \text{ has a solution}\}}.
\]
\end{frame}

\begin{frame}{Computing the fundamental spaces}
\footnotesize
Once again Gaussian elimination is the main tool for computing fundamental spaces: start with $A$, row reduce to $U$, compute the fundamental spaces of $U$. However, there are some subtleties involved. Here is the overall description for how to proceed:
\[
\pause 
\begin{array}{lcl}
\text{Space}&\text{Relation}&\text{How to pick a basis}\\
\hline
\text{Null space}&\NS(A)=\NS(U)&\left(\begin{array}{c}\text{find vector parametrization}\\ \text{of solutions to $U\boldx=\boldzero$} \end{array}\right) \\
\\
\pause\text{Row space}&\RS(A)=\RS(U) &\left(\begin{array}{c}\text{nonzero rows of $U$ }\\ \text{form a basis of $\RS(A)$} \end{array}\right) \\
\\
\pause\text{Column space}&\CS(A)\alert{\ne}\CS(U)&\left(\begin{array}{c}\text{pick columns of $A$ corresponding}\\ \text{to columns of $U$ with leading 1's} \end{array}\right) 
\end{array}
\]
\end{frame}
\begin{frame}
Let's prove some of the previous claims. We will begin by proving the following more general result.
\bspace
\alert{Claim}. If $A$ and $B$ are \alert{row equivalent}, then $\NS(A)=\NS(B)$, $\RS(A)=\RS(B)$, and if a certain subset of the columns of $B$ form a basis of $\CS(B)$, then the same is true of the corresponding columns of $A$ and $\CS(A)$. 
\pause
\begin{proof}
First observe that $A$ is row equivalent to $B$ iff $(E_rE_{r-1}\cdots E_1)A=B$ for some elementary matrices $E_i$ iff $QA=B$ for some invertible $Q$. (The latter ``iff" follows since the $E_i$ are elementary, and since all invertible matrices are products of elementary matrices. )
\pause
So we assume $QA=B$ for some invertible $Q$. 
\bpause 
\alert{Null space}. We have shown in an exercise that $A\boldx=\boldzero$ iff $QA\boldx=\boldzero$ iff $B\boldx=\boldzero$. It follows that $\NS(A)=\NS(B)$. 
\bpause
\alert{Row space}
To show $\RS(A)=\RS(B)$, it is enough to show that $\RS(B)\subseteq \RS(A)$; this is because we can then apply the same reasoning using the fact that $A=Q^{-1}B$, $Q^{-1}$ invertible. \pause The row method tells us that each row of $B=QA$ is a linear combination of the rows of $A$. Thus each row of $B$ lies in $\RS(A)$, the span of the rows of $A$. \pause But then we must have $\RS(B)\subseteq\RS(A)$, since $\RS(B)$ is the ``smallest" subspace containing all rows of $B$ (by properties of span).
\bpause
\alert{Column space} (Sketch). Here one can show that columns $\boldc_{i_1}, \boldc_{i_2},\dots, \boldc_{i_r}$ of $A$ form a basis of $\CS(A)$ iff $Q\boldc_{i_1}, Q\boldc_{i_2},\dots, Q\boldc_{i_r}$ form a basis of $\CS(B)$. By the column method the vectors $Q\boldc_{i_j}$ are precisely the corresponding columns of $B=QA$. 
\end{proof}

\end{frame}
\begin{frame}{Example}
\footnotesize
Let's compute bases/dimensions for the fundamental spaces of \\ 
$A=\begin{bmatrix}[rrrr]
2&2&4&2\\
6&6&11&5\\
-4&-4&-7&-3
\end{bmatrix}
$.\\
\pause $A$ reduces to the matrix 
$U=\begin{bmatrix}[rrrr]
1&1&2&1\\
0&0&1&1\\
0&0&0&0
\end{bmatrix}
$
\bpause
First compute 
\begin{eqnarray*}
\NS(A)=\NS(U)&=&\{(s-r,r,-s,s)\colon r, s\in\R\}\\
\pause&=&\{s(1,0,-1,1)+r(-1,1,0,0)\colon r,s\in\R\}\\
\pause&=&\Span(\{(1,0,-1,1),(-1,1,0,0)\}.\end{eqnarray*} 
Thus we see that $B=\{(-1,1,0,0), (1,0,-1,1)\}$ is a basis for $\NS(A)$. We conclude $\dim(\NS(A))=2$. 
\bpause
Next we have $\RS(A)=\RS(U)$, and we can take the nonzero rows of $U$ as a basis for this. Thus $B'=\{(1,1,2,1),(0,0,1,1)\}$ is a basis for $\RS(A)$, and this shows that $\dim(\RS(A))=2$. 
\bpause Finally, the columns with leading 1's form a basis of $\CS(U)$: that is, the first and third columns. It follows that the first and third columns of $A$ form a basis for $\CS(A)$. This gives us $B''=\{(2,6,-4), (4,11,-7)\}$ as a basis for $\CS(A)$, and this shows that  $\dim(\CS(A))=2$. 
\end{frame}
\begin{frame}{Extending/contracting to bases}
Our procedures can be used to extend or contract a given set to a to basis of $\R^n$.
\bpause
Let $\boldv_1,\boldv_2, \dots, \boldv_r\in \R^n$. 
\bb
\ii {\em Contracting to basis}. If the $\boldv_i$ span $\R^n$, then to contract to a basis proceed as follows:
\bb
\ii Build the matrix $A$ whose columns are $\boldv_1, \boldv_2, \dots, \boldv_r$. 
\ii Apply the column space algorithm to $A$. 
\ee
\ii {\em Extending to basis}. If the $\boldv_i$ are independent, then to extend to a basis of $\R^n$ proceed as follows:
\bb
\ii Build the matrix $A$ whose columns (in order) are $\boldv_1,\boldv_2, \dots, \boldv_r, \bolde_1,\bolde_2,\dots, \bolde_n$.
\ii Apply the column space algorithm to $A$. 
\ee

\ee
\end{frame}
\begin{frame}{Example}
\footnotesize
Let $W=\Span\{\boldv_1,\boldv_2, \boldv_3, \boldv_4, \boldv_5\}$ where 
\[
\begin{array}{ll}
\boldv_1=(1,-1,5,2)&\boldv_2=(-2,3,1,0)\\
\boldv_3=(4,-5,9,4)&\boldv_4=(0,4,2,-3)\\
\boldv_5=(-7,18,2,-8)
\end{array}
\]
Find a subset of the $\boldv_i$ that yields a basis of $W$. 
\bpause 
We know the $\boldv_i$ span $W$, so to get a basis we need to ``throw out the redundant ones". To figure out which ones need to go, we create a matrix $A$ by setting the $\boldv_i$ as  its \alert{columns} (not its rows)! The procedure for finding a basis for $\CS(A)$ then gives us a subset of these columns that forms a basis.
\bpause 
$A=\begin{bmatrix}[rrrrr]
1&-2&4&0&-7\\
-1&3&-5&4&18\\
5&1&9&2&2\\ 
2&0&4&-3&-8
\end{bmatrix}
\xrightarrow[]{\text{row red.}}
U=
\begin{bmatrix}[rrrrr]
\boxed{1}&0&2&0&-1\\
0&\boxed{1}&-1&0&3\\
0&0&0&\boxed{1}&2\\
0&0&0&0&0
\end{bmatrix}
$
\bpause
It follows that $\{\boldv_1,\boldv_2, \boldv_4\}$ is a basis for $W$. 
\end{frame}
\begin{frame}{Example}
Extend the set $S=\{ (1,1,1,1), (-2,-2,3,3)\}$ to a basis of $\R^4$. 
\bpause
Take the matrix 
\[
A=\begin{bmatrix}
1&-2&1&0&0&0\\
1&-2&0&1&0&0\\
1&3&0&0&1&0\\
1&3&0&0&0&1
\end{bmatrix}
\]
This matrix row reduces to 
\[
U=\begin{bmatrix}
\boxed{1}&-2&1&0&0&0\\
0&\boxed{1}&-1/5&0&1/5&0\\
0&0&\boxed{1}&-1&0&0\\
0&0&0&0&\boxed{1}&-1
\end{bmatrix}
\]
\pause
It follows that the first, second, third and fifth column of $A$ form a basis of $\CS(A)=\R^4$. Thus 
\[
B=\{(1,1,1,1), (-2,-2,3,3), (1,0,0,0), (0,0,1,0)\}
\]
is a basis of $\R^4$ extending $S$. 

\end{frame}
\begin{frame}{Working in other spaces $V$}
These algorithms gives us means of computing bases for subspaces inside $\R^n$. What about more exotic vector spaces, like $P_n$, $M_{mn}$, etc.? 
\bspace 
The trick when working in one of these more abstract spaces is to first pick a basis $B$ of $V$ (oftentimes the ``standard" one), and use the coordinate vector operator 
\[
\boldv\mapsto [\boldv]_B\in\R^n
\]
to translate the problem into a question about vectors in $\R^n$. 
\bpause Use one of the previous algorithms to solve the corresponding problem in $\R^n$, then translate your results back in terms of the original vector space $V$. 
\bspace The following examples illustrate this technique. 
\end{frame}
\begin{frame}{Example}
Let $W=\Span\{p_1,p_2, p_3, p_4, p_5\}\subset P_3$ where 
\[
\begin{array}{ll}
p_1=1-x+5x^2+2x^3&p_2=-2+3x+x^2\\
p_3=4-5x+9x^2+4x^3&p_4=4x+2x^2-3x^3\\
p_5=-7+18x+2x^2-8x^3
\end{array}
\]
Find a subset of the $p_i$ that yields a basis of $W$. 
\bpause 
Let $B=\{1,x,x^2, x^3\}$ be the standard basis for $P_3$. Apply $[\hspace{10pt}]_B$ to each of the $p_i$ to get the exact same vectors $\boldv_i$ from the previous example.
\bpause As we saw in that example, our column space algorithm showed that $\boldv_1, \boldv_2$ and $\boldv_4$ formed a basis for the span of the $\boldv_i$. 
\bpause 
It follows that $\{p_1, p_2, p_4\}$ is a basis for $W$ inside $P_3$.
 
\end{frame}
\begin{frame}{Example}
Let $A_1=\begin{bmatrix}[rr]
1&1\\-2&0
\end{bmatrix}
$,
$A_2=\begin{bmatrix}[rr]
1&-1\\-1&1
\end{bmatrix}
$,
$A_3=\begin{bmatrix}[rr]
1&1\\1&-3
\end{bmatrix}
$, and let $W=\Span(\{A_1,A_2,A_3\})$. 

Decide whether $A=\begin{bmatrix}
1&0\\-1&0
\end{bmatrix}
$
is in $W$. 
\bpause
Let $B$ be the standard basis of $M_{22}$. Translate everything to $\R^4$ using $(\underline{\hspace{.2cm}})_B$. 
\bspace We are then asking whether $\boldv=(1,0,1,0)$ is in the span of $\boldv_1=(1,1,-2,0)$, $\boldv_2=(1,-1,-1,1)$, $\boldv_3=(1,1,1,-3)$. This is equivalent to whether the matrix equation 
\[
\begin{bmatrix}[rrr]
1&1&1\\
1&-1&1\\
-2&-1&1\\
0&1&-3
\end{bmatrix}
\boldx
=\begin{bmatrix}[r]
1\\ 0\\-1 \\0 
\end{bmatrix}
\] 
has a solution. 

GE shows that in fact we can solve this. (Do it!) This means $\boldv$ is a linear combination of the $\boldv_i$, and thus that $A$ is a linear combination of the $A_i$. 

We conclude $A\in W$. 
\end{frame}
\begin{frame}%{Rank and nullity}
\footnotesize
\begin{definition}[Definition of rank and nullity]
Let $A$ be $m\times n$ and suppose $A$ row reduces to the matrix $U$ in row echelon form. We define 
\\
$\rank(A)=\#\text{(leading 1's in $U$)}=\#\text{nonzero rows of $U$}$
\\
$\nullity(A)=\dim(\NS(A))$.

\end{definition}
\pause
\alert{Comment 1}. Even though the matrix $U$ is not unique in relation to $A$, we have mentioned before that the number of leading 1's (and the columns in which they occur) \alert{is} unique. Thus the notion of rank is well-defined. 
\bpause
\alert{Comment 2}. We have seen that $\dim(\RS(A))=\#\text{nonzero rows of $U$}$, and that $\dim(\CS(A))=\#\text{leading 1's in $U$}$. It follows that
\[
\dim(\RS(A))=\dim(\CS(A))=\rank(A).
\]
\pause
\alert{Comment 3.} \\
Since $\RS(A)\subset \R^n$, $\dim(\RS(A))\leq n$. \\ 
Since $\CS(A)\subset\R^m$, $\dim(\CS(A))\leq m$. \\
It follows that $\rank(A))\leq \min\{m,n\}$.
\bpause
\alert{Comment 4.} Let $r=\#$(leading variables) and $s=\#$(free variables) in the linear system $A\boldx=\boldzero$. \\
Then $r=\#\text{(leading 1's in $U$)}=\rank(A)$. 
\\
Furthermore, our algorithm for $\NS(A)$ gives us one basis element for each free variable; thus $\dim(\NS(A))=\nullity(A)=s$. We conclude that 
\[
n=r+s=\rank(A)+\nullity(A).
\]
\end{frame}
\begin{frame}{Rank-nullity theorem for matrices}
We combine the previous observations into a theorem. 
\begin{theorem}[Rank-nullity theorem]
Let $A$ be $m\times {\color{red} n}$. Then:
\bb
\ii $\dim(\CS(A))=\dim(\RS(A))=\rank(A)\leq\min\{m,n\}$.
\ii $\nullity(A)+\rank(A)={\color{red} n}$.
\ee
\end{theorem}

\end{frame}
\begin{frame}
Since the rank nullity theorem (R-N) tells us that 
\[
\dim(\NS(A))+\dim(\CS(A))=n,
\]
we see that the {\color{red} bigger} the null space (dimension-wise), the {\color{blue} smaller} the column space, and vice versa. 

Let's see this numerically in action. 
\bb
\pause\ii Suppose $A$ is $3\times 5$. Prove that $\NS(A))$ is nontrivial. 
\bpause
R-N tells us that $\dim(\NS(A))=5-\rank(A)=5-\dim(\CS(A))$. Since $\CS(A)\subset\R^3$, it follows that $\dim(\CS(A))\leq 3$. 
\bpause 
But then $\dim(\NS(A))\geq 5-3=2$. In particular, $\NS(A)$ is nontrivial. 
\pause\ii Suppose $A$ is $5\times 7$ and $\dim(\NS(A))=4$. Show that there is a $\boldb\in\R^7$ for which $A\boldx=\boldb$ has no solution. 
\bpause 
R-N tells us that $\dim(\CS(A))=7-4=3$. Since $\CS(A)\subset\R^7$ and $\dim(\R^7)=7$, it follows that $\CS(A)\ne \R^7$. 
\bpause
Thus there is a $\boldb\in\R^7$ such that $\boldb\notin\CS(A)$. By the previous theorem, we cannot solve $A\boldx=\boldb$. 
\ee
\end{frame}

%\begin{corollary}[Corollary of R-N]
%Let $A$ be $m\times n$. 
%\bb[(i)]
%\pause\ii If $m>n$, then there is a $\boldb\in\R^m$ such that $A\boldx=\boldb$ has no solution (i.e., is inconsistent). 
%\ii If $m<n$, then there is a nontrivial $\boldx\ne\boldzero_n\in\R^n$ such that $A\boldx=\boldzero_m$. 
%\ee
%\pause
%\begin{proof}
%\ \\
%(i) If $m>n$, then $\dim(\CS(A))=\dim(\RS(A))\leq n<m$. Thus $\CS(A)\ne\R^m$, which means there is a $\boldb$ such that $A\boldx=\boldb$ has no solution. 
%\bpause
%(ii) If $m<n$, then $\dim(\CS(A))\leq m<n$. By the R-N theorem we have 
%\[
%\dim(\NS(A))=n-\dim(\CS(A))>n-n=0.
%\]
%Thus $\NS(A)$ is a nontrivial space, which means there is a $\boldx\ne\boldzero_n$ with $A\boldx=\boldzero_m$. 
%\end{proof}
%\end{corollary}
%\end{frame}
\begin{frame}{Invertibility Theorem}
\alert{Suppose $A$ is a square $n\times n$ matrix} . Recall that $A$ is invertible if and only if $A\boldx=\boldzero$ has a unique solution. This is equivalent to $\NS(A)=\{\boldzero\}$. Thus we have: 
\[
A \text{ invertible } \Leftrightarrow \NS(A)=\{\boldzero\}\Leftrightarrow \nullity(A)=0.
\]
\pause Similarly, we have $A$ invertible if and only if $A\boldx=\boldb$ has a solution \alert{for all} $\boldb\in\R^n$. As we saw earlier, $\CS(A)=\{\boldb\in\R^n\colon A\boldx=\boldb \text{ has a solution}\}$. Thus we see that 
\[
A \text{ invertible}\Leftrightarrow \CS(A)=\R^n \Leftrightarrow \rank(A)=n \Leftrightarrow \RS(A)=\R^n,
\]
where the last equivalence follows since $\RS(A)\subset\R^n$ and $\dim(\RS(A))=n$. 
\bpause 
Looks like we have quite a few new statements to add to our Invertibility Theorem! 
\end{frame}
\begin{frame}
\begin{theorem}[Invertibility theorem]
Let $A$ be $n\times n$. The following statements are equivalent. 
\bb[(a)]
\ii $A$ is invertible.
\ii $A\boldx=\boldzero$ has a unique solution (the trivial one). 
\ii $A$ is row equivalent to $I_n$, the $n\times n$ identity matrix.
\ii $A$ is a product of elementary matrices. 	
\ii $A\boldx=\boldb$ has a solution for every $n\times 1$ column vector $\boldb$. 
\ii $A\boldx=\boldb$ has a {\em unique} solution for every $n\times 1$ column vector $\boldb$. 
\ii $\det(A)\ne 0$.
\ii $\NS(A)=\{\boldzero\}$.
\ii $\nullity(A)=0$.
\ii $\rank(A)=n$. 
\ii $\CS(A)=\R^n$.
\ii $\RS(A)=\R^n$.
\ii The columns of $A$ are linearly independent (or span $\R^n$, or are a basis of $\R^n$).
\ii The rows of $A$ are linearly independent (or span $\R^n$, or are a basis of $\R^n$). 
\ee
\end{theorem}

\end{frame}






