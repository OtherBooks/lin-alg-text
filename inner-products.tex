\begin{frame}{Inner product spaces}
We now define the notion of an inner product $\langle v,w \rangle$ on a vector space $V$. 
\bpause An inner product is an additional layer of structure added to the vector space operations defined on $V$. As with those operations, we define inner products \alert{axiomatically}. 
\bpause
The dot product operation defined on $\R^2$ and $\R^3$ serves as the basic model of an inner product; our axioms simply generalize prominent and useful properties of this particular inner product. 
\bpause
The main virtues of an inner product are:
\bb
\ii it allows us to define notions of orthogonality, distance and angle on $V$;
\ii it allows us to construct \alert{orthonormal} bases of $V$, which are computationally very easy to work with.  
\ee  
\end{frame}
\begin{frame}{Definition of inner product}
\begin{definition}
Let $V$ be a vector space. An {\bf inner product} on $V$ is an operation
\begin{align*}
\langle \ , \rangle \colon &V\times V\rightarrow \R \\
(\boldv_1,\boldv_2)&\mapsto \langle \boldv_1,\boldv_2\rangle
\end{align*}
satisfying the following axions:
\begin{align*}
\langle \boldv,\boldw\rangle&=\langle \boldw,\boldv\rangle &\text{(Symmetry)}\\
\langle c\boldu+d\boldv,\boldw\rangle&=c\langle\boldu,\boldw\rangle+d\langle\boldv,\boldw\rangle &\text{(Linearity in first variable)}\\
\langle \boldv,\boldv\rangle\geq 0\text{ and }\langle \boldv,\boldv\rangle &=0 \text{ iff  }\boldv=\boldzero &\text{(Positivity)}\\
\end{align*}
A vector space $V$ along with a choice of inner product $\langle \ , \rangle$ is called an {\bf inner product space}. 
\end{definition}
\end{frame}
\begin{frame}{Example: the dot product on $\R^n$}
\begin{definition}
Given $\boldv=(v_1,\dots,v_n), \boldw=(w_1,\dots,w_n)\in\R^n$, we define the {\bf dot product} as 
\[
\boldv\cdot\boldw=v_1w_1+v_2w_2+\cdots +v_nw_n.
\]
\end{definition}
It is easy to see that the dot product satisfies the axioms of an inner product on $\R^n$
\bpause 
In fact, if we identify $n$-tuples with column vectors (or row vectors), then we can express the dot product as a certain matrix multiplication, in which case the inner product axioms follow simply from properties of matrix multiplication!  
\bpause
\alert{Columns:} think of $\boldv$ and $\boldw$ as $n\times 1$ column vectors. Then 
\[
\boldv\cdot\boldw=\boldv^T\boldw.
\]
\bpause
\alert{Rows:} think of $\boldv$ and $\boldw$ as $1\times n$ row vectors. Then 
\[
\boldv\cdot\boldw=\boldv\boldw^T
\]

\end{frame}
\begin{frame}{Dot product method of matrix multiplication}
The last observations also give us yet another way of looking at matrix multiplication! 
\bspace
Take $A=[a_{ij}]_{m\times n}$ and $B=[b_{ij}]_{n\times r}$. Think of $A=\begin{bmatrix} -\bolda_1-\\ \vdots \\ -\bolda_m-\end{bmatrix}$ as a collection of $m$ {\color{blue}row} vectors in $\R^n$, and $B=\begin{bmatrix}\vert & & \vert\\ \boldb_1&\cdots &\boldb_r\\ \vert & & \vert\end{bmatrix}$ as a collection of $r$ {\color{red} column} vectors in $\R^n$. 
\bpause 
Set $AB=C=[c_{ij}]_{m\times r}$. It follows directly from the original definition of matrix multiplication that 
\[
c_{ij}=\bolda_i\cdot\boldb_j.
\] 
That is, the $ij$-th entry of $AB$ is simply the dot product of the $i$-th row of $A$ and the $j$-th column of $B$!
\end{frame}

\begin{frame}{Inner products on $\R^n$}
Let $\boldx=(x_1,x_2,\dots ,x_n)$ and $y=(y_1,y_2,\dots y_n)$ throughout. We consider $\boldx$ and $\boldy$ as columns by default. 

\alert{Example 1}. The \alert{dot product} $\langle\boldx,\boldy\rangle:=x_1y_1+x_2y_2+\cdots x_ny_n$ is an inner product on $\R^n$. 
\bpause
\alert{Example 2}. For any choice of \alert{positive constants} $c_1,c_2,\dots c_n>0$, the \alert{weighted dot product} $\langle \boldx,\boldy\rangle:=c_1x_1y_1+c_2x_2y_2+\cdots +c_nx_ny_n$ is an inner product on $\R^n$. 
\bpause
\alert{Example 3}. (Why we need $c_i>0$). The operation $\langle\boldx,\boldy\rangle=2x_1y_1+(-1)x_2y_2$ is NOT an inner product on $\R^2$ as $\langle (1,\sqrt{2}), (1,\sqrt{2})\rangle=2-2=0$, in violation of positivity. 
\bpause
\alert{Example 4}. Looking forward a bit, it turns out that Examples 1 and 2 are cases of the following more general construction. 

Let $A$ be any symmetric $n\times n$ matrix, all of whose eigenvalues are \alert{positive}. Then the operation 
\[
\langle \boldx, \boldy\rangle:=\boldx^TA\boldy
\]
defines an inner product on $\R^n$. 

Example 1 is the case where $A=I$. 

Example 2 is the case where $A=\begin{bmatrix}c_1&0&0&\dots\\
0&c_2&0&\dots \\
\vdots &\\
0&0&\dots&c_n
\end{bmatrix}$

\end{frame}
\begin{frame}{Inner products on $P_n$}
Throughout we let $p(x)=a_nx^n+\cdots +a_1x+a_0$ and $q(x)=b_nx^n+\cdots +b_1x+b_0$. 

\alert{Example 1}. The operation $\langle p(x),q(x)\rangle :=a_0b_0+a_1b_1+\cdots +a_nb_n$ defines an inner product on $P_n$. (This is just the dot product in disguise!) 
\bpause
\alert{Example 2}. Fix any distinct constants $c_0,c_1,\dots ,c_n$. Then the operation 
$\langle p(x), q(x)\rangle=p(c_0)q(c_0)+p(c_1)q(c_1)+\cdots +p(c_n)q(c_n)$ defines an inner product, called an \alert{evaluation inner product}.  
\pause
\begin{proof}
It is fairly clear that the evaluation product is symmetric and linear in each variable.

Consider positivity. We have $\langle p(x), p(x)\rangle:=p(c_0)^2+p(c_1)^2+\cdots +p(c_n)^2\geq 0$, since each term is a square.  

Furthermore we see this sum is equal to 0 iff $p(c_i)=0$ for all $i$. But this is the case iff $p(x)=\boldzero$ is the zero polynomial, as no other polynomial in $P_n$ can have $n+1$ distinct roots!  
\end{proof}
\end{frame}
\begin{frame}{Standard inner product on $M_{mn}$}
Given $A, B\in M_{mn}$, we define $\langle A, B\rangle=\tr (A^TB)$. Recall: the trace $\tr(C)$ of a matrix is the sum of its diagonal entries. 

It is an interesting exercise to show the operation thus defined does indeed satisfy three axioms. 
\bpause
(i) The fact that $\langle A,B\rangle=\langle B,A \rangle$ follows from the fact that $\tr(A^TB)=\tr(B^TA)$. Can you prove the latter? Hint: what is the relation between $A^TB$ and $B^TA$? 
\bpause
(ii) Bilinearity follows easily from \alert{three} different distributive properties: (a) for matrix multiplication, (b) for taking transposes, and (c) for taking the trace.
\bpause
(iii) Positivity is the most challenging. The most direct way of proving that this property holds is to let $A=[a_{ij}]$ and actually compute a formula for $\tr(A^TA)$ in terms of the $a_{ij}$. I leave it to you.   
\end{frame}
\begin{frame}{Standard inner product on function spaces}
Let $V=C([a,b])$ where $[a,b]$ is some fixed interval. 

The \alert{standard inner product} on $C([a,b])$ is defined via an integral:
\[
\langle f(x), g(x)\rangle:=\int_a^b f(x)g(x) \, dx.
\]
(Note that continuity of $f$ and $g$ ensures this integral exists and is finite!) 
\pause
\begin{proof}
{\bf Symmetry}: $\langle f,g\rangle=\int_a^b fg\, dx=\int_a^b gf\, dx=\langle g,f\rangle$

{\bf Linearity}: $\langle cf+dg,h\rangle=\int_a^b(cf(x)+dg(x))h(x)\, dx=c\int_a^bf(x)h(x)\, dx+d\int_a^bg(x)h(x)\, dx=c\langle f,h\rangle + d\langle g,h \rangle$
\pause

{\bf Positivity}: we have $\langle f, f\rangle=\int_a^b f^2(x)\, dx$. 

We have $f^2(x)\geq 0$ for all $x$; i.e., $f^2$ is nonnegative. It follows from some basic calculus that the integral of $f^2$ is always nonnegative. Furthermore, since $f^2$ is also \alert{continuous} the integral is 0 iff $f^2=0$ iff $f=0$.  This proves positivity. 
\end{proof}

\end{frame}
\begin{frame}{Further properties of inner products}
The following properties follow formally from the defining axioms of an inner product space. 
\begin{align*}
\langle\boldu,c\boldv+d\boldw\rangle&=c\langle\boldu,\boldv\rangle+d\langle\boldu,\boldw\rangle &\text{(Linearity in second variable)} \\
\langle\boldzero,\boldv\rangle&=\langle\boldv,\boldzero\rangle=0
\end{align*} 
\pause 
The first property follows easily from linearity in the first variable and symmetry. 

Note: the fact that the inner product is linear in both variables is an important property that we call \alert{bilinearity}. 
\bpause
To prove the second property it is enough by symmetry to prove $\langle\boldzero,\boldv\rangle=0$. (Note the two different types of zero here!)

\pause
We have 
\begin{align*}
\langle\boldzero,\boldv\rangle&=\langle 0\cdot\boldzero,\boldv\rangle &\text{(since $0\cdot\boldzero=\boldzero$)}\\
&=0\langle\boldzero,\boldv\rangle &\text{(linearity in first variable)}\\
&=0
\end{align*} 
\end{frame}
\begin{frame}{Orthogonality, norm, distance, angle}
Given an inner product space $(V, \langle \ , \rangle)$ we define the following notions. 

\pause
\alert{Orthogonality}: vectors $\boldv$ and $\boldw$ are {\bf orthogonal} if $\langle \boldv,\boldw\rangle=0$. 

\alert{Norm}: the {\bf norm} (or {\bf length}) of a vector $\boldv$ is defined as 
\[
\norm{\boldv}:=\sqrt{\langle\boldv,\boldv\rangle}.
\]
A {\bf unit vector} is a vector $\boldv$ with $\norm{\boldv}=1$. Given any vector $\boldv$, one can show that $\boldu=(\frac{1}{\norm{\boldv}})\boldv$ (or $\frac{\boldv}{\norm{\boldv}}$ by slight abuse of notation) is a unit vector. 

\pause
\alert{Distance:} the {\bf distance} between two vectors $\boldv$ and $\boldw$ is defined as 
\[
d(\boldv,\boldw)=\norm{\boldv-\boldw}.
\]
\pause
\alert{Angle:} the {\bf angle} between two vectors $\boldv$ and $\boldw$ is defined as the unique $0\leq \theta \leq \pi$ satisfying 
\[
\cos(\theta)=\frac{\langle\boldv,\boldw\rangle}{\norm{\boldv}\norm{\boldw}}.
\]
\pause Of course for this to make sense we better have 
\[
\val{\langle\boldv,\boldw\rangle}\leq\norm{\boldv}\norm{\boldw}
\]
This is the content of the Cauchy-Schwarz inequality! 
\end{frame}
\begin{frame}{Cauchy-Schwarz Inequality}
\begin{theorem}[Cauchy-Schwarz inequality]
Let $(V,\langle \ , \rangle)$ be an inner product space. Then for all $\boldv,\boldw\in V$ 
\[
\val{\langle\boldv,\boldw\rangle}\leq\norm{\boldv}\norm{\boldw}.
\]
Furthermore, we have an actual equality above iff $\boldv=c\boldw$ for some $c\in\R$. 
\end{theorem}
\pause
\begin{proof}\scriptsize
Fix any two vectors $\boldv$ and $\boldw$. For any $t\in\R$ we have by positivity 
\[
0\leq \langle t\boldv-\boldw,t\boldv-\boldw\rangle=\langle\boldv,\boldv\rangle t^2-2\langle\boldv,\boldw\rangle t+\langle\boldw,\boldw\rangle=at^2-2bt+c=f(t),
\]
where $a=\langle\boldv,\boldv\rangle=\norm{v}^2$, $b=\langle\boldv,\boldw\rangle$ and $c=\langle\boldw,\boldw\rangle=\norm{w}^2$. 
\bpause
The inequality $f(t)\geq 0$ tells us that the quadratic polynomial $f(t)$ has \alert{at most} one root. This means its discriminant $4b^2-4ac\leq 0$, since otherwise there would be two roots. 
\bpause 
Subbing back in for $a,b,c$ and rearranging yields 
\[
(\langle\boldv,\boldw\rangle)^2\leq \norm{\boldv}^2\norm{\boldw}^2.
\]
Taking square-roots yields the desired inequality. 

\pause
The same reasoning shows that the Cauchy-Schwarz inequality is an actual equality iff $f(t)=0$ for some $t$ iff $0=\langle t\boldv-\boldw,t\boldv-\boldw\rangle$ iff $\boldv=t\boldw$ for some $t$ (by positivity). 
\end{proof}
\end{frame}
\begin{frame}{Triangle Inequalities}
The following so-called triangle inequalities are now just formal consequences of Cauchy-Schwarz.
\begin{theorem}[Triangle Inequalities]
Let $(V, \langle \ , \rangle)$ be any inner product space. Then 
\bb
\ii $\norm{\boldv+\boldw}\leq \norm{\boldv}+\norm{\boldw}$ 
\ii $d(\boldv,\boldw)\leq d(\boldv,\boldu)+d(\boldu,\boldw)$
\ee
\end{theorem}

\end{frame}
\begin{frame}{Choosing your inner product}
Why, given a fixed vector space $V$, would we prefer one inner product definition to another? 

One way of understanding a particular choice of inner product is to ask what its corresponding notion of distance measures. 
\bpause
\alert{Example}. Take $P_n$ with the evaluation inner product at inputs $x=c_0, c_1,\dots, c_n$. Given two polynomials $p(x), q(x)$, the distance between them with respect to this inner product is 
\[
\norm{p(x)-q(x)}=\sqrt{(p(c_0)-q(c_0))^2+(p(c_1)-q(c_1))^2+\cdots +(p(c_n)-q(c_n))^2}.
\]
So in this inner product space the ``distance" between two polynomials is a measure of how different their values are at the inputs $x=c_0,c_1,\dots ,c_n$. This inner product may be useful if you are particularly interested in how a polynomial behaves at this finite list of inputs. 
\bpause
\alert{Example}. Take $C[a,b]$ with the standard inner product $\langle f, g \rangle=\int_a^b f(x)g(x) \ dx$. Here the distance between two functions is defined as $\ds \norm{f-g}=\sqrt{\int_a^b (f(x)-g(x))^2 \ dx}$. In particular, a function $f$ is ``close" to the zero function (i.e. ``is small") if the {\em integral} of $f^2$ is small. This notion is useful in settings where integrals of functions represent quantities we are interested in (e.g. in probability theory, thermodynamics, wave and quantum mechanics).  
\end{frame}