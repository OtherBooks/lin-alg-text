\bb [itemsep=5pt]
\ii Compute the characteristic polynomial, eigenvalues, and eigenspace bases for each of the following matrices.
\bb[itemsep=5pt]
\ii
$A=
\begin{bmatrix}[cc]
2&1\\
1&2
\end{bmatrix}
$

\ii
$A=
\begin{bmatrix}[cc]
2&-3\\
0&2
\end{bmatrix}
$
\ii
$A=
\begin{bmatrix}[cc]
2&0\\
0&2
\end{bmatrix}
$
\ii 
$A=
\begin{bmatrix}[cc]
1&2\\
-2&-1
\end{bmatrix}
$
\ee
\begin{solution}
\noindent (a) 
The characteristic polynomial is
$$
\det(t I - A) =
\begin{vmatrix}[cc]
t - 2&-1\\
-1&t-2
\end{vmatrix}
= (t - 2)^2 - 1 = t^2 -4t +3 = (t - 3)(t - 1) =0
$$
Thus the eigenvalues are $\lambda = 3$ and $\lambda = 1$. 

For $W_3$ we see that $3I-A=
\begin{bmatrix}[rr]
1&-1\\
-1&1
\end{bmatrix}
$
has rank 1, and hence nullity 1. By inspection we see that $\boldv_1=(1,1)$ comprises a basis for $W_3$. 

For $\lambda = 1$, we have $I-A=
\begin{bmatrix}[rr]
-1&-1\\
-1&-1
\end{bmatrix}
$. Again, by inspection we see that $I-A$ has rank 1, hence nullity 1, and that $\boldv_2=(1,-1)$ forms a basis. 
\\
(b) 
$$
\det(t I- A) =
\begin{vmatrix}[cc]
t - 2& 3\\
0&t - 2
\end{vmatrix} =
t^2 - 4t + 4 = 0
$$
Which factors as $(t -2)^2 = 0$. Thus we have only one eigenvalue, $\lambda = 2$.

We have $2I-A=\begin{bmatrix}[cc]
0&3\\
0&0
\end{bmatrix}
$, a matrix of rank 1, and hence nullity 1. We see that $\boldv_1=(1,0)$ forms a basis for $W_2$. 
\\
(c) 
$$
\det(t I- A) =
\begin{vmatrix}[cc]
t - 2& 0\\
0&t - 2
\end{vmatrix} =
t^2 - 4t + 4 = 0
$$
Which factors as $(t -2)^2 = 0$. Thus we have only one eigenvalue, $t = 2$. 

We have $2I-A=\begin{bmatrix}[cc]
0&0\\
0&0
\end{bmatrix}
$
Then clearly $W_2=\NS(2I-A)=\R^2$, and $\{(1,0), (0,1)\}$ is a basis of $W_2$: i.e., all nonzero vectors of $\R^2$ are eigenvectors of $A$! 
 \\
(d) The characteristic polynomial is
$$
\det(t I- A) =
\begin{vmatrix}[cc]
t - 1& -\\
2&t + 1
\end{vmatrix} =
t^2 + 3 = 0
$$
Which does not have any real eigenvalues.
\end{solution}
\ii Compute the characteristic polynomial, eigenvalues, and eigenspace bases for
$$A=
\begin{bmatrix}[ccc]
1&0&-2\\
0&0&0\\
-2&0&4
\end{bmatrix}
$$
\begin{solution}
The characteristic polynomial is
$$
\det(t I - A) =
\begin{vmatrix}[ccc]
t - 1&0&2\\
0&t &0\\
2&0&t -4
\end{vmatrix}
=
t^3-5t = 0
$$
This factors as $t^2(t - 5)=0$. So the eigenvalues are $\lambda =0$ and $\lambda = 5$. To find the basis for $\lambda = 0$ we use
$$
\begin{bmatrix}[ccc]
-1&0&2\\
0&0&0\\
2&0&-4
\end{bmatrix}
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
0\\
0\\
0
\end{bmatrix}
$$
Thus $x_1 = 2x_3$, with the free variables $x_2=t$ and $x_3=r$.
$$
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
2r\\
t\\
r
\end{bmatrix}
=
r
\begin{bmatrix}[c]
2\\
0\\
1
\end{bmatrix} +t
\begin{bmatrix}[c]
0\\
1\\
0
\end{bmatrix}
$$
Thus the basis when $\lambda = 0$ is
$$
\left\{
\begin{bmatrix}[c]
2\\
0\\
1
\end{bmatrix},
\begin{bmatrix}[c]
0\\
1\\
0
\end{bmatrix}
\right\}
$$
Now for $\lambda = 5$ we use
$$
\begin{bmatrix}[ccc]
4&0&2\\
0&5&0\\
2&0&1
\end{bmatrix}
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
0\\
0\\
0
\end{bmatrix}
$$
Thus $x_1 = -1/2x_3$, $x_2 = 0$ and the free variable $x_3 = r$
$$
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
-1/2r\\
0\\
r
\end{bmatrix}
=
r
\begin{bmatrix}[c]
-1/2\\
0\\
1
\end{bmatrix}
$$
Thus the basis when $\lambda = 5$ is
$$
\left\{
\begin{bmatrix}[c]
-1/2\\
0\\
1
\end{bmatrix}
\right\}
$$
\end{solution}
\ii
Compute the characteristic polynomial, eigenvalues, and eigenspace bases for
$$A=
\begin{bmatrix}[ccc]
0&1&1\\
1&0&1\\
1&1&0
\end{bmatrix}
$$
\begin{solution}
The characteristic polynomial is
$$
\det(t I - A) =
\begin{vmatrix}[ccc]
t&-1&-1\\
-1&t &-1\\
-1&-1&t
\end{vmatrix}
=
t^3-3t - 2 = 0
$$
This factors as $(t+1)^2(t - 2)=0$. So the eigenvalues are $\lambda =-1$ and $\lambda = 2$. To find the basis for $\lambda = -1$ we use
$$
\begin{bmatrix}[ccc]
-1&-1&-1\\
-1&-1&-1\\
-1&-1&-1
\end{bmatrix}
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
0\\
0\\
0
\end{bmatrix}
$$
Thus $x_1 = -x_2-x_3$, with the free variables $x_2=t$ and $x_3=r$.
$$
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
-t-r\\
t\\
r
\end{bmatrix}
=
t
\begin{bmatrix}[c]
-1\\
1\\
0
\end{bmatrix} +r
\begin{bmatrix}[c]
-1\\
0\\
1
\end{bmatrix}
$$
Thus the basis when $\lambda = -1$ is
$$
\left\{
\begin{bmatrix}[c]
-1\\
1\\
0
\end{bmatrix},
\begin{bmatrix}[c]
-1\\
0\\
-1
\end{bmatrix}
\right\}
$$
Now for $\lambda = 2$ we use
$$
\begin{bmatrix}[ccc]
2&-1&-1\\
-1&2&-1\\
-1&-1&2
\end{bmatrix}
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
0\\
0\\
0
\end{bmatrix}
$$
Thus $x_1 = x_3$, $x_2 = x_3$ and the free variable $x_3 = r$
$$
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
r\\
r\\
r
\end{bmatrix}
=
r
\begin{bmatrix}[c]
1\\
0\\
1
\end{bmatrix}
$$
Thus the basis when $\lambda = 2$ is
$$
\left\{
\begin{bmatrix}[c]
1\\
1\\
1
\end{bmatrix}
\right\}
$$
\end{solution}
\ii 
Compute the characteristic polynomial, eigenvalues, and eigenspace bases for
$$A=
\begin{bmatrix}[ccc]
1&-3&3\\
3&-5&3\\
6&-6&4
\end{bmatrix}
$$
\begin{solution}
The characteristic polynomial is
$$
\det(t I - A) =
\begin{vmatrix}[ccc]
t-1&3&-3\\
-3&t + 5&-3\\
-6&6&t-4
\end{vmatrix}
=
t^3-12t - 16 = 0
$$
This factors as $(t+2)^2(t - 4)=0$. So the eigenvalues are $\lambda =-2$ and $\lambda = 4$. To find the basis for $\lambda = -2$ we use
$$
\begin{bmatrix}[ccc]
-3&3&-3\\
-3&3&-3\\
-6&6&-6
\end{bmatrix}
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
0\\
0\\
0
\end{bmatrix}
$$
Thus $x_1 = x_2-x_3$, with the free variables $x_2=t$ and $x_3=r$.
$$
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
t-r\\
t\\
r
\end{bmatrix}
=
t
\begin{bmatrix}[c]
1\\
1\\
0
\end{bmatrix} +r
\begin{bmatrix}[c]
-1\\
0\\
1
\end{bmatrix}
$$
Thus the basis when $\lambda = -2$ is
$$
\left\{
\begin{bmatrix}[c]
1\\
1\\
0
\end{bmatrix},
\begin{bmatrix}[c]
-1\\
0\\
1
\end{bmatrix}
\right\}
$$
Now for $\lambda = 4$ we use
$$
\begin{bmatrix}[ccc]
3&3&-3\\
-3&9&-3\\
-6&6&0
\end{bmatrix}
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
0\\
0\\
0
\end{bmatrix}
$$
Thus $x_1 = 1/2x_3$, $x_2 = 1/2x_3$ and the free variable $x_3 = r$
$$
\begin{bmatrix}[c]
x_1\\
x_2\\
x_3
\end{bmatrix}
=
\begin{bmatrix}[c]
1/2r\\
1/2r\\
r
\end{bmatrix}
=
r
\begin{bmatrix}[c]
1/2\\
1/2\\
1
\end{bmatrix}
$$
Thus the basis when $\lambda = 4$ is
$$
\left\{
\begin{bmatrix}[c]
1/2\\
1/2\\
1
\end{bmatrix}
\right\}
$$
\end{solution}

\ii Find the characteristic polynomial by inspection.
$$
\begin{bmatrix}[rrrr]
9&-8&6&3\\
0&-1&0&0\\
0&0&3&0\\
0&0&0&7
\end{bmatrix}
$$
\begin{solution}
Since the given matrix is upper triangular, so is $tI - A$. Since determinants of triangular matrices are just the product of their diagonal entries, we have 
\[
p(t)=\det(tI-A)=(t-9)(t+1)(t-3)(t-7).
\]
Similar reasoning shows, in general, that the eigenvalues of a triangular matrix (upper or lower or diagonal) are simply the entries along the diagonal. 
\end{solution}


\ii \label{ex:last} Let $A=\begin{bmatrix}[rrr] -5&0&3\\ -6&1&3\\ -6&0&4\end{bmatrix}$. 
Compute $p(t)$ and find bases for all eigenspaces of $A$. 
%\vfill
\\
\begin{solution}
	The characteristic polynomial of $A$ is $p(t)=t^3-3t+2=(t-1)^2(t+2)$. 
	
	We compute:
	\begin{align*}
	W_{1}&=\NS(I-A)=\Span(\{(1,0,2),(0,1,0) \})\\
	W_{-2}&=\NS(-2I-A)=\Span(\{(1,1,1)\})
	\end{align*}
\end{solution}
\ii The matrix $B=\begin{bmatrix}[rrr]-2&-3&3\\ -3&-3&4\\ -3&-4&5\end{bmatrix}$ has the same characteristic polynomial of $A$ in Exercise \ref{ex:last}. Find bases for all eigenspaces of $B$. 
%\vfill
\\
\begin{solution}
	Since $B$ also has characteristic polynomial $p(t)=(t-1)^2(t+2)$, it has only two eigenspaces. 
	
	We compute:
	\begin{align*}
	W_{1}&=\NS(I-B)=\Span(\{ (0,1,1) \})\\
	W_{-2}&=\NS(-2I-B)=\Span(\{(1,1,1)\})
	\end{align*}
	The two problems taken together show that the characteristic polynomial is not the whole story! The matrix $A$ has more linearly independent eigenvectors than $B$. 
\end{solution}
\ii Use induction to prove that if $\lambda$ is an eigenvalue of $A$, then $\lambda^k$ is an eigenvalue of $A^k$ for all positive integers $k$. Make sure to invoke the definition of eigenvalue in your proof. 
%\vfill
\begin{solution}
	Take $\boldv\ne \boldzero$ with $A\boldv=\lambda\boldv$. We show by induction that $A^k\boldv=\lambda^k\boldv$, and thus that $\lambda^k$ is an eigenvalue of $A^k$. 

The base case, $k=1$, is just the statement that $\boldv$ is an eigenvector. 

Induction step: assume $A^{k}\boldv=\lambda^k\boldv$. Then 
\begin{align*}
A^{k+1}\boldv &=AA^{k}\boldv\\
&=A(\lambda^k\boldv) &\text{(induction hypo.)}\\
&=\lambda^kA\boldv &\text{($\lambda$ is a scalar)}\\
&=\lambda^k\lambda\boldv&\text{($\boldv$ is an eigenvector)}\\
&=\lambda^{k+1}\boldv.
\end{align*} 
If we didn't know proof by induction, our ``proof" would look something like 
\begin{align*}
A^k\boldv &=\underbrace{A\cdot A\cdots A}_{\text{$k$ times}}\boldv\\
&=\underbrace{A\cdot A\cdots A}_{\text{$k-1$ times}}(A\boldv)\\
&=\underbrace{A\cdot A\cdots A}_{\text{$k-1$ times}}(\lambda\boldv) &\text{(since $A\boldv=\lambda\boldv$)}\\
&=\lambda\underbrace{A\cdot A\cdots A}_{\text{$k-1$ times}}(\boldv) &\text{(since $\lambda$ is a scalar)}\\
&=\lambda^2\underbrace{A\cdot A\cdots A}_{\text{$k-2$ times}}(\boldv) &\text{(by same logic)}\\
&\vdots \\
&=\lambda^{k-1}A\boldv \\
&=\lambda^k\boldv
\end{align*}
You can think of proof by induction as a way of replacing all these $\dots$ and $\ \vdots \ $ with a rigorous argument! 
\end{solution}
\ii Prove: $A$ is invertible if and only if $\lambda=0$ is {\em not} an eigenvalue of $A$. 
%\vfill
\\
\begin{solution}
There are many different ways of proving this.

Let $\lambda_1, \dots , \lambda_n$ be the eigenvalues of $A$. Recall that $\det(A)=\lambda_1\lambda_2\cdots \lambda_n$. 

Then we have 
\begin{align*}
A \text{ invertible} &\Leftrightarrow \det(A)\ne 0 \\
&\Leftrightarrow \lambda_1\lambda_2\cdots \lambda_n\ne 0\\
&\Leftrightarrow \lambda_i\ne 0 \text{ for all $i$}\\
&\Leftrightarrow 0 \text{ is not an eigenvalue of $A$}.
\end{align*}
\end{solution}
\ii
Let $B=\{\cos(x), \sin(x), \cosh(x), \sinh(x)\}$, let $V=\Span(B)$, and define $T\colon V\rightarrow V$ by $T(f)=f'$. 

You may take as a fact that the set $B$ is linearly independent, hence a basis. 

Recall: $\ds\cosh(x)=\frac{e^x+e^{-x}}{2}$, $\ds\sinh(x)=\frac{e^x-e^{-x}}{2}$. 
\bb
\ii Compute $A=[T]_B$. What does $A$ tell you about the invertibility of $T$?
\ii Find all eigenvalues of $T$.  
\ii For each eigenvalue $\lambda$ of $T$, find a basis for $W_\lambda$. 
\ii Use your work in (c) to find a new basis $B'$ of $V$ that contains two eigenvectors of $T$.
\ii  Compute $[T]_{B'}$. 
\ee
%\vfill
\begin{solution}
	We find that $A=\begin{bmatrix}
	0&1&0&0\\
	-1&0&0&0\\
	0&0&0&1\\
	0&0&1&0
	\end{bmatrix}$.
	\\
	Since $A$ is invertible (its rank is clearly 4), it follows that $T$ is invertible. 
	\\ \\
	The characteristic polynomial of $A$ (in factored form) is $p(t)=(t-1)(t+1)(t^2+1)$. Thus the eigenvalues of $A$ (and hence also $T$) are $\lambda=1$ and $\lambda=-1$. 
	
	We compute bases for the eigenspaces of $A$:
	\begin{align*}
	W_1&=\NS(I-A)=\Span(\{ (0,0,1,1)\}\\
	W_{-1}&=\NS(-I-A)=\Span(\{(0,0,1,-1)\}
	\end{align*}
	Lifting these vectors back to $V$ via the $[\hspace{5pt}]_B$ map yields bases for the same eigenspaces of $T$: \begin{align*}
	W_1&=\Span(\{\cosh x+\sinh x\}=\Span(\{e^x\})\\
	W_{-1}&=\Span(\{\cosh x-\sinh x\}=\Span(\{e^{-x}\})
	\end{align*}
	We construct the new basis $B'=\{\cos(x),\sin(x),e^x, e^{-x}\}$ (it is easy to see that these four functions are independent), and compute 
	\[
	A'=[T]_{B'}=\begin{bmatrix}
	0&1&0&0\\
	-1&0&0&0\\
	0&0&1&0\\
	0&0&0&-1
	\end{bmatrix}
	\]
\end{solution}
\ii Suppose $A$ is a $2\times 2$ matrix satisfying $\tr A=2$ and $\det A=-15$. Find the eigenvalues of $A$.
\\
\begin{solution}
Let $\lambda, \lambda'$ be the eigenvalues of $A$ (possibly complex, possibly repeated). We have 
\begin{align*}
\lambda+\lambda'&=\tr A=2 &\text{(trace is the sum of the eigenvalues)}\\
\lambda\lambda'&=\det A=-15&\text{(det. is the product of the eigenvalues)}
\end{align*}
The system implies $\lambda'=2-\lambda$, and hence that $\lambda^2-2\lambda-15=0$. Thus $\lambda=5$ or $\lambda=-3$. Thus the eigenvalues of $A$ are $5$ and $-3$. 

Alternatively, the characteristic polynomial of $A$ is 
\[
p(x)=x^2-\tr A x+\det A=x^2-2x-15=(x-5)(x+3).
\]
Thus the eigenvalues of $A$ are $5$ and $-3$. 

\end{solution} 
\ii Let $T\colon V\rightarrow V$ be a linear transformation, and suppose $B=\{\boldv_1, \boldv_2,\dots, \boldv_n\}$. 

\noindent
Prove: $[T]_B$ is a diagonal matrix if and only if $\boldv_j$ is an eigenvector of $T$ for all $1\leq i\leq n$. 

\noindent
{\em Moral}. We can represent $T$ by a diagonal matrix if and only if $T$ has ``enough" linearly independent eigenvectors to build a basis. 
\\
\begin{solution}
\noindent
We prove the two implications separately. 
\vspace{.1in}
\\
($\Rightarrow$) Suppose the vectors $\boldv_i$ are all eigenvectors with $T(\boldv_i)=\lambda_i\boldv_i$. Using the recipe for $[T]_{B}$ we compute 
\begin{align*}
[T]_B&=\begin{bmatrix}
\vert &\vert & &\vert\\
[T(\boldv_1)]_B&[T(\boldv_2)]&\cdots &[T(\boldv_n)]_B\\
\vert &\vert & &\vert
\end{bmatrix}\\
&=\begin{bmatrix}
\vert &\vert & &\vert\\
[\lambda_1\boldv_1]_B&[\lambda_2\boldv_2]&\cdots& [\lambda_n\boldv_n]_B\\
\vert &\vert & &\vert
\end{bmatrix}\\
&=\begin{bmatrix}\lambda_1&0&\cdots &0\\
0&\lambda_2&\cdots&0\\
\vdots & & &\\
0&0&\cdots&\lambda_n
\end{bmatrix},
\end{align*}
where the last equality holds since $\lambda_1\boldv_1=\lambda_1\boldv_1+0\boldv_2+\dots, \lambda_2\boldv_2=0\boldv_1+\lambda_2\boldv_2+0\boldv_3+\dots$, etc. 
\vspace{.1in}
\\
($\Leftarrow$) Suppose 
\[
[T]_B=D=\begin{bmatrix}d_1&0&\cdots &0\\
0&d_2&\cdots&0\\
\vdots & & &\\
0&0&\cdots&d_n
\end{bmatrix}.
\]
I claim $T(\boldv_i)=d_i\boldv_i$ for all $i$, which proves $B$ consists of eigenvectors of $T$. To do so, first observe that 
\begin{align*}
[T(\boldv_i)]_B&=[T]_B[\boldv_i]_B &\text{(defining property of $[T]_B$)}\\
&=D\bolde_i &\text{(since $[\boldv_i]_B=(0,0,\dots, 1,0,\dots, 0)=\bolde_i$)}\\
&=\begin{bmatrix}
0\\ \vdots \\ d_i\\ \vdots \\ 0
\end{bmatrix}.
\end{align*}
But we have $[T(\boldv_i)]_B=(0,0,\dots, d_i, 0,\dots, 0)$ if and only if $T(\boldv_i)=d_i\boldv_i$, by definition of the coordinate vector map. This proves each $\boldv_i$ is an eigenvector with eigenvalue $d_i$. 
\end{solution}
\ee
