\bb[itemsep=5pt, topsep=5pt]

\ii Find the eigenvalues of $A$. For each eigenvalue $\lambda$, find the rank of the matrix $\lambda I - A$. Is $A$ diagonalizable? Justify.
\[
A=
\begin{bmatrix}[rrr]
3&0&0\\
0&2&0\\
0&1&2
\end{bmatrix}
\]
\\
\begin{solution}
\noindent We compute $p(t)$: 
$$
\det(t I - A) =
\begin{vmatrix}[rrr]
t - 3&0&0\\
0&t - 2&0\\
0&-1&t - 2
\end{vmatrix}
= (t - 3)(t - 2)^2 = 0
$$
Thus the eigenvalues are $\lambda =3$ and $\lambda = 2$.
\\
We compute $\dim W_3=\NS(3I-A)$: 
\[
3I-A=
\begin{bmatrix}[rrr]
0&0&0\\
0&1&0\\
0&-1&1
\end{bmatrix}
\]
Since the rank of this matrix is 2, its nullity is 3-2=1. This means $\dim(W_{3})=1$. 
\\
Next, we have 
$$2I-A=
\begin{bmatrix}[rrr]
-1&0&0\\
0&0&0\\
0&-1&0
\end{bmatrix}
$$
Once again the nullity is 1, which means $\dim(W_2)=1$. Since the sum of the dimensions of the eigenspaces is $1+1\ne 3$, we conclude that $A$ is not diagonalizable. 
\end{solution}

\ii  For each matrix $A$ below, find the geometric and algebraic multiplicity of each eigenvalue of the matrix $A$, and determine whether $A$ is diagonalizable. If $A$ is diagonalizable, then find the matrix $P$ that diagonalizes $A$, and find $P^{-1}AP$.
\bb[itemsep=5pt, topsep=5pt]
\ii 
\[
A =
\begin{bmatrix}[rrr]
-1&4&-2\\
-3&4&0\\
-3&1&3
\end{bmatrix}
\]
\ii 
\[
A=
\begin{bmatrix}[ccc]
19&-9&-6\\
25&-11&-9\\
17&-9&-4
\end{bmatrix}
\]
\ii 
\[
A=
\begin{bmatrix}[rrr]
0&0&0\\
0&0&0\\
3&0&1
\end{bmatrix}
\]
\ii
\[
A =
\begin{bmatrix}[rrr]
5&0&0\\
1&5&0\\
0&1&5
\end{bmatrix}
\]
\ee
\begin{solution}
\noindent 
(a) Start by finding the eigenvalues of matrix $A$.
$$
\det(t I - A) =
\begin{vmatrix}[rrr]
t +1&-4&2\\
3&t - 4&0\\
3&-1&t - 3
\end{vmatrix}
= (t - 1)(t - 2)(t - 3) = 0
$$
We have a theorem that says a matrix $A\in M_{nn}$ with $n$ distinct eigenvalues is diagonalizable. Thus $A$ is diagonalizable. To construct the matrix $P$, we need to come up with bases for each space $W_\lambda=\NS(\lambda I-A)$. Each of these has dimension 1, and we easily come up with bases by inspection. This yields the basis $\boldv_1=(1,1,1)$, $\boldv_2=(2,3,3)$ and $\boldv_3=(1,3,4)$.  

According to the recipe, we then have 
$$
P =
\begin{bmatrix}[ccc]
1&2&1\\
1&3&3\\
1&3&4
\end{bmatrix}
,  D=
\begin{bmatrix}[rrr]
1&0&0\\
0&2&0\\
0&0&3
\end{bmatrix}, \text{ and } D=P^{-1}AP.
$$
\\
(b) 
Start by finding the eigenvectors in the usual way
$$\det(t I - A) =
\begin{vmatrix}[ccc]
t-19 &9&6\\
-25&t+11&9\\
-17&9&t +4
\end{vmatrix}
= t^3 - 4t^2+5t-2 = (t -1)^2(t-2) = 0
$$
For $\lambda = 1$, we can see that the algebraic multiplicity is 2. Consider the matrix
$$
\begin{bmatrix}[ccc]
-18 &9&6\\
-25&12&9\\
-17&9&5
\end{bmatrix}
\rightarrow
\begin{bmatrix}[ccc]
1&0&-1\\
0&1&-4/3\\
0&0&0
\end{bmatrix}
$$
Thus the rank is 2 and the geometric multiplicity is 1. Since the geometric multiplicity is less than the algebraic multiplicity for one of the eigenspaces, we know immediately that $A$ is not diagonalizable. 

However, we continue on nonetheless. For $\lambda =2$, we can see that the algebraic multiplicity is 1. We know that $1\leq \dim W_2\leq 1$, hence we must have $\dim W_2=1$. Let's verify this: 
$$
2I-A=
\begin{bmatrix}[ccc]
-17 &9&6\\
-25&13&9\\
-17&9&6
\end{bmatrix}
\rightarrow
\begin{bmatrix}[ccc]
1&0&-3/4\\
0&1&-3/4\\
0&0&0
\end{bmatrix},
$$
a matrix of rank 2, and hence nullity 1. 

Lastly, we see that $\dim W_1+\dim W_2=1+1=2\ne 3$. Hence $A$ is not diagonalizable, as we claimed above. 
\\
(c)
$$\det(t I - A) =
\begin{vmatrix}[ccc]
t &0&0\\
0&t&0\\
-3&0&t -1
\end{vmatrix}
= t^2(t -1) = 0
$$
Thus the eigenvalues are $\lambda =0$ and $\lambda = 1$. For $\lambda =0$, notice that the algebraic multiplicity is 2. Now consider the matrix
$$
\begin{bmatrix}[rrr]
0&0&0\\
0&0&0\\
-3&0&-1
\end{bmatrix}
$$
The rank is 1, thus the geometric multiplicity is  2.  For $\lambda = 1$, notice that the algebraic multiplicity is 1. Now consider the matrix
$$
\begin{bmatrix}[rrr]
1&0&0\\
0&1&0\\
-3&0&0
\end{bmatrix}
$$
The rank is 2, thus the geometric multiplicity is 1. Since for all $\lambda$ the geometric multiplicity is equal to the algebraic multiplicity, the initial matrix is diagonalizable.  As usual, we build $P$ by first computing bases for $W_0$ and $W_1$, and then putting these in as the columns of $P$. One possible answer is 
$$P =
\begin{bmatrix}[ccc]
-1&0&0\\
0&1&0\\
3&0&1
\end{bmatrix}
$$
with corresponding 
\[
D=
\begin{bmatrix}[rrr]
0&0&0\\
0&0&0\\
0&0&1
\end{bmatrix}
\]
\ \\
(d)
$$\det(t I - A) =
\begin{vmatrix}[ccc]
t - 5&0&0\\
-1&t-5&0\\
0&-1&t -5
\end{vmatrix}
= (t -5)^3 = 0
$$
Thus $\lambda = 5$ is the only eigenvalue and its algebraic multiplicity is 3. Now considering when $\lambda$ is 5, we look at the matrix
$$
\begin{bmatrix}[rrr]
0&0&0\\
-1&0&0\\
0&-1&0
\end{bmatrix}
$$
The rank of this matrix is 2, thus the geometric multiplicity is 1. Since algebraic multiplicity is not equal to the geometric multiplicity, the initial matrix is not diagonalizable.
\end{solution}
\ii Let $A=\begin{bmatrix}
a&b\\ c&d
\end{bmatrix}$. Derive necessary and sufficient conditions for $A$ to be diagonalizable expressed in terms of equalities and inequalities involving $a,b,c,d$. 
\\
\begin{solution}
\noindent
First analyze the diagonalizability of $A$ based on the number of distinct real eigenvalues it has: 0, 1, or 2. 

\noindent
If $A$ has 0 eigenvalues, then it is clearly not diagonalizable, as it has no eigenvectors. 

\noindent
If $A$ has 2 distinct eigenvalues, then it is automatically diagonalizable, by the corollary to the linear independence of eigenvectors theorem. 

\noindent
If $A$ has exactly one eigenvalue $\lambda$, then $A$ is diagonalizable if and only if $\dim W_\lambda =2$ (by the diagonalization theorem) if and only if $\dim\NS \lambda I -A=2$ if and only if $\rank \lambda I-A=0$ if and only if $\lambda I-A=\underset{2\times 2}{\boldzero}$ if and only if $A=\lambda I$ is diagonal !!

\noindent
We conclude that $A$ is diagonalizable if and only if (a) it has two distinct eigenvalues or (b) it has one eigenvalue and is diagonal. We can relate these conditions to $a,b,c,d$ by looking at the characteristic polynomial $p(x)$. 

\noindent
The characteristic polynomial of $A=\begin{bmatrix}
a&b\\ c&d
\end{bmatrix}$ is $p(x)=x^2-(a+d)x+(ad-bc)$. Its roots are given by 
\[
x=\frac{1}{2}(a+d\pm \sqrt{a^2+2ad+d^2-4ad+4bc}\ )=\frac{1}{2}(a+d\pm \sqrt{(a-d)^2+4bc}\ )
\]
We see that (a) $A$ has two real eigenvalues iff $(a-d)^2>-4bc$, and that (b) $A$ is diagonal with one distinct eigenvalue iff $a=d$ and $b=c=0$. 

\noindent
We conclude: 
\[
\text{$A$ is diagonalizable } \Longleftrightarrow (a-d)^2>-4bc\text{ or } a-d=b=c=0.
\]
\end{solution}
\ii Prove all statements of the properties of similarity theorem, stated below. 
\begin{namedtheorem}[Properties of similarity theorem]
 Suppose $A$ is similar to $B$: i.e., there is an invertible matrix $P$ such that $B=P^{-1}AP$. Then:
 \bb[(a)]
\ii $B$ is similar to $A$. ({\em Similarity is symmetric}.)
\ii $A$ and $B$ have the same trace and determinant. 
\ii $A$ and $B$ have the same rank nullity and rank. 
\ii $A$ and $B$ have the same characteristic polynomial.  
\ii $A$ and $B$ have the same eigenvalues. 
\ii Given any $\lambda\in\R$, let $W_\lambda$ be the corresponding eigenspace for $A$, and $W_\lambda'$ the corresponding eigenspace for $B$. Then $\dim W_{\lambda}=\dim W_{\lambda}'$. 
\ee
 \end{namedtheorem}
\begin{solution}
\noindent
(a) If $B=P^{-1}AP$, then $A=PBP^{-1}=Q^{-1}BQ$, where $Q=P^{-1}$. 
\vspace{.1in}
\\
(d) This was proven in the lecture notes. 
\vspace{.1in}
\\
(b)\& (e) Follow directly from (d) and our theorem about characteristic polynomials. Indeed, let $p(x)=x^n+a_{n-1}x^{n-1}+\cdots +a_1x+a_0$ be the characteristic polynomial of $A$ and $B$. Then $\tr A=\tr B=-a_{n-1}$, $\det A=\det B=(-1)^na_0$, and the eigenvalues of both $A$ and $B$ are precisely the roots of $p(x)$. 
(c) I claim $\boldx'\in \NS B$ if and only if $\boldx'=P^{-1}\boldx$ for some $\boldx\in \NS B$. To see this, first observe that if $\boldx\in\NS A$, then 
\[
B\boldx'=B(P^{-1}\boldx)=P^{-1}AP(P^{-1}\boldx)=P^{-1}A\boldx=\boldzero.
\]
Now suppose $\boldx'\in \NS B$. A similar argument (using $A=PBP^{-1}$) shows that $\boldx=P\boldx'\in \NS A$; and we have $\boldx'=P\boldx$.
\vspace{.1in}
\\
From our claim it follows that $P^{-1}$ defines an invertible linear transformation (with inverse $P$) from $\NS A$ to $\NS B$. In fancy terms this means $\NS A$ and $\NS B$ are isomorphic, and hence have the same dimension. More directly, if $B=\{\boldv_1,\dots, \boldv_r\}$ is a basis for $\NS A$, then you can show that $\{P^{-1}\boldv_1, P^{-1}\boldv_2, \dots , P^{-1}\boldv_r\}$ is a basis for $\NS B$.  
\vspace{.1in}
\\
Either way, we see that $\nullity A=\dim \NS A=\dim\NS B=\nullity B$. The rank-nullity theorem then implies that $\rank A=\rank B$. 
\vspace{.1in}
\\
(f) An argument exactly like the the one above shows that the map $\boldx\mapsto P^{-1}\boldx$ defines an invertible linear transformation from $W_\lambda$ to $W_\lambda'$, and that $P^{-1}$ maps any basis of $W_{\lambda}$ to a basis of $W_{\lambda}'$. Thus $\dim W_{\lambda}=\dim W_{\lambda}'$. 
\end{solution}
\ii Similar matrices have the same rank. Show the converse is false by showing the matrices
$$
A =
\begin{bmatrix}[rr]
1&0\\
0&0
\end{bmatrix}
,
B =
\begin{bmatrix}[rr]
0&1\\
0&0
\end{bmatrix}
$$
have the same rank, but are not similar.
\\
\begin{solution}Notice that $\rank(A) = \rank(B) = 1$. Also notice that $\tr(A) = 1 \neq 0 = \tr(B)$. Since the traces are different, the matrices are not similar. 
\end{solution}
\ii Similar matrices have the same eigenvalues. Show the converse is false by showing the matrices
$$
A =
\begin{bmatrix}[rr]
1&1\\
0&1
\end{bmatrix}
,
B =
\begin{bmatrix}[rr]
1&0\\
0&1
\end{bmatrix}
$$
have the same eigenvalues, but are not similar.
\\
\begin{solution}
Notice both $A$ and $B$ are triangular, thus $\det(t I - A) = (t - 1)^2 = \det(t I -B)$. Thus $\lambda = 1$ is the only eigenvalue for both $A$ and $B$. 

However, as one can easily compute, for $A$ the eigenspace $W_1$ has dimension 1, whereas for $B$ the eigenspace $W_1$ has dimension 2. Thus the two cannot be similar! 
\end{solution}
\ii Prove that if $B = P^{-1}AP$, and $\boldv$ is an eigenvector of $B$ corresponding to the eigenvalue $\lambda$, the $P\boldv$ is the eigenvector of $A$ corresponding to $\lambda$.
\\
\begin{solution}
We assume that $\boldv$ is an eigenvector of $B$, and must show that $P\boldv$ is an eigenvector of $A$. 

Thus by definition of eigenvector we assume
\bb[(i)]
\ii $B\boldv=\lambda\bold v$, and 
\ii $\boldv\ne\boldzero$. 
\ee
But then 
\begin{align*}
A(P\boldv)&=AP\boldv\\
&=PB\boldv &\text{(since $B=P^{-1}AP$)}\\
&=P(\lambda\boldv) &\text{ (since $B\boldv=\lambda\boldv$)}\\
&=\lambda P\boldv) &\text{ (since $\lambda$ is a scalar)}.
\end{align*}
Thus $A(P\boldv)=\lambda P(\boldv)$. 

To show $P\boldv$ is an eigenvector, it remains only to show that $P\boldv\ne\boldzero$. But since $P$ is invertible, and since $\boldv\ne\boldzero$, it follows that $P\boldv\ne\boldzero$. 
\end{solution}
\ii \label{ex:conjpoly}
Let $A$ be an $n$ by $n$ matrix, and let $q(A)$ be the matrix
$$
q(A) = a_nA^n +a_{n-1}A^{n-1}+\cdots+a_1A+a_0I_n
$$
Prove that if $B = P^{-1}AP$, then $q(B) = P^{-1}q(A)P$.
\\
\begin{solution}
First we need to show (by induction) that $B^n =P^{-1}A^nP$ for all $n$. The base case, when $n=1$ is give by the hypothesis. Assume that equality holds when $n \leq k$. We need to show that equality holds when $n = k+1$.
\begin{eqnarray*}
B^{k+1} &=& BB^k\\
&=&(P^{-1}AP)B^k\\
&=& (P^{-1}AP)(P^{-1}A^kP)\\
&=& P^{-1}A(PP^{-1})A^kP\\
&=&P^{-1}A^{k+1}P
\end{eqnarray*}
Thus $B^n =P^{-1}A^nP$ holds for all $n$. Now we can show the desired result.
\begin{eqnarray*}
q(B) &=& a_nB^n + a_{n-1}B^{n-1}+\cdots+a_1B+a_0I_n\\
&=& a_n(P^{-1}A^nP) + a_{n-1}(P^{-1}A^{n-1}P)+\cdots+a_1(P^{-1}AP)+a_0I_n\\
&=& P^{-1}(a_nA^nP) + (a_{n-1}A^{n-1}P)+\cdots+(a_1AP)+a_0I_n)\\
&=&P^{-1}(a_nA^n) + (a_{n-1}A^{n-1})+\cdots+(a_1A)+a_0I_n)P\\
&=& P^{-1}(q(A))P
\end{eqnarray*}
\end{solution}

\ii  Let $A$ be a $3\times 3$ matrix with eigenvalues $1, -1, 0$. 
\bb
\ii Prove that $A$ is diagonalizable. What is $D$ in this case?
\ii Show that $A^n=A$ for any {\bf odd} $n\geq 0$: e.g., $A^{13}=A$, $A^{77}=A$, etc.
\ee
\begin{solution}
%\ \\
The matrix is diagonalizable as it has three distinct eigenvalues. We have $D=P^{-1}AP$ where $D=\begin{bmatrix}
1&0&0\\
0&-1&0\\
0&0&0
\end{bmatrix}$. 
\\
It is easy to see that 
\[
D^n=\begin{bmatrix}
1^n&0&0\\
0&(-1)^n&0\\
0&0&0^n
\end{bmatrix}
=\begin{bmatrix}
1&0&0\\
0&(-1)^n&0\\
0&0&0
\end{bmatrix}
\]
Thus we see clearly that $D^n=D$ if and only if $n$ is odd. 

Next, write $A=PDP^{-1}$. Then $A^n=(PDP^{-1})^n=PD^nP^{-1}$, where this last step is the extremely useful fact that ``conjugation respects powers of matrices".  If $n$ is odd, we thus have $A^n=PD^nP^{-1}=PDP^{-1}=A$, as claimed. 
\end{solution}
\ii The {\em Fibonacci sequence} $F_0, F_1, F_2, \dots$ is defined recursively as follows: 
\begin{align*}
F_0&=0\\
F_1&=1\\
F_n&=F_{n-2}+F_{n-1} &\text{for $n\geq 2$}
\end{align*}
Let $A=\begin{bmatrix}
0&1\\
1&1
\end{bmatrix}$.
\bb
\ii Show that 
\[
A^n\begin{bmatrix}
0 \\ 1
\end{bmatrix}=\begin{bmatrix}
F_n\\ F_{n+1}
\end{bmatrix}
\]
for all $n\geq 0$. 
\ii Diagonalize $A$. Write its two eigenvalues as $\lambda_1$ and $\lambda_2$. 

\ii Derive a closed formula for $A^n$. 

\ii Derive  a closed formula for $F_n$. 

\ee
\begin{solution}
\noindent
(a) Proof by induction. Base case ($n=0$) is trivial. For induction step, suppose 
$A^n\begin{bmatrix}
0 \\ 1
\end{bmatrix}=\begin{bmatrix}
F_n\\ F_{n+1}
\end{bmatrix}$, then 
\begin{align*}
A^{n+1}\begin{bmatrix}
0\\ 1
\end{bmatrix}&=AA^n\begin{bmatrix}
0\\ 1
\end{bmatrix}\\
&=A\begin{bmatrix}
F_n\\ F_{n+1}
\end{bmatrix} &\text{(induction hypo.)}\\
&=\begin{bmatrix}
F_{n+1}\\ F_n+F_{n+1}
\end{bmatrix}\\
&=\begin{bmatrix}
F_{n+1}\\ F_{n+2}.
\end{bmatrix}
\end{align*}
as desired. 
\vspace{.1in}
\\
(b) The eigenvalues of $A$ are $\lambda_1=\frac{1}{2}(1+\sqrt{5})$ and $\lambda_2=\frac{1}{2}(1-\sqrt{5})$.
\\
Note that the $\lambda_i$ satisfy\\
$\lambda_1+\lambda_2=1$\\
$\lambda_1\lambda_2=-1$\\
$\lambda_1^2=\lambda_1+1$, $\lambda_2^2+\lambda_2=1$. 
\\
(These follow from the fact that they are roots of $p(x)=x^2-x-1=(x-\lambda_1)(x-\lambda_2)$. )
\vspace{.1in}
\\
From these relations it is easy to verify that 
$\boldv_1=(1,\lambda_1)$ and $\boldv_2=(1,\lambda_2)$ form bases of $W_{\lambda_1}$ and $W_{\lambda_2}$, respectively. 
\vspace{.1in}
\\
Then we have $D=P^{-1}AP$ where 
\[
D=\begin{bmatrix}[rr]
\lambda_1&0\\
0&\lambda_2
\end{bmatrix}, \ P=\begin{bmatrix}[rrr]
1&1\\
\lambda_1&\lambda_2
\end{bmatrix}, \ P^{-1}=-\frac{1}{\sqrt{5}}\begin{bmatrix}[rrr]
\lambda_2&-1\\
-\lambda_1&1
\end{bmatrix}.
\]
(c) 
Now we compute  
\[
A^n=(PDP^{-1})^n=PD^nP^{-1}=-\frac{1}{\sqrt{5}}\begin{bmatrix}[rrr]
1&1\\
\lambda_1&\lambda_2
\end{bmatrix}\begin{bmatrix}[rr]
\lambda_1^n&0\\
0&\lambda_2^n
\end{bmatrix}\begin{bmatrix}[rrr]
\lambda_2&-1\\
-\lambda_1&1
\end{bmatrix}
=\frac{1}{\sqrt{5}}\begin{bmatrix}
\lambda_1^{n-1}-\lambda_2^{n-1}&\lambda_1^n-\lambda_2^n\\
\lambda_1^n-\lambda_2^n&\lambda_1^{n+1}-\lambda_2^{n+1}
\end{bmatrix}
\]
(d) Lastly, using our formula above for $A^n$ we see that
\[
\begin{bmatrix}
F_n\\
F_{n+1}
\end{bmatrix}=A^n\begin{bmatrix}
0\\1
\end{bmatrix}
=\frac{1}{\sqrt{5}}\begin{bmatrix}
\lambda_1^n-\lambda_2^n\\
\lambda_1^{n+1}-\lambda_2^{n+1}
\end{bmatrix}.
\]
It follows that 
\[
F_n=\frac{1}{\sqrt{5}}(\lambda_1^{n}-\lambda_2^n), 
\]
where $\lambda_1=\frac{1}{2}(1+\sqrt{5}), \lambda_2=\frac{1}{2}(1-\sqrt{5})$. 
\end{solution}
\ii Let $A\in M_{nn}$ be defined as the matrix of all 1's: i.e., $a_{ij}=1$ for all $1\leq i, j\leq n$. 
\\
Show that $A$ has exactly 2 eigenvalues and that $A$ is diagonalizable. 
\\
Hint: do NOT compute the characteristic polynomial of $A$. Instead, begin by first considering $\NS(A)$. 
\\
\begin{solution}
%\ \\
\noindent 
Since $\rank(A)=1$, we have $\nullity(A)=\dim W_0=n-1$. Thus $0$ is an eigenvalue of $A$ with geometric multiplicity $n-1$. Since geometric multiplicity is bounded by algebraic multiplicity, we must have the characteristic polynomial $p(t)$ factoring as $p(t)=t^{n-1}(t-c)$ for some $c\in \R$. (Remember that $\deg p(t)=n$.) Now use the fact that 
\[
n=\tr A=(\text{sum of eigenvalues})=0+0+\cdots +0+c.
\]
It follows that $c=n$. Thus the distinct eigenvalues of $A$ are $0$ and $c$. Since $\dim W_0=n-1$ and $1\leq \dim W_n\leq 1$, we have 
\[
\dim W_0+\dim W_n=n-1+1=n.
\] 
Thus $A$ is diagonalizable. 

By the way, try diagonalizing $A$ yourself: that is, produce a basis of eigenvectors. 
\end{solution}
\ii The {\em Cayley-Hamilton theorem} states that any $A\in M_{nn}$ satisfies its own characteristic polynomial $p(t)$: i.e., $p(A)=\boldzero_{n\times n}$. 

Prove the Cayley-Hamilton theorem for {\em diagonalizable} matrices. 
\\
\begin{solution}
%\ \\
\noindent 
First, since $A$ is diagonalizable, we write $D=P^{-1}AP$ where 
\[
D=\begin{bmatrix}
d_1&0&\dots \\
0&d_2&0&\dots &0\\
\vdots \\
0&0&\dots &0&d_n
\end{bmatrix} 
\]
Next, observe that for any polynomial $q(t)$, we easily compute $q(D)$ as follows: 
\[
q(D)=\begin{bmatrix}
q(d_1)&0&\dots \\
0&q(d_2)&0&\dots &0\\
\vdots \\
0&0&\dots &0&q(d_n)
\end{bmatrix} 
\]
That is, when evaluating $q(D)$ we simply get the diagonal matrix whose $j$-th diagonal entry is $q(d_j)$. 

Now let $p(t)$ be the characteristic polynomial of $A$. Since $A$ and $D$ are similar, $p(t)$ is also the characteristic polynomial of $D$, which is clearly $p(t)=(t-d_1)(t-d_2)\cdots (t-d_n)$. From the observation above, we see that 
\[
p(D)=\begin{bmatrix}
p(d_1)&0&\dots \\
0&p(d_2)&0&\dots &0\\
\vdots \\
0&0&\dots &0&p(d_n)
\end{bmatrix} 
=0_{n\times n},
\] 
since clearly $p(d_j)=0$ for all $j$. 

Lastly, we compute 
\[
p(A)=p(PDP^{-1})=Pp(D)P^{-1}=P0_{n\times n}P^{-1}=0_{n\times n}.
\]
(Note that the equality $p(PDP^{-1})=Pp(D)P^{-1}$ follows from Exercise \ref{ex:conjpoly}.)
\\
We have thus shown that $A$ satisfies its own characteristic polynomial, as claimed. 
\end{solution}
\ii Information about three matrices is given below. Find all eigenvalues of each matrix and decide whether it is diagonalizable (you are given enough information in each case).
\bb
\ii $A_1\in M_{33}$, $p(t)=\det(tI-A_1)=t^3-t^2$, $\NS(A_1)=$the plane perpendicular to $(1,0,1)$.  
\ii $A_2\in M_{33}$, $p(t)=\det(tI-A_2)=t^3+t^2-t$.
\ii $A_3\in M_{22}$, $\tr A=4$, $\det A=3$. 
\ee
\begin{solution}
%\ \\
\noindent (a) The eigenvalues of $A$ are $0, 1$. We are given that $\NS(A_1)=W_0$ is a plane, hence 2-dimensional. Since $W_1$ must be 1-dimensional (looking at multiplicity), we have $\dim W_0+\dim W_1=3$. Hence $A$ is diagonalizable. 
\\
(b) The eigenvalues of $A$ are $0, (-1+\sqrt{5})/2, (-1-\sqrt{5})/2$. Since $A$ has three distinct eigenvalues, it is diagonalizable. 
\\
(c) Let $\lambda_1, \lambda_2$ be the roots of $p(t)$. Then we have 
\begin{align*}
\tr A=\lambda_1+\lambda_2&=4\\
\det A=\lambda_1\lambda_2&3
\end{align*}
We easily solve this system of two equations in the two unknown $\lambda_i$ to conclude $\lambda_1=1$, $\lambda_2=3$. Since $A$ has two distinct eigenvalues, it is diagonalizable. 
\end{solution}
\ii A matrix $A\in M_{nn}$ is called {\bf nilpotent} if $A^k=0$ for some $k\geq 1$. 
\\
In what follows, assume $A$ is nilpotent. 
\bb[itemsep=5pt]
\ii Prove that $0$ is the only {\em possible} eigenvalue of $A$. 
\ii Prove that $0$ is indeed an eigenvalue of $A$. 
\\
Thus $0$ is the only eigenvalue of $A$. 
\ee
\begin{solution}
%\ \\
\noindent (a) Suppose $\lambda$ is an eigenvalue of $A$. Then we have $A\boldv=\lambda \boldv$ for some $\boldv\ne 0$. Then $A^k\boldv=\lambda^k\boldv$, as shown earlier. Since $A^k=0_{n\times n}$, we thus have $\boldzero=\lambda^k\boldv$. Since $\boldv$ is {\em nonzero (!!)}, we conclude that $\lambda^k=0$, and hence that $\lambda=0$. 
\\
\noindent 
(b) Now we show that $0$ actually is an eigenvalue. Since $A^k=0_{n\times n}$, we have $\det(A^k)=(\det A)^k=0$. Hence $A$ is not invertible. From the invertibility theorem we conclude that $0$ is an eigenvalue of $A$. 
\end{solution}
\ii A matrix $A\in M_{nn}$ is called {\bf idempotent} if $A^2=A$.
\\
In what follows, assume $A$ is idempotent. 
\bb
\ii Prove that the only {\em possible} eigenvalues of $A$ are $0$ and $1$. 
\ii Prove that either $0$ is an eigenvalue of $A$ or else $A=I_n$. 
\ii Give an example of an idempotent matrix for each of the following three possibilities: only 0 is an eigenvalue; only 1 is an eigenvalue; 0 and 1 are both eigenvalues. 
\ee 
\begin{solution}
%\ \\
\noindent 
(a) Suppose $\lambda$ is an eigenvalue of $A$. Then we have $A\boldv=\lambda \boldv$ for some $\boldv\ne 0$. Then $A^2\boldv=\lambda^2\boldv$ as shown earlier. Since $A^2=A$, we have $A\boldv=\lambda^2\boldv$, and hence that $\lambda^2\boldv=\lambda\boldv$. Since $\boldv$ is {\em nonzero (!!)}, we conclude that $\lambda^2=\lambda$, and hence that $\lambda=0$ or $\lambda=1$. 
\\
(b) Suppose $0$ is not an eigenvalue. Then $A$ is invertible. \\
Then $A^2=A\Rightarrow A^{-1}A^2=A^{-1}A\Rightarrow A=I$. 
\\
(c) Easy. You do it. 
\end{solution}
\ii Let $A\in M_{nn}$ and suppose the characteristic polynomial of $A$ factors as 
\[
p(t)=(t-\lambda)^{m}g(t),
\]
where the $\lambda\in\R$ and $\lambda$ is not a root of $g(t)$.
\\
Recall that we define the algebraic multiplicity of $\lambda$ to be $m$; and we define the geometric multiplicity of $\lambda$ to be $\dim W_\lambda$. 
\\
In this exercise we will show $\dim W_{\lambda}\leq m$.  To do so, set $\dim W_{\lambda}=r$. 
\\
Show that $A$ is similar to a matrix $B$ whose characteristic polynomial is divisible by $(t-\lambda)^r$. 
\\
Conclude that $(t-\lambda)^r$ divides $p(t)$, and hence that $r\leq m$. 
\\
\begin{solution}
%\ \\
\noindent Set $T=T_A$. Suppose $\dim W_\lambda=r$. Let $\boldv_1,\boldv_2,\dots, \boldv_r$ be a basis for $W_\lambda$ and extend to a full basis $B'$ of $\R^n$. Representing $T$ with respect to $B'$ yields 
\[
A'=[T]_{B'}=\begin{bmatrix}
\lambda I_{r}&C\\
0_{(n-r)\times r}&D
\end{bmatrix} 
\]
Here I've represented $A'$ in terms of matrix blocks. Now, since $A$ and $A'$ are just two matrix representations of $T$, they are similar, hence have the same characteristic polynomial. From the block matrix description above it is easy to see that $p(t)=\det(tI-A')$ factors as $p(t)=(t-\lambda)^rq(t)$. Thus the algebraic multiplicity of $\lambda$ is {\em at least} $r=\dim W_\lambda$, as claimed. 
\end{solution}
\ii Each of the matrices below has characteristic polynomial $p(x)=(x-1)^2(x+2)$. 
\\
For each decide whether it is diagonalizable. If yes, provide explicit $P$ and $D$ witnessing this fact. 
\bb[itemsep=5pt, topsep=5pt]
\ii $A=\begin{bmatrix}[rrr] -5&0&3\\ -6&1&3\\ -6&0&4\end{bmatrix}$.
\ii $B=\begin{bmatrix}[rrr]-2&-3&3\\ -3&-3&4\\ -3&-4&5\end{bmatrix}$.
\ee
\begin{solution}
These are the matrices that showed up in a previous exercise, where we did essentially all the necessary work. 

For $A$ we have $\dim W_1+\dim W_{-2}=2+1=3$, so $A$ is diagonalizable. 

For $B$ we have $\dim W_1+\dim W_{-2}=1+1=2<3$ so $B$ is not diagonalizable. 
\end{solution}
\ii Let 
$A=\begin{bmatrix}[rrr] -5&0&3\\ -6&1&3\\ -6&0&4\end{bmatrix}$. Use your work above to do the following to find a matrix $C$ such that $C^3=A$.  
\\
\begin{solution}
From a previous exercise, we know that $A$ has eigenspaces $W_1$ and $W_-2$ with corresponding bases $\{(1,0,2),(0,1,0)\}$ and $\{(1,1,1)\}$. It follows that $D=P^{-1}AP$, where 
\[
D=\begin{bmatrix}[rrr]
1&0&0\\
0&1&0\\
0&0&-2
\end{bmatrix}, \ P=\begin{bmatrix}[rrr]
1&0&1\\
0&1&1\\
2&0&1
\end{bmatrix}
\] 
We first find a cube-root of $D$. This is easy! Take \[
E=\begin{bmatrix}
1&0&0\\
0&1&0\\
0&0&-\sqrt[3]{2}
\end{bmatrix}
\]
for example. Then $C=PEP^{-1}$ satsifies $C^3=(PEP^{-1})^3=PE^3P^{-1}=PDP^{-1}=A$. Thus $C$ is a a cube-root of $A$. Numerically, we have 
\[
C=\begin{bmatrix}[rrr]
1&0&1\\
0&1&1\\
2&0&1
\end{bmatrix}
\begin{bmatrix}
1&0&0\\
0&1&0\\
0&0&-\sqrt[3]{2}
\end{bmatrix}
\begin{bmatrix}[rrr]
1&0&1\\
0&1&1\\
2&0&1
\end{bmatrix}^{-1}=\begin{bmatrix}[rrr]
-1-2\sqrt[3]{2}&0&1+\sqrt[3]{2}\\
-2-2\sqrt[3]{2}&1&1+\sqrt[3]{2}\\
-2-2\sqrt[3]{2}&0&2+\sqrt[3]{2}
\end{bmatrix}.
\]
\end{solution}
\ii Suppose $A$ is $3\times 3$, has determinant 0, and has among its eigenvalues $\lambda=2$ with eigenspace $W_2$ a plane in $\R^3$. 
\bb
\ii Find all eigenvalues of $A$.
\ii Find $\tr(A)$. 
\ii Decide whether $A$ is diagonalizable. 
\ee
\begin{solution}
Consider the characteristic polynomial $p(x)$ of $A$. Observe that $p(x)$ has degree 3. 

Since $W_2$ is has dimension 2, we must have $p(x)=(x-2)^2(x-a)$, where $a$ is possibly a second distinct eigenvalue, though it is also possible that $a=2$. However $\det(A)=0$ implies 0 is an eigenvalue of $A$ (see previous classwork). Thus we must have $a=0$ and $p(x)=x(x-2)^2$. 

The trace of $A$ is the sum of the eigenvalues. Thus $\tr(A)=2+2+0=4$. 

We have $\dim W_2=2$. We have automatically that $\dim W_0\geq 1$ (if $\lambda$ is an eigenvalue, then by definition the eigenspace $W_\lambda$ is nonzero). Since the algebraic multiplicity of $0$ is 1, it follows that $\dim W_0=1$, and $\dim W_2+\dim W_0=3$. This shows $A$ is diagonalizable.  
\end{solution}
\ii Give an example of a $3\times 3$ matrix $A$ satisfying the following conditions:
\bb[(i)]
\ii $\lambda=2$ is an eigenvalue and $(1,0,1), (1,1,1)$ are 2-eigenvectors. 
\ii $\lambda=-1$ is an eigenvalue and $(1,0,-1)$ is a $-1$-eigenvector. 
\ee  
\begin{solution}
Such an $A$ will be diagonalizable (since we have a basis of eigenvectors). Using this fact to reverse engineer $A$, we see that the corresponding $P$ and $D$ would be 
\[
P=\begin{bmatrix}[rrr]
1&1&1\\
0&1&0\\
1&1&-1
\end{bmatrix}, D=\begin{bmatrix}[rrr]
2&0&0\\
0&2&0\\
0&0&-1
\end{bmatrix}.
\]
Since $D=P^{-1}AP$, it follows that 
\[
A=PDP^{-1}=\begin{bmatrix}[rrr]
1/2&0&3/2\\
0&2&0\\
3/2&0&1/2
\end{bmatrix}.
\]
\end{solution}
\ii True or false. If true, provide a proof; if false, give an explicit counterexample. 
\bb
\ii If $A\in M_{nn}$ has less than $n$ distinct eigenvalues, then $A$ is not diagonalizable. 
\ii If $A\in M_{nn}$ has fewer than $n$ linearly independent eigenvectors, then $A$ is not diagonalizable. 
\ii If $A\in M_{nn}$ is diagonalizable, then there is a {\em unique} matrix $P$ such that $P^{-1}AP$ is diagonal. 
\ii If $A\in M_{nn}$ is diagonalizable, then $A^{-1}$ is diagonalizable. 
\ii If $A\in M_{nn}$ is diagonalizable, then $A^T$ is diagonalizable. s
\ee
\begin{solution}
%\ \\
\noindent 
(a) False. Take the identity matrix $I_2$. It is diagonal, hence diagonalizable! 
\\
(b) True. This is the content of our main diagonalization theorem.
\\
(c) False. Take $A=I_2$. Then we have $D=I_2$, and $D=P^{-1}AP$ for {\em any} invertible matrix $P$ whatsoever! 
\\
(d) True. Suppose $D=P^{-1}AP$ is diagonal. Then so is $D^{-1}$, and we have $D^{-1}=P^{-1}A^{-1}(P^{-1})^{-1}=P^{-1}A^{-1}P$. 
\\
(e) True. Suppose $D= P^{-1}AP$ is diagonal. Then so is $D^T$ and we have 
\[
D^T=P^TA^T(P^{-1})^T=P^TA^T(P^T)^{-1}=Q^{-1}AQ,
\]
where $Q=(P^T)^{-1}$. 
\end{solution}
\ee