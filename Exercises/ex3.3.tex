\bb
\ii \label{ex:trace} Given any square matrix $A=[a_{ij}]\in M_{nn}$, we define the {\bf trace} of $A$ as $\tr A=\sum_{i=1}^n a_{ii}$. In other words, the trace $\tr A$ is the sum of the diagonal entries of $A$. 
\\
Let $V=M_{22}$, and define $W=\{A\in M_{22}\colon \tr(A)=0\}$. 
\bb
\ii Prove that $W$ is a subspace. 
\ii Find a spanning set for the subspace 
\[
W=\{A\in M_{22}\colon \tr(A)=0\}.
\]
\ee
\begin{solution}
\ \\
\end{solution}
\ii Let $V=M_{22}$, and define $A_1=\begin{bmatrix}
1&1\\1&1
\end{bmatrix}$, $A_2=\begin{bmatrix}
0&1\\1&0
\end{bmatrix}$, $A_3=\begin{bmatrix}
1&1\\ 1&0
\end{bmatrix}$. 
Compute $W=\Span(\{A_1,A_2,A_3\})$, identifying it as a certain {\em familiar} set of matrices. 
 
%\vfill
\begin{solution}
Using the definition of $\Span$, we compute 
\begin{align*}
\Span\{A_1,A_2,A_3\}&=\{c_1A_1+c_2A_2+c_3A_3\colon c_i\in\R\} \\
&=\left\{\begin{bmatrix} c_1+c_3&c_1+c_2+c_3\\ c_1+c_2+c_3 & c_1\end{bmatrix}\colon c_i\in\R\right\}.
\end{align*}
We claim this last set is none other than the set of all symmetric matrices. It is clear that each element in the set above is symmetric, so it remains only to show that if $A$ is symmetric, then $A$ can be written in the above form for some choice of $c_1,c_2,c_3$. 

Write $A=\begin{bmatrix}a&b\\b&c \end{bmatrix}$. We need to find $c_1,c_2,c_3$ such that 
\[
\begin{bmatrix} c_1+c_3&c_1+c_2+c_3\\ c_1+c_2+c_3 & c_1\end{bmatrix}=\begin{bmatrix}a&b\\b&c \end{bmatrix}
\]
The bottom-right entry tells us that we must pick $c_1=c$. Then the top-left entry tells us that $c_3=a-c$. Lastly, the off diagonal entries tells us that $c_2=b-a$. 

Thus we can write $A=cA_1+(b-a)A_2+(a-c)A_3$, showing $A\in\Span\{A_1,A_2,A_3\}$, and proving that $\Span\{A_1,A_2,A_3\}=(\text{set of symmetric matrices)}$. 
\end{solution}
\ii Let $V=\R^4$ and define $W=\{(x_1,x_2,x_3,x_4)\colon x_1-x_4=x_2+x_3=x_1+x_2+x_3=0\}$. 
\bb
\ii Show $W$ is a subspace by identifying it as $\NS T$ for an explicit linear transformation. 
\ii Describe $W$ in as simple a manner as possible. 
\ee
\begin{solution}
\noindent
(a) The subset $W$ can be described as the set of solutions to the linear system
\[
\begin{linsys}{4}
x_1&+& & & &- &x_4&=0\\
& &x_2&+&x_3& & &=0\\
x_1&+&x_2&+&x_3& & &=0
\end{linsys}\tag{$*$}
\]
We can express this with the matrix equation $A\boldx=\boldzero$, where 
\[
A=\begin{bmatrix}[rrrr]
1&0&0&-1\\
0&1&1&0\\
1&1&1&0
\end{bmatrix}
\]
We see then that $W=\NS T_A=\NS A$. This proves $W$ is a subspace, since $T_A$ is a linear transformation. 
\\
(b) We use Gaussian elimination to help describe $W$. The system $(*)$ has corresponding augmented maqtrix $[A\vert \boldzero]$, which row reduces to 
\[
\begin{bmatrix}[rrrr|r]
\boxed{1}&0&0&-1&0\\
0&\boxed{1}&1&0&0\\
0&0&0&\boxed{1}&0
\end{bmatrix}
\]  
Thus $x_3=t$ is the only free variable, and we back-substitute to find $(x_1,x_2,x_3,x_4)=(0,-t,t,0)$. We conclude that 
$W=\{(0,-t,t,0)\colon t\in\R\}$. 
\end{solution}



\ii Determine which of the following subsets $W$ are subspaces of $M_{nn}$.
\bb
\ii $W=\{\text{diagonal $n\times n$ matrices}\}$ 
\ii $W=\{A\in M_{nn}\colon \det A=0\}$
\ii $W=\{\text{upper triangular $n\times n$ matrices}\}$ 
\ii $W=\{A\in M_{nn}\colon \tr A=0\}$.  (See Exercise \ref{ex:trace} above for the definition of $\tr A$. )
\ii $W=\{\text{symmetric $n\times n$ matrices}\}=\{A\in M_{nn}\colon A^T=A\}$. 
\ii $W=\{A\in M_{nn}\colon A^T = -A\}$. (These are called {\bf skew-symmetric matrices}.)
\ii The set $W$ of all $n$ by $n$ matrices $A$ for which $A\boldx = \bold0$ has only the trivial solution.
\ii The set $W$ of all $n$ by $n$ matrices $A$ such that $AB=BA$, where $B$ is a fixed $n\times n$ matrix.
\ee
\begin{solution}
\noindent
(a) Yes, let
$$ A = 
\begin{bmatrix}
  a_{11} & 0 & \cdots & 0 \\
  0 & a_{22} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & a_{nn} 
 \end{bmatrix}
B = \begin{bmatrix}
  b_{11} & 0 & \cdots & 0 \\
  0 & b_{22} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & b_{nn} 
 \end{bmatrix}
 $$
 Then,
 $$
 A + B = 
  \begin{bmatrix}
  a_{11} + b_{11} & 0 & \cdots & 0 \\
  0 & a_{22}+b_{22} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & a_{nn}+b_{nn} 
 \end{bmatrix}
 \in M
 $$
 And, for some scalar k
 $$
 kA = k\begin{bmatrix}
  a_{11} & 0 & \cdots & 0 \\
  0 & a_{22} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & a_{nn} 
 \end{bmatrix}
 =
 \begin{bmatrix}
  ka_{11} & 0 & \cdots & 0 \\
  0 & ka_{22} & \cdots & 0 \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  0 & 0 & \cdots & ka_{nn} 
 \end{bmatrix}
 \in M
 $$
 \ \\
(b) No, consider
$$
A = 
\begin{bmatrix}[rr]
1&-1\\
-1&1
\end{bmatrix}
B =
\begin{bmatrix}[rr]
3&3\\
3&3
\end{bmatrix}
A+B = 
\begin{bmatrix}[rr]
4&2\\
2&4
\end{bmatrix}
$$
Thus $\det(A) = 0$ and $\det(B)=0$, but $\det(A+B) = 16 - 4 = 12$. So addition is not closed.
\\
(c) Yes. Since $\underset{n\times n}{\boldzero}$ is upper triangular, we have $\underset{n\times n}{\boldzero}\in W$. 
If $A=[a_{ij}]$ and $B=[b_{ij}]$ are upper triangular, then $a_{ij}=b_{ij}=0$ for all $j>1$. Then we also have $a_{ij}+b_{ij}=0$ for all $j>i$, showing that $A+B$ is upper triangular. Hence $W$ is closed under addition. A similar argument shows $W$ is closed under scalar multiplication. 
\\
(d) Yes. We prove by first showing that the map $\tr\colon M_{nn}\rightarrow \R$ is a linear transformation. Since $W=\{A\in M_{nn}\colon \tr A=0\}=\NS \tr$, our null space theorem implies $W$ is a subspace. 

To prove $\tr\colon M_{nn}\rightarrow\R$ is linear, we observe that 
\begin{align*}
\tr(cA+dB)&=\sum_{i=1}^n (cA+dB)_{ii}\\
&=\sum_{i=1}^n c(A)_{ii}+d(B)_{ii}\\
&=c\sum_{i=1}^n(A)_{ii}+d\sum_{i=1}^n(B)_{ii}\\
&=c\tr A+d\tr B
\end{align*} 
\ \\
(e) Yes. We prove this in the usual fashion. Remember: a matrix $A$ is symmetric iff $A^T=A$. Thus $W=\{A\in M_{nn}\colon A^T=A\}$. 
\bb[(i)]
\ii $\boldzero=0_{nn}$ is clearly symmetric. Hence $\boldzero\in W$. 
\ii Suppose $A_1,A_2\in W$. Then $(A_i)^T=A_i$ for $i=1,2$. But then 
\begin{align*}
(A_1+A_2)^T&=A_1^T+A_2^T &\text{ (prop. of transp.)}\\
&=A_1+A_2 &\text{(since $A_i\in W$)}
\end{align*}
Thus $(A_1+A_2)^T=A_1+A_2$, showing $A_1+A_2\in W$. 
\ii Suppose $A\in W$. Then $A^T=A$. But then 
\begin{align*}
(kA)^T&=kA^T &\text{(prop. of transp.)}\\
&=kA &\text{(since $A\in W$)}
\end{align*}
Thus $kA\in W$. 
\ee
\ \\
(f) Yes. The proof of this looks nearly exactly like the one above, only now we are always showing a matrix $B$ satisfies $B^T=-B$. The exact same properties of transpose will be invoked. 
\ \\
(g) No. The zero vector in $M_{nn}$ is $\underset{n\times n}{\boldzero}$, the matrix with all zeros. Clearly the matrix equation  $\underset{n\times n}{\boldzero}\boldx=\boldzero$ has more than just the trivial solution; in fact all vectors $\boldx$ satisfy this equation! Thus $\underset{n\times n}{\boldzero}\notin W$, showing that $W$ is not a subspace. 
\ \\
(h) Yes.
\bb[(i)]
\ii Clearly $\boldzero=0_{nn}$ satisfies $0_{nn}B=B0_{nn}$. Thus $\boldzero\in W$. 
\ii Next, let $A_1,A_2 \in W$. Then
$$
(A_1+A_2)B = A_1B +A_2B = BA_1+BA_2 + B(A_1+A_2)
$$
So, $A_1 + A_2 \in W$. 
\ii Finally, suppose $A_1\in W$ and let $k$ be any scalar. Then
$$
(kA_1)B = k(A_1B) = k(BA_1) = (kB)A_1 = B(kA_1)
$$
Thus $kA_1 \in W$.
\ee
\end{solution}
\ii Determine which of the following are subspaces of $P_3$.
\bb
\ii The set $W$ of all polynomials $a_0+a_1x+a_2x^2+a_3x^3$ for which $a_0 = 0$
\ii The set $W$ of all polynomials $a_0+a_1x+a_2x^2+a_3x^3$ for which $a_0 + a_1 + a_2 + a_3 = 0$.
\ii All polynomials of the form $a_0+a_1x+a_2x^2+a_3x^3$ in which $a_0,a_1,a_2,a_3$ are rational numbers.
\ii All polynomials of the form $a_0 +a_1x$, where $a_0,a_1$ are real numbers.

\ee
\begin{solution}
\noindent (a) 
Yes. 
\bb[(i)]
\ii First off the zero vector $\boldzero$ in this case is the zero polynomial $p(x)=0+0x+0x^2+0x^3$. This is clearly an element of $W$.
\ii Next, suppose $a_0 = b_0 = 0$. Then,
\begin{eqnarray*}
a_0+a_1x+a_2x^2+a_3x^3 + b_0+b_1x+b_2x^2+b_3x^3 &=& (a_0+b_0) + (a_1 +b_1)x + (a_2 +b_2)x^2 + (a_3 + b_3)x^3\\
&=& 0 + (a_1 +b_1)x + (a_2 +b_2)x^2 + (a_3 + b_3)x^3
\end{eqnarray*}
Thus addition is closed. 
\ii Finally, let $k$ be some scalar. Then,
\begin{eqnarray*}
k(a_0+a_1x+a_2x^2+a_3x^3) &=& ka_0+ka_1x+ka_2x^2+ka_3x^3\\
&=& 0 + ka_1x+ka_2x^2+ka_3x^3
\end{eqnarray*}
Thus scalar multiplication is also closed.
\ee
\ \\
(b) Yes. 
\bb[(i)]
\ii First off, clearly the zero element $p(x)=0+0x+0x^2+0x^3$ is in $W$. 
\ii Next, suppose $a_0 + a_1 + a_2 + a_3 = 0$ and $b_0 + b_1 + b_2 + b_3 = 0$. Then,
\begin{eqnarray*}
(a_0 + a_1x + a_2x^2 + a_3x^3) + (b_0 + b_1x + b_2x^2 + b_3x^3)\\ = (a_0+b_0) + (a_1+b_1)x + (a_2+b_2)x^2+(a_3+b_3)x^3
\end{eqnarray*}
So,
\begin{eqnarray*}
(a_0+b_0) + (a_1+b_1) + (a_2+b_2) + (a_3+b_3)\\
= (a_0 + a_1 + a_2 + a_3) + (b_0 + b_1 + b_2 + b_3)\\
=0 + 0 = 0
\end{eqnarray*}
Thus addition is closed. 
\ii Lastly, let $k$ be some scalar. Then,
\begin{eqnarray*}
k(a_0+a_1x+a_2x^2+a_3x^3) = ka_0+ka_1x+ka_2x^2+ka_3x^3
\end{eqnarray*}
So,
$$
ka_0+ka_1+ka_2+ka_3 = k(a_0 + a_1 + a_2 + a_3) = k(0)=0
$$
Thus scalar multiplication is also closed.
\ee

\ \\ (c) No.  Let $p(x)=1+x+x^2$, a polynomial with rational coefficients. Then $\sqrt{2}p(x)=\sqrt{2}+\sqrt{2}x+\sqrt{2}x^2$ does not have rational coefficients, hence does not lie in the set. 
\ \\
(d) Yes. This is just the subspace $P_1$, which we have already shown to be a subspace. 
\end{solution}
\ii Which of the following are subspaces of $F(-\infty,\infty)$?
\bb
\ii The set $W$ of all functions $f$ in $F(-\infty,\infty)$ for which $f(0) = 0$.
\ii All functions $f$ in $F(-\infty,\infty)$ for which $f(0) = 1$.
\ii The set $W$ of all functions $f$ in $F(-\infty,\infty)$ for which $f(-x) = x$.
\ii All polynomials of degree 2.

\ee
\begin{solution}
\noindent 
(a)
Yes. First note that the zero function clearly satisfies the stated condition. Thus $\boldzero\in W$. 

Now suppose $f(0) = 0$ and $g(0) = 0$. Then
$$
(f + g)(0) = f(0) + g(0) = 0 + 0 = 0
$$
And for some scalar $k$,
$$
k(f(0)) = k(0) = 0
$$
\ \\
(b) No not closed under addition. Consider $f(0) = 1$ and $g(0) = 1$, then
$$
(f+g)(0) = f(0)+g(0) = 1 + 1 =2 \neq 1
$$
\ \\
(c) Yes. Again, clearly the zero function satisfies this condition. Thus $\boldzero\in W$. 

Next, suppose  $f(-x) = x$ and $g(-x) = x$, then
$$
(f+g)(x) = f(x) + g(x) = f(-x) +g(-x) = (f+g)(-x)
$$
And for some scalar $k$,
$$
(kf)(x)=k(f(x))=k(f(-x)) = (kf)(-x)
$$
\ \\
(d) No, consider $-x^2$ and $x^2 +3x +1$. Then
$$
-x^2 +(x^2 +3x +1) = 3x +1
$$
which is not a polynomial of degree 2.
\end{solution}
\ii Express the following as linear combinations of $\boldu = (2,1,4)$, $\boldv = (1,-1,3)$, and $\boldw = (3,2,5)$
\bb
\ii $(-9,-7,-15)$
\ii $(6,11,6)$
\ii $(0,0,0)$

\ee
\begin{solution}
\noindent (a) 
\begin{eqnarray*}
\begin{bmatrix}[rrr|r]
2&1&3&-9\\
1&-1&2&-7\\
4&3&5&-15
\end{bmatrix}
&\xrightarrow[]{1/2r_1 - r_2}&
\begin{bmatrix}[rrr|r]
2&1&3&-9\\
0&3/2&-1/2&5/2\\
4&3&5&-15
\end{bmatrix}\\
&\xrightarrow[]{2r_1 - r_3}&
\begin{bmatrix}[rrr|r]
2&1&3&-9\\
0&3/2&-1/2&5/2\\
0&-1&1&-3
\end{bmatrix}\\
&\xrightarrow[]{1/2r_1}&
\begin{bmatrix}[rrr|r]
1&1/2&3/2&-9/2\\
0&3/2&-1/2&5/2\\
0&-1&1&-3
\end{bmatrix}\\
&\xrightarrow[]{2/3r_2}&
\begin{bmatrix}[rrr|r]
1&1/2&3/2&-9/2\\
0&1&-1/3&5/3\\
0&-1&1&-3
\end{bmatrix}\\
&\xrightarrow[]{r_2 + r_3}&
\begin{bmatrix}[rrr|r]
1&1/2&3/2&-9/2\\
0&1&-1/3&5/3\\
0&0&2/3&-4/3
\end{bmatrix}\\
&\xrightarrow[]{3/2r_3}&
\begin{bmatrix}[rrr|r]
1&1/2&3/2&-9/2\\
0&1&-1/3&5/3\\
0&0&1&-2
\end{bmatrix}\\
\end{eqnarray*}
Solving by back substitution,
\begin{eqnarray*}
x_3 &=& -2\\
x_2 &=& 5/3 - 2/3 = 1\\
x_1 &=& -9/2 + 3 - 1/2 = -2
\end{eqnarray*}
Thus
$$
-2\boldu + 1\boldv +(-2)\boldw = (-9,-7,-15)
$$
\ \\
(b) 
\begin{eqnarray*}
\begin{bmatrix}[rrr|r]
2&1&3&6\\
1&-1&2&11\\
4&3&5&6
\end{bmatrix}
&\xrightarrow[]{r_1 \leftrightarrow r_2}&
\begin{bmatrix}[rrr|r]
1&-1&2&11\\
2&1&3&6\\
4&3&5&6
\end{bmatrix}\\
&\xrightarrow[]{2r_1 - r_2}&
\begin{bmatrix}[rrr|r]
1&-1&2&11\\
0&-3&1&16\\
4&3&5&6
\end{bmatrix}\\
&\xrightarrow[]{4r_1 - r_3}&
\begin{bmatrix}[rrr|r]
1&-1&2&11\\
0&-3&1&16\\
0&-7&3&38
\end{bmatrix}\\
&\xrightarrow[]{-1/3r_2}&
\begin{bmatrix}[rrr|r]
1&-1&2&11\\
0&1&-1/3&-16/3\\
0&-7&3&38
\end{bmatrix}\\
&\xrightarrow[]{7r_2 + r_3}&
\begin{bmatrix}[rrr|r]
1&-1&2&11\\
0&1&-1/3&-16/3\\
0&0&2/3&2/3
\end{bmatrix}\\
\end{eqnarray*}
Solving by back substitution,
\begin{eqnarray*}
x_3 &=& 1\\
x_2 &=& -16/3 + 1/3 = -5\\
x_1 &=& 11 -2 -5 = 4
\end{eqnarray*}
Thus
$$
4\boldu + (-5)\boldv +1\boldw = (6,11,6)
$$
\ \\ (c) 
$$0\boldu + 0\boldv + 0\boldw = (0,0,0)$$
\end{solution}

\ii Which are the following are linear combinations of:
$$A =
\begin{bmatrix}[rr]
4&0\\
-2&-2
\end{bmatrix}
\hspace{5pt}, 
B =
\begin{bmatrix}[rr]
1&-1\\
2&3
\end{bmatrix}
\hspace{5pt}, 
C=
\begin{bmatrix}[rr]
0&2\\
1&4
\end{bmatrix}?
$$
\bb
\ii 
$
\begin{bmatrix}[rr]
6&-8\\
-1&-8
\end{bmatrix}
$
\ii 
$
\begin{bmatrix}[rr]
0&0\\
0&0
\end{bmatrix}
$
\ii
$
\begin{bmatrix}[rr]
-1&5\\
7&1
\end{bmatrix}
$

\ee


\begin{solution}
\noindent (a) Yes, this is a linear combination.
\begin{eqnarray*}
\begin{bmatrix}[rrrr]
4&1&0&6\\
0&-1&2&-8\\
-2&2&1&-1\\
-2&3&4&-8
\end{bmatrix}
&\xrightarrow[]{r_1 + 2r_3}&
\begin{bmatrix}[rrrr]
4&1&0&6\\
0&-1&2&-8\\
0&5&2&4\\
-2&3&4&-8
\end{bmatrix}\\
&\xrightarrow[]{r_1 + 2r_4}&
\begin{bmatrix}[rrrr]
4&1&0&6\\
0&-1&2&-8\\
0&5&2&4\\
0&7&8&-10
\end{bmatrix}\\
&\xrightarrow[]{-r_2}&
\begin{bmatrix}[rrrr]
4&1&0&6\\
0&1&-2&8\\
0&5&2&4\\
0&7&8&-10
\end{bmatrix}\\
&\xrightarrow[]{1/4r_1}&
\begin{bmatrix}[rrrr]
1&1/4&0&3/2\\
0&1&-2&8\\
0&5&2&4\\
0&7&8&-10
\end{bmatrix}\\
&\xrightarrow[]{5r_2-r_3}&
\begin{bmatrix}[rrrr]
1&1/4&0&3/2\\
0&1&-2&8\\
0&0&-12&36\\
0&7&8&-10
\end{bmatrix}\\
&\xrightarrow[]{7r_2-r_4}&
\begin{bmatrix}[rrrr]
1&1/4&0&3/2\\
0&1&-2&8\\
0&0&-12&36\\
0&0&-22&66
\end{bmatrix}\\
&\xrightarrow[]{-1/12r_3}&
\begin{bmatrix}[rrrr]
1&1/4&0&3/2\\
0&1&-2&8\\
0&0&1&-3\\
0&0&-22&66
\end{bmatrix}\\
&\xrightarrow[]{22r_3+r_4}&
\begin{bmatrix}[rrrr]
1&1/4&0&3/2\\
0&1&-2&8\\
0&0&1&-3\\
0&0&0&0
\end{bmatrix}\\
\end{eqnarray*}
Now by substitution:
\begin{eqnarray*}
x_3 &=& -3\\ 
x_2 &=& 8 +2(-3) = 2\\
x_1 &=& 3/2 - 1/4(2) = 1
\end{eqnarray*}
\ \\
(b) Yes, this is a linear combination.
$$
0A + 0B +0C = \begin{bmatrix}[rr]
0&0\\
0&0
\end{bmatrix}
$$
\ \\
(c) No, this is not a linear combination
\begin{eqnarray*}
\begin{bmatrix}[rrrr]
4&1&0&-1\\
0&-1&2&5\\
-2&2&1&7\\
-2&3&4&1
\end{bmatrix}
&\xrightarrow[]{1/2r_1+r_3}&
\begin{bmatrix}[rrrr]
4&1&0&-1\\
0&-1&2&5\\
0&5/2&1&13/2\\
-2&3&4&1
\end{bmatrix}\\
&\xrightarrow[]{1/2r_1+r_4}&
\begin{bmatrix}[rrrr]
4&1&0&-1\\
0&-1&2&5\\
0&5/2&1&13/2\\
0&7/2&4&1/2
\end{bmatrix}\\
&\xrightarrow[]{-r_2}&
\begin{bmatrix}[rrrr]
4&1&0&-1\\
0&1&-2&-5\\
0&5/2&1&13/2\\
0&7/2&4&1/2
\end{bmatrix}\\
&\xrightarrow[]{5/2r_2 - r_3}&
\begin{bmatrix}[rrrr]
4&1&0&-1\\
0&1&-2&-5\\
0&0&-6&-19\\
0&7/2&4&1/2
\end{bmatrix}\\
&\xrightarrow[]{7/2r_2 - r_4}&
\begin{bmatrix}[rrrr]
4&1&0&-1\\
0&1&-2&-5\\
0&0&-6&-19\\
0&0&-11&-18
\end{bmatrix}\\
&\xrightarrow[]{-1/6r_3}&
\begin{bmatrix}[rrrr]
4&1&0&-1\\
0&1&-2&-5\\
0&0&1&19/6\\
0&0&-11&-18
\end{bmatrix}\\
&\xrightarrow[]{11r_3 + r_4}&
\begin{bmatrix}[rrrr]
4&1&0&-1\\
0&1&-2&-5\\
0&0&1&19/6\\
0&0&0&101/6
\end{bmatrix}\\
\end{eqnarray*}
The system is not consistent, thus it cannot be a linear combination.
\end{solution}
\ii Express $p(x)=7 +8x + 9x^2$ as a linear combination of
$$
p_1 = 2 + x + 4x^2, p_2=1-x+3x^2, p_3=3+2x+5x^2
$$
\begin{solution}
\noindent
We have $p$ equal to a linear combination of $p_1, p_2, p_3$ if and only if $p=ap_1+bp_2+cp_3$ for some $a,b,c\in\R$ iff 
\[
7 +8x + 9x^2=a(2+x+4x^2)+b(1-x+3x^2)+c(3+2x+5x^2)=(2a+b+3c)+(a-b+2c)x+(4a+3b+5c)x^2.
\]
Equating coefficients, we see that $p$ is a linear combination of $p_1,p_2,p_3$ iff the following system is consistent:
\[
\begin{linsys}{3}
2a&+&b&+&3c&=&7\\
a&-&b&+&2c&=&8\\
4a&+&3b&+&5c&=&9
\end{linsys}
\]
Now perform Gaussian elimination:
\begin{eqnarray*}
\begin{bmatrix}[rrrr]
2&1&3&7\\
1&-1&2&8\\
4&3&5&9
\end{bmatrix}
&\xrightarrow[]{r_1 \leftrightarrow r_2}&
\begin{bmatrix}[rrrr]
1&-1&2&8\\
2&1&3&7\\
4&3&5&9
\end{bmatrix}\\
&\xrightarrow[]{2r_1 - r_2}&
\begin{bmatrix}[rrrr]
1&-1&2&8\\
0&-3&1&9\\
4&3&5&9
\end{bmatrix}\\
&\xrightarrow[]{4r_1 - r_3}&
\begin{bmatrix}[rrrr]
1&-1&2&8\\
0&-3&1&9\\
0&-7&3&23
\end{bmatrix}\\
&\xrightarrow[]{-1/3r_2}&
\begin{bmatrix}[rrrr]
1&-1&2&8\\
0&1&-1/3&-3\\
0&-7&3&23
\end{bmatrix}\\
&\xrightarrow[]{7r_2 + r_3}&
\begin{bmatrix}[rrrr]
1&-1&2&8\\
0&1&-1/3&-3\\
0&0&2/3&2
\end{bmatrix}\\
&\xrightarrow[]{3/2r_3}&
\begin{bmatrix}[rrrr]
1&-1&2&8\\
0&1&-1/3&-3\\
0&0&1&3
\end{bmatrix}\\
\end{eqnarray*}
Now by substitution
\begin{eqnarray*}
c &=& 3\\
b &=& -3 + 1 = -2\\
a &=& 8-6-2 = 0
\end{eqnarray*}
Thus
$$
7 +8x + 9x^2 = 0p_1 -2p_2 + 3p_3
$$
\end{solution}
\ii Determine whether the following polynomials span $P_2$.
$$
p_1= 1-x+2x^2, \ p_2=3+x, \ p_3 = 5 -x +4x^2,\  p_4 = -2-2x+2x^2
$$
\begin{solution} Reasoning exactly as in the previous exercise, we see that a polynomial $p(x)=a+bx+cx^2$ is a linear combination of the $p_i$ if and only if we have $p=k_1p_1+k_2p_2+k_3p_3+k_4p_4$ for some $k_i\in\R$. After expanding the expression on the right and equating coefficients, we conclude that $p$  lies in the span of the $p_i$ if and only if the system 
\[
\begin{linsys}{4}
k_1&+&3k_2&+&5k_3&-&2k_4&=&a\\
-k_1&+&k_2&-&k_3&-&2k_4&=&b\\
2k_0&+&0k_2&+&4k_3&+&2k_4&=&c
\end{linsys}
\]
Now apply Gaussian elimination. 
\begin{eqnarray*}
\begin{bmatrix}[rrrrr]
1&3&5&-2&a\\
-1&1&-1&-2&b\\
2&0&4&2&c
\end{bmatrix}
&\xrightarrow[]{r_2+r_1}&
\begin{bmatrix}[rrrrr]
1&3&5&-2&a\\
0&4&4&-4&a+b\\
2&0&4&2&c
\end{bmatrix}\\
&\xrightarrow[]{r_3-2r_1}&
\begin{bmatrix}[rrrrr]
1&3&5&-2&a\\
0&4&4&-4&a+b\\
0&6&6&-6&c-2a
\end{bmatrix}\\
&\xrightarrow[]{r_3-3/2r_2}&
\begin{bmatrix}[rrrrr]
1&3&5&-2&a\\
0&4&4&-4&a+b\\
0&0&0&0&-7/2a-3/2b+c
\end{bmatrix}
\end{eqnarray*}
We see the system will be consistent if and only if $-7/2a-3/2b+c=0$ (to avoid having a leading 1 in the last column). For example, if we pick $a=0, b=0, c=1$, the system is not consistent. This means the corresponding polynomial, $p(x)=0+0x+1x^2=x^2$ is not in the span of the $p_i$. 
\end{solution}
\ii Let $f(x)= \cos^2x$ and $g(x)= \sin^2x$. Determine whether the following functions lie in the space spanned by $f$ and $g$. If yes, provide an explicit linear combination; if no, show no linear combination exists. (You may want to review your trig identities.) 
\bb
\ii $q(x)=\cos 2x$
\ii $q(x)=3+x^2$
\ii $q(x)=1$
\ii $q(x)=\sin x$
\ii $q(x)=0$
\ee

\begin{solution}
%\ \\
\noindent (a) Yes.
$$
f - g = \cos^2x - \sin^2x = \cos2x
$$
\ \\
(b) No. Assume we can find $k_1, k_2$ such that 
$$
3+x^2 = k_1\cos^2x +k_2\sin^2x
$$
for all $x$. 
\\
\\
Evaluating this identity at $x=0$, we see that we must have $3=k_1$. 
\\ \\
However, evaluating the same identity at $x=\pi$ we see that $3+\pi^2=k_1$. Thus $k_1=3$, and $k_1=3+\pi^2$, a contradiction. 
\\ \\
We conclude there can be no such $k_1, k_2$. 
\ \\
(c) Yes.
$$
f + g = \cos^2x + \sin^2x = 1
$$
\ \\
(d) No, assume we can find $k_1, k_2$ such that 
$$
\sin x = k_1\cos^2x + k_2\sin^2x
$$
for all $x$. Then 
\begin{eqnarray*}
\sin(-x) &=& k_1\cos^2(-x) + k_2\sin^2(-x)\\
&=& k_1\cos^2(x) + k_2\sin^2(x)\\
&=&\sin(x)
\end{eqnarray*}
But it is not the case that $\sin(x)=\sin(-x)$ for all $x$.  Thus there are no such $k_i$, and $\sin(x)$ is not in the span.
\ \\
(e) 
 Yes.
$$
0f + 0g = 0
$$
\end{solution}
\ii Show that the solution vectors of a consistent nonhomogeneous system of $m$ linear equations in $n$ unknowns do not form a subspace of $R^n$.
\\
\begin{solution}
\noindent
Represent the system as a matrix equation of the form 
\[
A\boldx=\boldy,
\]
where $\boldy\ne\boldzero$, since the system is assumed to be nonhomogeneous. 
\\
We can describe the set of solutions as $W=\{\boldx\colon A\boldx=\boldy\}$. Then $W$ is not a subspace as $\boldzero\notin W$. Indeed $A\boldzero=\boldzero\ne\boldy$. 
\end{solution}
\ii Define the function $T\colon P_2\rightarrow \R^2$ by
\\
$
T(p(x))=\begin{bmatrix} p(1)\\ p(-1) \end{bmatrix}.
$
\\
(a) Prove $T$ is a linear transformation. 
\\
(b) Compute  $\NS(T)$ and $\range(T)$ and describe these in as simple a manner as possible. 
\\
\begin{solution} \noindent
(a) 
\begin{align*}
T(cp_1+dp_2)&=\begin{bmatrix} (cp_1+dp_2)(1)\\ (cp_1+dp_2)(-1)\end{bmatrix}\\
&=\begin{bmatrix} cp_1(1)+dp_2(1)\\ cp_1(-1)+dp_2(-1)\end{bmatrix}\\
&=c\begin{bmatrix} p_1(1)\\ p_1(-1)\end{bmatrix}+
d\begin{bmatrix}p_2(1)\\ p_2(-1)\end{bmatrix}\\
&=cT(p_1)+dT(p_2)
\end{align*}
\\
(b) 
\begin{align*}
\NS(T)&=\{p\in P_2\colon T(p)=\boldzero\}\\
&=\{p\in P_2\colon \begin{bmatrix}
p(1)\\ p(-1)
\end{bmatrix}=\begin{bmatrix}
0\\ 0
\end{bmatrix}\}\\
&=\{p\in P_2\colon p(1)=p(-1)=0\}.
\end{align*}
We must now describe all polynomials $p(x)=ax^2+bx+c$ satisfying $p(1)=p(-1)=0$. You could set up the system of equations by evaluating and then solve directly for $a, b, c$. Alternatively, you can observe that a degree 2 polynomial satisfying $p(1)=p(-1)=0$ must {\em factor} as 
\[
p(x)=c(x-1)(x+1)=c(x^2-1)=cx^2-c.\] It follows that $\NS T=\{p(x)\in P_2\colon p(x)=cx^2-c \text{ for some } c\in\R\}=\Span(\{x^2-1\})$. 

\noindent
The range of $T$ is the set of vectors $(b_1,b_2)$ such that $(b_1,b_2)=(p(1),p(-1))$ for some $p(x)=a_0+a_1x+a_2x^2$: i.e., the set of pairs $(b_1,b_2)$ for which the system 
\[
\begin{linsys}{3}
a_0&+&a_1&+&a_2&=&b_1\\
a_0&-&a_1&+&a_2&=&b_2
\end{linsys}
\]
is consisent.  Perform Gaussian elimination on the corresponding augmented matrix:
\[
\begin{bmatrix}[rrr|r]
1&1&1&b_1\\
1&-1&1&b_2
\end{bmatrix}
\xrightarrow{\text{row red.}}
\begin{bmatrix}[rrr|r]
1&1&1&b_1\\
0&1&0&\frac{b_1-b_2}{2}
\end{bmatrix}
\]
We see the system is always consistent, and hence that all $2$-vectors are included in the range: i.e., $\range T=\R^2$.  
\end{solution}
\vspace{.2in}
\ii Define $T\colon P_2\rightarrow P_3$ by $T(p)=q$, where $\ds q(x)=\int_0^x p(t) \ dt$. 
\bb
\ii Prove that $T$ is a linear transformation. 
\ii Compute $\NS(T)$ and $\range(T)$ and describe these spaces in as simple a manner as possible. 
\ee
\begin{solution}
\noindent 
(a) 
\begin{align*}
T(cp+dq)&=\int_0^x cp(t)+dq(t) \ dt &\text{(by def.)}\\
&=c\int_0^x p(t) \ dt+d\int_0^x q(t) \ dt &\text{(by calc.)}\\
&=cT(p)+dT(q) &\text{(by def)}
\end{align*}
\\
(b) For an arbitrary polynomial $p(x)=ax^2+bx+c$, we have $T(p)=\frac{1}{3}ax^3+\frac{1}{2}bx^2+cx$. Clearly $T(p)=\boldzero$ if and only if $a=b=c=0$. Thus $\NS(T)=\{\boldzero\}$. 
\\
We clearly have $\range(T)\subseteq W=\{q(x)\in P_3\colon q(x)=dx^3+ex^2+fx\}$: the set of polynomials whose constant term is zero. Furthermore, given any $q(x)=dx^3+ex^2+fx\in W$, we have $q=T(3dx^2+2ex+f)$, showing that $W\subseteq \range(T)$. We conclude $W=\range(T)$. 
\end{solution}
\ii Define $S\colon M_{nn}\rightarrow M_{nn}$ by $S(A)=A^T-A$. 
\bb
\ii Show $S$ is linear. 
\ii Compute $\NS(S)$ and $\range(S)$ and identify both of these spaces as certain special named families of matrices. Make sure to prove your claims.   
\ee
\begin{solution}
\noindent 
(a) 
\begin{align*}
S(cA+dB)&=(cA+dB)^T-(cA+dB) &\text{(by def)}\\
&=cA^T+dB^T-cA-cB &\text{(transpose prop.)}\\
&=c(A^T-A)+d(B^T-B)&\text{(matrix arith.)}\\
&=cS(A)+dS(B) &\text{(by def)}
\end{align*}
\noindent
(b) $S(A)=\boldzero$ iff $A^T-A=\boldzero$ iff $A^T=A$. Thus $\NS(A)$ is the set of symmetric matrices. 

\noindent
Now we identify $\range T$. Let $B=S(A)=A^T-A$. Then $B^T=(A-A^T)=-(A^T-A)=-B$. This shows $\range(T)$ is a subset of the space of skew-symmetric matrices. Does $\range T$ include {\em all } skew-symmetric matrices? Suppose $B$ is skew, so that $B^T=-B$. Set $A=-\frac{1}{2}B$. Then 
\[
T(A)=(-\frac{1}{2}B)^T-\frac{1}{2}B)=-\frac{1}{2}(B^T-B)=-\frac{1}{2}(-B-B)=B.
\]
This shows directly that $B\in \range T$, and thus that $\range T=\{\text{skew-symmetric $n\times n$ matrices}\}$. 

%Since $\dim\NS(A)=\dim\{\text{sym. matricies}\}=\frac{n(n+1)}{2}$, the rank-nullity theorem implies 
%\[
%\dim\range(T)=n^2-\frac{n(n+1)}{2}=\frac{n(n-1)}{2}=\dim\{\text{skew-symm. matricies}\}.
%\]
%Since $\range(T)\subseteq \{ \text{skew-symm. matricies}\}$ and $\dim\range(T)=\dim\{\text{skew-symm. matricies}$, we must have 
%$\range(T)=\dim\{\text{skew-symm. matricies}$ by the dimension theorem compendium (subspace part). 
\end{solution}
\ii Define $T\colon P_3\rightarrow M_{22}$ by $T(p)=\begin{bmatrix}p(1)&p(-1)\\ p(2)&p(-2)\end{bmatrix}$. 
\bb
\ii Show $T$ is linear. 
\ii 
Compute $\NS(T)$ and $\range(T)$. 

Hint: a nonzero polynomial of degree 3 can have at most 3 roots! 
\ee
\begin{solution}
\noindent (a) 
\begin{align*}
T(cp+dq)&=\begin{bmatrix}(cp+dq)(1)&(cp+dq)(-1)\\ (cp+dq)(2)&(cp+dq)(-2)\end{bmatrix} &\text{(by def.)}\\
&=\begin{bmatrix}cp(1)+dq(1)&cp(-1)+dq(-1)\\ cp(2)+dq(2)&cp(-2)+dq(-2)\end{bmatrix} &\text{(function arith.)}\
\\
&=c\begin{bmatrix}p(1)&p(-1)\\ p(2)&p(-2)\end{bmatrix}+d\begin{bmatrix}q(1)&q(-1)\\ q(2)&q(-2)\end{bmatrix}&\text{(matrix arith.)}\\
&=cT(p)+dT(q) &\text{(by def)}
\end{align*}
\noindent 
(b) We have $T(p)=\boldzero$ iff $p(1)=p(-1)=p(2)=p(-2)=0$ iff $p(x)=\boldzero$, since a nonzero polynomial of degree at most 3 can have at most 3 distinct roots!  Thus $\NS(T)=\{\boldzero\}$ is the trivial subspace.
\\
For given $A=\begin{bmatrix}
a&b\\ c&d
\end{bmatrix}$, deciding whether there is a $p(x)=a_0+a_1x+a_2x^2+a_3$ such that $T(p)=A$ amounts to deciding whether the system 
\[
\begin{linsys}{4}
a_0&+&a_1&+&a_2&+&a_3&=&a\\
a_0&-&a_1&+&a_2&-&a_3&=&b\\
a_0&+&2a_1&+&4a_2&+&8a_3&=&c\\
a_0&-&2a_1&+&4a_2&-&8a_3&=&d
\end{linsys}
\]
is consistent. This in turn can answered by deciding when the following matrix equation can be solved: 
\[
\begin{bmatrix}[rrrr]
1&1&1&1\\
1&-1&1&-1\\
1&2&4&8\\
1&-2&4&-8
\end{bmatrix}
\begin{bmatrix}
a_0\\ a_1\\ a_2\\ a_3
\end{bmatrix}
=\begin{bmatrix}
a\\b\\c\\d
\end{bmatrix}
\]
A determinant computation shows that the given $4\times 4$ matrix is invertible! (In fact this is an instance of the Vandermonde determinant!) By the invertibility theorem, for any choice of $a,b,c,d$ there is {\em always} a solution. Thus $\range T=M_{22}$. 
%The rank-nullity theorem now implies that 
%\[
%\dim\range(T)=\dim P_3-0=4-0=4.
%\]
%Since $\range(T)\subseteq M_{22}$ and $\dim\range(T)=\dim M_{22}$, we must have $\range(T)=M_{22}$. 
\end{solution}
\ii Define the function $T\colon C^\infty(\R)\rightarrow C^\infty(\R)$ by $T(f)=f'$.
\bb
\ii Show $T$ is a linear transformation. 
\ii Compute $\NS(T)$ and $\range T$. 
\ee
\begin{solution}
\ \\ (a) Shown in the notes. To repeat: $T(cf+dg)=(cf+dg)'=cf'+dg'$, using familiar properties of the derivative.  
\\
(b) $\NS(T)=\{f\colon f'=0\}=\{\text{constant functions}\}$, again using a familiar fact from calculus. 

\noindent 
I claim $\range T=C^\infty(\R)$. Indeed, any $g\in C^\infty(\R)$ is continuous, and thus has an antiderivative by the fundamental theorem of calculus. Thus there is a function $f$ such that $f'=g$. Since $g$ is infinitely differentiable, it follows that this $f$ is also infinitely differentiable. Thus we have found an $f\in C^\infty(\R)$ such that $T(f)=f'=g$. 
\end{solution}
\ii Define $T\colon F(-\infty,\infty)\rightarrow F(-\infty, \infty)$ by $T(f)=g$, where $g(x)=f(x)+f(-x)$. 
\\
Show that $T$ is linear, and identify $\NS(T)$ and $\range(T)$ as certain special named types of functions. Make sure to prove your claims. 
\\
\begin{solution}
\noindent Linearity:
\begin{align*}
T(cf+dg)&=(cf+dg)(x)+(cf+dg)(-x) &\text{(by def)}\\
&=c(f(x)+f(-x))+d(g(x)+g(-x)) &\text{(arith)}\\
&=cT(f)+dT(g) &\text{(by def)}
\end{align*}
\noindent $T(f)=\boldzero$ iff $f(x)+f(-x)=\boldzero$ iff $f(-x)=-f(x)$. Thus $\NS(T)$ is the set of odd functions. 

I claim $\range(T)$ is the space of even functions. Indeed, given any $g=T(f)=f(x)+f(-x)\in\range(T)$, we have $g(-x)=f(-x)+f(x)=g(x)$. Thus $\range(T)$ is a subspace of the space of even functions. For the other direction, given even function $g(x)$, set $f(x)=\frac{1}{2}g(x)$. Then 
\begin{align*}
T(f)&=\frac{1}{2}g(x)+\frac{1}{2}g(-x)\\
&=\frac{1}{2}g(x)+\frac{1}{2}g(x) &\text{(since $g$ is even)}\\
&=g(x)
\end{align*}
Thus the space of even functions is a subspace of $\range(T)$. We conclude the two spaces are equal. 
\end{solution} 

\ee