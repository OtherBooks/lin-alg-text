\bb
\ii Recall that the set of solutions $(x,y)$ to a single linear equation in 2 variables constitutes a line $\ell$ in $\R^2$. We denote this $\ell\colon ax+by=c$. 
\\
Similarly, the set of solutions $(x,y,z)$ to a single linear equation in 3 variables $ax+by+cz=d$ constitutes a plane $\mathcal{P}$ in $\R^3$. We denote this $\mathcal{P}\colon ax+by+cz=d$. 
\bb[(a)]
\ii Fix $m>1$ and consider a system of $m$ linear equations in the 2 unknowns $x$ and $y$. What do solutions $(x,y)$ to this \emph{system} of linear equations correspond to geometrically? 
\ii Use your interpretation to give a {\em geometric} argument that a system of $m$ equations in 2 unknowns will have either (i) 0 solutions, (ii) 1 solution, or (iii) infinitely many solutions. 
\ii Use your geometric interpretation to help produce explicit examples of systems in 2 variables satisfying these three different cases (i)-(iii).
\ii Now repeat (a)-(b) for systems of linear equations in 3 variables $x,y, z$.  
\ee 
\ \\
\begin{solution}
\noindent
(a) Geometrically, each equation  in the system represents a line $\ell_i\colon a_ix+b_iy=c_i$. A solution $(x,y)$ to the $i$-th equation corresponds to a point on $\ell_i$. Thus a solution $(x,y)$ to the system corresponds to a point lying on {\em all} of the lines: i.e., a point of intersection of the lines. 
\\
(b) First of all to prove the desired ``or" statement it suffices to prove that if the number of solutions is greater than 1, then there are infinitely many solutions. 
\\
Now suppose there is more than one solution. Then there are at least two different solutions: $P_1=(x_1,y_1)$ and $P_2=(x_2,y_2)$. Take any of the two lines $\ell_i, \ell_j$. By above the intersection of $\ell_i$ and $\ell_j$ contains $P_1$ and $P_2$. But two {\em distinct} lines intersect in at most one point. It follows that $\ell_i$ and $\ell_j$ must be equal. Since $\ell_i$ and $\ell_j$ were arbitrary, it follows {\em all} of the lines $\ell_i$ are in fact the same line $\ell$.  But this means the common intersection of the lines is $\ell$, which has infinitely many points. It follows that the system has infinitely many solutions. 
\\
(c)  We will get 0 solutions if the system includes two different parallel lines: e.g., $\ell_1\colon x+y=5$ and $\ell_2\colon x+y=1$. 
\\
We will get exactly one solution when the slopes of each line in the system are distinct. 
\\
We will get infinitely many solutions when {\em all} equations in the system represent the {\em same line}. This happens when all equations are multiples of one another. 
\\
(d) Now each equation in our system defines a plane $\mathcal{P}_i\colon a_ix+b_iy+c_iz=d_i$. A solution $(x,y,z)$ to the system corresponds to a point $P=(x,y,z)$ of intersection of the planes. We recall two facts from Euclidean geometry:
\bb
\ii[Fact 1] Given two distinct points, there is a unique line containing both of them.
\ii[Fact 2] Given any number of distinct planes, they either do not intersect, or intersect in a line. 
\ee
We proceed as in part (b) above: that is show that if there are two distinct solutions to the system, then there are infinitely many solutions. First, for simplicity, we may assume that the equations $\mathcal{P}_i\colon a_ix+b_iy+c_iz=d_i$ define {\em distinct} planes; if we have two equations defining the same plane, we can delete one of them and not change the set of solutions to the system. 

Now suppose $P=(x_1,y_1,z_1)$ and $Q=(x_2,y_2,z_2)$ are two distinct solutions to the system. Let $\ell$ be the unique line containing $P$ and $Q$ (Fact 1). I claim that $\ell$ is precisely the set of solutions to the system. To see this, take any two equations in the system $\mathcal{P}_i\colon a_ix+b_iy+c_iz=d_i$ and $\mathcal{P}_j\colon a_jx+b_jy+c_iz=d_j$. Since the two corresponding planes are distinct, and intersect in at least the points $P$ and $Q$, they must intersect in a line (Fact 2); since this line contains $P$ and $Q$, it must be the line $\ell$ (Fact 1). Thus any two planes in the system intersect in the line $\ell$. From this it follows that: (a) a point satisfying the system must lie in $\ell$; and (b) all points on $\ell$ satisfy the system (since we have shown that $\ell$ lies in all the planes). It follows that $\ell$ is precisely the set of solutions, and hence that there are infinitely many solutions. 
\end{solution}
\\
\ii
\noindent
 We made the claim that each of our three row operations 
\bb[i]
\ii scalar multiplication ($e_i\mapsto c\cdot e_i$ for $c\ne 0$),
\ii swap ($e_i\leftrightarrow e_j$), 
\ii addition ($e_i\mapsto e_i+c\cdot e_j$ for some $c$) 
\ee
do not change the set of solutions of a linear system.

To prove this claim, let $L$ be a general linear system
\[
 \numeqsys.
\]
Now consider each type of row operation separately, write down the new system $L'$ you get by applying this row operation, and prove that an $n$-tuple $s=(s_1,s_2,\dots ,s_n)$ is a solution to the original system $L$ \framebox{if and only if} it is a solution to the new system $L'$. 
\\
\begin{solution} \noindent
Let $S$ be the original system with equations $e_1,e_2,\dots ,e_m$. \\
For each specified row operation, we will call the resulting new system $S'$ and its equations $e'_1,e'_2,\dots , e'_m$.  

\begin{description}
\ii[Row swap.] In this case systems $S$ and $S'$ have exactly the same equations, just written in a different order. Thus the $n$-tuple $s$ satifies $S$ if and only if it satisfies each of the equations $e_i$, if and only if it satisfies each of the equations $e'_i$, since these are the same equations! It follows that $s$ is a solution of $S$ if and only if it is a solution to $S'$.
\ii[Scalar mult.] In this case $e_j=e'_j$ for all $j\ne i$, and $e'_i=c\cdot e_i$ for some $c\ne 0$. Since only the $i$-th equation has changed, it suffices to show that $s$ is a solution to $e_i$ if and only if $s$ is a solution to $c\cdot e_i$. Let's prove each direction of this if and only if separately. 
\begin{align*}
s \text{ satisfies } e_i&\Rightarrow  a_{ii}s_1+a_{i2}s_2+\cdots +a_{in}s_n=b_i & \text{ (by def.)}\\
&\Rightarrow  ca_{i1}s_1+ca_{i2}s_2+\cdots +ca_{in}s_n=cb_i & \text{ (mult both sides by $c$)}\\
&\Rightarrow  s \text{ satisfies } e'_i &\text{ (by def)}
\end{align*}
Now for the other direction of the if and only if. 
\begin{align*}
s \text{ satisfies } e'_i &\Rightarrow ca_{ii}s_1+ca_{i2}s_2+\cdots ca_{in}s_n=cb_i &\text{ (by def)}\\
&\Rightarrow \frac{1}{c}(ca_{i1}s_1+ca_{i2}s_2+\cdots +ca_{in}s_n)=\frac{1}{c}\cdot cb_i & \text{ (mult both sides by $\frac{1}{c}$)}\\
&\Rightarrow a_{i1}s_1+a_{i2}s_2+\cdots +a_{in}x_n=b_i & \text{ (distribute $\frac{1}{c}$)}\\
&\Rightarrow s \text{ satisfies } e'_i & \text{ (by def.).}
\end{align*}
%As I mentioned in class, we could also have done both directions at once by making a chain of $\Leftrightarrow$'s. However, for this to be convincing, it must be clear that each link in the chain is truly a $\Leftrightarrow$, as opposed to just a $\Rightarrow$ or $\Leftarrow$. Below I show you what this chain looks like, and I highlight the link that is not completely obvious:
%\begin{eqnarray*}
%s \text{ satisfies } e_i&\Rightarrow & a_{ii}s_1+a_{i2}s_2+\cdots +a_{in}s_n=b_i \ \text{ (by def.)}\\
%&\left(\begin{array}{c}\Rightarrow\\
%{\color{red}\Leftarrow?}\end{array} \right)& ca_{ii}s_1+ca_{i2}s_2+\cdots +ca_{in}s_n=cb_i \ \text{ (if $x=y$, then $cx=cy$)}\\
%&\Rightarrow & s \text{ satisfies } e'_i
%\end{eqnarray*}
%Indeed it is the ${\color{red}\Leftarrow?}$ link where we need to use the fact that $c\ne 0$, so this isn't completely obvious! 
\ii[Row addition.] Here $e'_i=e_i+ce_j$. The only equation of $S'$ that differs from $S$ is 
\[
e_i'=e_i+ce_j.
\]
Writing this equation out in terms of coefficients gives us
\[
e_i': a_{i1}x_1+a_{i2}x_2+\cdots +a_{in}x_n+c(a_{j1}x_1+a_{j2}x_2+\cdots +a_{jn}x_n)=b_i+cb_j.
\] 
Now if $s$ satisfies $S$, then it satisfies $e_i$ and $e_j$, in which case evaluating the RHS of the $e_i'$ above at $s$ yields 
\begin{eqnarray*}
a_{i1}s_1+a_{i2}s_2+\cdots +a_{in}s_n+c(a_{j1}s_1+a_{j2}s_2+\cdots +a_{jn}s_n)&=&b_i+cb_j
\end{eqnarray*}
showing that $s$ satisfies $e_i'$. 

Now suppose $s=(s_1,s_2,\dots, s_n)$ satisfies $S'$. Since $s$ satisfies $e_j'=e_j$, we have 
 \[
 a_{j1}s_1+a_{j2}s_2+\cdots +a_{jn}s_n=b_j\tag{$*$}.
 \]
Since $s$ satisfies $e_i'$, we have 
\[
a_{i1}s_1+a_{i2}s_2+\cdots +a_{in}s_n+c(a_{j1}s_1+a_{j2}s_2+\cdots +a_{jn}s_n)=b_i+cb_j
\]
Substituting $(*)$ into the equation above we get 
\[
a_{i1}s_1+a_{i2}s_2+\cdots +a_{in}s_n+c(b_j)=b_i+cb_j,
\]
and hence 
\[
a_{i1}s_1+a_{i2}s_2+\cdots +a_{in}s_n=b_i.
\]
This shows that $s$ satisfies $e_i$. It follows that $s$ satisfies $S$.

\end{description}
\end{solution}
\ \\
\ii
\noindent
Consider the linear system
\begin{eqnarray*}
7x_5&=&2x_3+12\\
2x_1+4x_2-10x_3+6x_4+12x_5&=&28\\
2x_1+4x_2-5x_3+6x_4-5x_5&=&-1
\end{eqnarray*}
Using augmented matrices, try and solve this system by reducing it to a simpler one using row operations. How many solutions (0, 1, or $\infty$) does the system have? 
\\
\begin{solution}
See my lecture notes on Gaussian elimination!
\end{solution}
\ii Compute the set of solutions $S$ to the following system of linear equations:
\[
\begin{linsys}{4}
x_1&+&x_2&-&x_3&+&x_4&=&1\\
-2x_1&-&2x_2&+&2x_3&-&2x_4&=&-2\\
x_1&+&x_2&+&x_3&+&2x_4&=&3
\end{linsys}
\]
Follow the Gaussian elimination algorithm {\em to the letter}, indicating what row operations are used. Make sure to indicate how the $x_i$ are sorted into free and leading variables. 

{\bf Your final answer should be described in set notation}. 

\begin{solution}
\begin{eqnarray*}
\begin{bmatrix}[rrrrr]
1&1&-1&1&1\\
-2&-2&2&-2&-2\\
1&1&1&2&3
\end{bmatrix}
&\xrightarrow[\hspace{35pt}]{r_2+2r_1}&
\begin{bmatrix}[rrrrr]
1&1&-1&1&1\\
0&0&0&0&0\\
1&1&1&2&3
\end{bmatrix}\\
&\xrightarrow[\hspace{35pt}]{r_3-r_1}&
\begin{bmatrix}[rrrrr]
1&1&-1&1&1\\
0&0&0&0&0\\
0&0&2&1&2
\end{bmatrix}\\
&\xrightarrow[\hspace{35pt}]{r_2\leftrightarrow r_3 }&
\begin{bmatrix}[rrrrr]
1&1&-1&1&1\\
0&0&2&1&2\\
0&0&0&0&0
\end{bmatrix}\\
&\xrightarrow[\hspace{35pt}]{\frac{1}{2}r_2}&
\begin{bmatrix}[rrrrr]
\boxed{1}&1&-1&1&1\\
0&0&\boxed{1}&1/2&1\\
0&0&0&0&0
\end{bmatrix}
\end{eqnarray*}
The row echelon matrix tells us that $x_2=s$ and $x_4=t$ are the free variables. Back substitution then yields the general solution:
\begin{eqnarray*}
x_1&=&2-s-\frac{3t}{2}\\
x_2&=&s\\
x_3&=&1-\frac{t}{2}\\
x_4&=&t,
\end{eqnarray*}
Alternatively, the set of solutions is 
\[
S=\left\{(2-s-\frac{3t}{2},s,1-\frac{t}{2},t)\colon s, t\in\R\right\}.
\] 
\end{solution}
\ii
Determine all values of $a$ for which the system has (a) no solutions, (b) exactly one solution, or (c) infinitely-many solutions. 
\[
\begin{linsys}{3}
x&+&2y&-&3z&=&4\\
3x&-&y&+&5z&=&2\\
4x&+&y&+&(a^2-14)z&=&a+2
\end{linsys}
\]
\\
\begin{solution}
The augmented matrix row reduces to 
\[
\begin{bmatrix}[rrr|r]
1&2&3&4\\
0&-7&14&-10\\
0&0&(a^2-16)&a-4
\end{bmatrix}
\]
From this it follows that the system has:
\\
a) 0 solutions iff $a^2-16=0$ and $a-4\ne 0$ iff $\boxed{a=-4}$;
\\
(b) exactly one solution iff $a^2-16\ne 0$ iff $\boxed{a\ne\pm 4}$;
\\
(c) infinitely many solutions iff $a^2-16=0$ and $a-4=0$ iff $\boxed{a=4}$.  
\end{solution}
\ii
Use Gaussian elimination to find the general solution to the following system of linear equations:
\[\large
\begin{linsys}{4}
x_1&+&2x_2&=&x_3&+&x_4&+&3\\
3x_1&+&6x_2&=&2x_3&-&4x_4&+&8\\
-x_1&+&2x_3&=&2x_2&-&x_4&-&1
\end{linsys}
\]
Follow the Gaussian elimination algorithm to the letter, indicating what row operations are used. Make sure to indicate how the $x_i$ are sorted into free and leading variables. 
\\
\begin{solution}
\begin{eqnarray*}
\begin{bmatrix}[rrrrr]
1&2&-1&-1&3\\
3&6&-2&4&8\\
-1&-2&2&1&-1
\end{bmatrix}
&\xrightarrow[\hspace{35pt}]{r_2-3r_1}&
\begin{bmatrix}[rrrrr]
1&2&-1&-1&3\\
0&0&1&7&-1\\
-1&-2&2&1&-1
\end{bmatrix}\\
&\xrightarrow[\hspace{35pt}]{r_3+r_1 }&
\begin{bmatrix}[rrrrr]
1&2&-1&-1&3\\
0&0&1&7&-1\\
0&0&1&0&2
\end{bmatrix}
\\
&\xrightarrow[\hspace{35pt}]{r_3-r_2 }&
\begin{bmatrix}[rrrrr]
1&2&-1&-1&3\\
0&0&1&7&-1\\
0&0&0&-7&3
\end{bmatrix}
\\
&\xrightarrow[\hspace{35pt}]{-\frac{1}{7}r_3}&
\begin{bmatrix}[rrrrr]
\boxed{1}&2&-1&-1&3\\
0&0&\boxed{1}&7&-1\\
0&0&0&\boxed{1}&-\frac{3}{7}
\end{bmatrix}
\end{eqnarray*}
The row echelon matrix tells us that $x_2=t$ is the only free variable. Back substitution then yields the general solution:
\begin{eqnarray*}
x_1&=&\frac{32}{7}-2t\\
x_2&=&t\\
x_3&=&2\\
x_4&=&-\frac{3}{7}.
\end{eqnarray*}
Alternatively, the set of solutions is 
\[
S=\{\left(\frac{32}{7}-2t, t, 2, -3/7\right)\colon t\in\R\}.
\]
\end{solution}
\ii
\noindent
Show that a linear system with more unknowns than equations has has either 0 solutions or infinitely many solutions.
\\
\begin{solution}
\noindent Suppose we have a system of $m$ equations in $n$ unknowns $x_1,x_2,\dots, x_n$. We assume $n>m$. Let $A$ be the augmented matrix associated to the system, and suppose $A$ is reduced to a matrix $U$ in row echelon form. 
\\
Since $U$ has $m$ rows, there are {\em at most} $m$ leading 1's in $U$, which means there are at most $m$ leading variables among the $x_i$. Since $n>m$, not all the $x_i$ can be leading. Thus the system {\em must} have a free variable. 
\\
What does this mean? Note that the system could still be inconsistent, meaning no solutions. However, the existence of a free variable means if there is a solution, then there are infinitely many, because the parametric equations for the $x_i$ will involve at least one parameter. 
\\
We conclude that the system is either inconsistent, or has infinitely many solutions. 
\end{solution}
\ii
\noindent
True or false. If true you must prove it so; if false, you must give an explicit counterexample. 
\bb
\ii Every matrix has a unique row echelon form. 
\ii Any homogeneous linear system with more unknowns than equations has infinitely many solutions. 
\ii The only solution to a homogeneous system of $n$ equations in $n$ unknowns with $n$ leading 1's is the trivial solution $s=(0,0,\dots,0)$. 
\ee
 \begin{solution}\noindent
(a) False. Let $A=\begin{bmatrix}1&1\\ 0&1 \end{bmatrix}$. Then $A$ is already in row echelon form, but can be further reduced to $I=\begin{bmatrix}1&0\\0&1\end{bmatrix}$, which is also in row echelon form. Thus $A$ and $I$ are two different row echelon forms of $A$.
\\
(b) True. Since its homogeneous, it is consistent: thus 1 or $\infty$-many solutions. Furthermore, we showed above that a system with more variables than equations has a free variable. Thus it must have infinitely many solutions. 
\\
(c) True. First note that the given $s$ is always a solution. Since the system has $n$ leading ones, there will be no free variables, which implies this is the only solution. 
\end{solution}
\ii \label{1.1-1.2:A}
\noindent Use Gaussian elimination to solve the following system:
\[
\begin{linsys}{3}
2x_1 &+& 2x_2  &+&2x_3&=&0\\
-2x_1 &+& 5x_2 &+&2x_3&=&1\\
8x_1 &+&  x_2   &+&4x_3&=&-1
\end{linsys}
\]
\\
\begin{solution}
\begin{eqnarray*}
\begin{bmatrix}[rcl|r]
2&2&2&0\\
-2&5&2&1\\
8&1&4&-1
\end{bmatrix}
&\xrightarrow[]{\frac{1}{2}r_1}&
\begin{bmatrix}[rcl|r]
1&1&1&0\\
-2&5&2&1\\
8&1&4&-1
\end{bmatrix}
\\
&\xrightarrow[]{r_2+2r_1}&
\begin{bmatrix}[rcl|r]
1&1&1&0\\
0&7&4&1\\
8&1&4&-1
\end{bmatrix}
\\
&\xrightarrow[]{r_3-8r_1}&
\begin{bmatrix}[rcl|r]
1&1&1&0\\
0&7&4&1\\
0&-7&-4&-1
\end{bmatrix}
\\
&\xrightarrow[]{\frac{1}{7}r_1}&
\begin{bmatrix}[rcl|r]
1&1&1&0\\
0&1&\frac{4}{7}&\frac{1}{7}\\
0&-7&-4&-1
\end{bmatrix}
\\
&\xrightarrow[]{r_3+7r_2}&
\begin{bmatrix}[rcl|r]
1&1&1&0\\
0&1&\frac{4}{7}&\frac{1}{7}\\
0&0&0&0
\end{bmatrix}
\end{eqnarray*}
Now solve. We set the free variable $x_3=r$ and do back substitution: 
\begin{eqnarray*}
x_1&=&-\frac{1}{7}-\frac{3}{7}r \\
x_2&=&\frac{1}{7}-\frac{4}{7}r \\
x_3&=& r
\end{eqnarray*}
\end{solution}
\ii
\noindent Solve by Gaussian elimination:
\[
\begin{linsys}{3}
&&-2b  &+&3c&=&1\\
3a &+& 6b &-&3c&=&-2\\
6a &+&  6b   &+&3c&=&5
\end{linsys}
\]

 
\ \\
\begin{solution}
\begin{align*}
\begin{bmatrix}[rcl|r]
0&-2&3&1\\
3&6&-3&-2\\
6&6&3&5
\end{bmatrix}
&\xrightarrow[]{r_1 \leftrightarrow r_2}
\begin{bmatrix}[rcl|r]
3&6&-3&-2\\
0&-2&3&1\\
6&6&3&5
\end{bmatrix}
\\
&\xrightarrow[]{r_3-2r_1}
\begin{bmatrix}[rcl|r]
3&6&-3&-2\\
0&-2&3&1\\
0&-6&9&9
\end{bmatrix}\\
&\xrightarrow[]{-\frac{1}{2}r_2}
\begin{bmatrix}[rcl|r]
3&6&-3&-2\\
0&1&-\frac{3}{2}&-\frac{1}{2}\\
0&-6&9&9
\end{bmatrix}\\
&\xrightarrow[]{r_3+6r_2}
\begin{bmatrix}[rcl|r]
3&6&-3&-2\\
0&1&-\frac{3}{2}&-\frac{1}{2}\\
0&0&0&6
\end{bmatrix}
\end{align*}
And since $0 \neq 6$, the system is inconsistent.
\end{solution}
\ii 
Solve the following system using  Gauss-Jordan elimination: that is, first reduce the corresponding augmented matrix to reduced row echelon form. 
\[
\begin{linsys}{3}
2x_1&+&2x_2&+&2x_3&=&0\\
-2x_1&+&5x_2&+&2x_3&=&1\\
8x_1&+&x_2&+&4x_3&=&-1
\end{linsys}
\]
\noindent
\begin{solution}
\noindent The corresponding augmented matrix is 
\[
\begin{bmatrix}[rrr|r]
2&2&2&0\\
-2&5&2&1\\
8&1&4&-1
\end{bmatrix},
\]
which row reduces first to 
\[
\begin{bmatrix}[rcl|l]
1&1&1&0\\
0&1&\frac{4}{7}&\frac{1}{7}\\
0&0&0&0
\end{bmatrix}
\]
and then further to 
\[
\begin{bmatrix}[rcl|r]
1&0&\frac{3}{7}&-\frac{1}{7}\\[1ex]
0&1&\frac{4}{7}&\frac{1}{7}\\[1ex]
0&0&0&0
\end{bmatrix}
\]
The corresponding system has solution set 
\[
S=\left\{\left(-\frac{1}{7}-\frac{3}{7}r,\frac{1}{7}-\frac{4}{7}r ,r\right)\colon r\in \R\right\}
\]
as in Exercise \ref{1.1-1.2:A}.
\end{solution}
\ii 
Solve the following system of equations by any method:
\[
\begin{linsys}{5}
& & && Z_3&+&Z_4&+&Z_5 &=&0\\
-Z_1&-&Z_2 &+&2Z_3 &-&3Z_4&+&Z_5 &=&0\\
Z_1&+ &Z_2 &-&2Z_3 &&&-&Z_5 &=&0\\
2Z_1&+ &2Z_2 &-&Z_3 &&&+&Z_5 &=&0\\
\end{linsys}
\]
\ \\
\begin{solution}
\begin{eqnarray*}
\begin{bmatrix}[rcccl|l]
0&0&1&1&1&0\\
-1&-1&2&-3&1&0\\
1&1&-2&0&-1&0\\
2&2&-1&0&1&0
\end{bmatrix}
&\xrightarrow[]{-r_2 \leftrightarrow r_1}
\begin{bmatrix}[rcccl|l]
1&1&-2&3&-1&0\\
0&0&1&1&1&0\\
1&1&-2&0&-1&0\\
2&2&-1&0&1&0
\end{bmatrix}\\
&\xrightarrow[]{r_3-r_1}
\begin{bmatrix}[rcccl|l]
1&1&-2&3&-1&0\\
0&0&1&1&1&0\\
0&0&0&-3&0&0\\
2&2&-1&0&1&0
\end{bmatrix}\\
&\xrightarrow[]{r_4-2r_1}
\begin{bmatrix}[rcccl|l]
1&1&-2&3&-1&0\\
0&0&1&1&1&0\\
0&0&0&-3&0&0\\
0&0&3&-6&3&0
\end{bmatrix}\\
&\xrightarrow[]{r_1-2r_2}
\begin{bmatrix}[rcccl|l]
1&1&0&5&1&0\\
0&0&1&1&1&0\\
0&0&0&-3&0&0\\
0&0&3&-6&3&0
\end{bmatrix}\\
&\xrightarrow[]{r_4-3r_2}
\begin{bmatrix}[rcccl|l]
1&1&0&5&1&0\\
0&0&1&1&1&0\\
0&0&0&-3&0&0\\
0&0&0&-9&0&0
\end{bmatrix}\\
&\xrightarrow[]{-\frac{1}{3}r_3}
\begin{bmatrix}[rcccl|l]
1&1&0&5&1&0\\
0&0&1&1&1&0\\
0&0&0&1&0&0\\
0&0&0&-9&0&0
\end{bmatrix}\\
&\xrightarrow[]{r_4+9r_3}
\begin{bmatrix}[rcccl|l]
1&1&0&5&1&0\\
0&0&1&1&1&0\\
0&0&0&1&0&0\\
0&0&0&0&0&0
\end{bmatrix}\\
&\xrightarrow[]{r_2-r_3}
\begin{bmatrix}[rcccl|l]
1&1&0&5&1&0\\
0&0&1&0&1&0\\
0&0&0&1&0&0\\
0&0&0&0&0&0
\end{bmatrix}\\
&\xrightarrow[]{r_1-5r_3}
\begin{bmatrix}[rcccl|l]
1&1&0&0&1&0\\
0&0&1&0&1&0\\
0&0&0&1&0&0\\
0&0&0&0&0&0
\end{bmatrix}\\
\end{eqnarray*}
Now solve. We set the free variables $x_2=r$ and $x_5 = s$ and substitute: 
\begin{eqnarray*}
Z_1&=&-r-s \\
Z_2&=&r \\
Z_3&=&-s \\
Z_4&=&0 \\
Z_5&=&s
\end{eqnarray*}
\end{solution}
\ii
Interpret each matrix below as an augmented matrix of a linear system. Asterisks represent an unspecified real number. For each matrix, determine whether the corresponding system is consistent or inconsistent. 
\vspace{.1in}
\\
If the system is consistent, decide further whether the solution is unique or not. 
\vspace{.1in}
\\
If there is not enough information answer `inconclusive' and back up your claim by giving an explicit example where the system is consistent, and an explicit example where the system is inconsistent. 
\vspace{.1in}
\\
\bb
\ii $\begin{bmatrix}[cccc]
1&*&*&*\\
0&1&*&*\\
0&0&1&1
\end{bmatrix}
$
\vspace{.1in}
\\
\ii $\begin{bmatrix}[cccc]
1&0&0&*\\
*&1&0&*\\
*&*&1&*
\end{bmatrix}
$
\vspace{.1in}
\\
\ii $\begin{bmatrix}[cccc]
1&0&0&0\\
1&0&0&1\\
1&*&*&*
\end{bmatrix}
$
\vspace{.1in}
\\
\ii $\begin{bmatrix}[cccc]
1&*&*&*\\
1&0&0&1\\
1&0&0&1
\end{bmatrix}
$


\ee
\begin{solution}\ \\
(a) The corresponding system is consistent since the row echelon form of the augmented matrix has no leading 1 in the last column. Since the three columns corresponding to the three variables all have leading 1's, there are no free variables. Hence the system has a unique solution. 
\\
(b) This system has a unique solution. You can see this either by noting that the ``reverse staircase pattern" allows us to do ``forwards substitution", solving first for $x_1$, then for $x_2$, etc., or else by noting that the 1's along the diagonal (and 0's above them) allow us to row reduce the matrix further to one have exactly three leading 1's in the first three columns.  
\\
(c)
Inconsistent. Rows 1 and 2 give
$$ x_1 = 0 \hspace{7mm} x_1 = 1$$
\\
(d)
Inconclusive. Consider
\begin{eqnarray*}
\begin{bmatrix}[rccl]
1&a&b&c\\
1&0&0&1\\
1&0&0&1
\end{bmatrix}
\end{eqnarray*}
If $a=b=0$ and $c=2$ the system is inconsistent: the matrix row reduces to one with a leading 1 in the last column.  If $a = b = 0$ and $c=1$, the system has infinitely many solutions: the matrix row reduces to one with a leading 1 in the first column only.  
\end{solution}
\ii
Determine the values of $a$ for which the system below has  no solutions, many solutions, or one solution.
\[
\begin{linsys}{3}
x&+&2y&+&z&=&2\\
2x&-&2y&+&3z&=&1\\
x&+&2y&-&(a^2-3)z&=&a
\end{linsys}
\]
\\
\begin{solution}
\begin{eqnarray*}
\begin{bmatrix}[rccl]
1&2&1&2\\
2&-2&3&1\\
1&2&3-a^2&a
\end{bmatrix}
&\xrightarrow[]{r_1 - r_3}
\begin{bmatrix}[rccc]
1&2&1&2\\
2&-2&3&1\\
0&0&a^2-2&2-a
\end{bmatrix}\\
&\xrightarrow[]{2r_1 - r_2}
\begin{bmatrix}[rccc]
1&2&1&2\\
0&6&-1&3\\
0&0&a^2-2&2-a
\end{bmatrix}\\
&\xrightarrow[]{\frac{1}{6}r_2}
\begin{bmatrix}[rccc]
1&2&1&2\\
0&1&-\frac{1}{6}&\frac{1}{2}\\
0&0&a^2-2&2-a
\end{bmatrix}
\end{eqnarray*}
\newline
The row echelon form, and thus the set of solutions, now depends on whether $a^2-2=0$ or not: equivalently, whether $a=\pm\sqrt{2}$ or not. This gives us two cases:
\begin{description}
\item[$a=\pm\sqrt{2}$] In this case $2-a\ne 0$, which means the row echelon matrix will end up having a leading 1 in the last column, resulting in an inconsistent system. There are no solutions in this case. 
\item[$a\ne \pm\sqrt{2}$] In this case the third column of the row echelon form will have a leading 1, and all variables are leading variables. Thus there is a unique solution in this case, obtained by back substitution. 
\end{description}
\end{solution}
\ii\noindent
What condition must a,b, and c satisfy for the system to be consistent?
\[
\begin{linsys}{3}
x&+&3y&+&z&=&a\\
-x&-&2y&+&z&=&b\\
3x&+&7y&-&z&=&c
\end{linsys}
\]
 
\ \\
\begin{solution}
\begin{eqnarray*}
\begin{bmatrix}[rccl]
1&3&1&a\\
-1&-2&1&b\\
3&7&-1&c
\end{bmatrix}
&\xrightarrow[]{r_1 + r_2}&
\begin{bmatrix}[rccl]
1&3&1&a\\
0&1&2&a+b\\
3&7&-1&c
\end{bmatrix}\\
&\xrightarrow[]{3r_1 - r_3}&
\begin{bmatrix}[rccc]
1&3&1&a\\
0&1&2&a+b\\
0&2&4&3a-c
\end{bmatrix}\\
\newline
&\xrightarrow[]{2r_2 - r_3}&
\begin{bmatrix}[rccc]
1&3&1&a\\
0&1&2&a+b\\
0&0&0&a-2b-c
\end{bmatrix}
\end{eqnarray*}
\newline
We see the system is consistent as long as  
$a-2b-c = 0$, which guarantees there is no leading 1 in the last column. 
\end{solution}
\ii
\noindent Solve for $x$, $y$, and $z$:
\[
\begin{linsys}{3}
\frac{1}{x}&+&\frac{2}{y}&-&\frac{4}{z}&=&1\\ \\
\frac{2}{x}&+&\frac{3}{y}&+&\frac{8}{z}&=&0\\ \\
-\frac{1}{x}&+&\frac{9}{y}&+&\frac{10}{z}&=&5
\end{linsys}
\]
\\
\begin{solution}
Start by replacing variables.  Let $X = \frac{1}{x}$, $Y = \frac{1}{y}$, and $Z = \frac{1}{z}.$ Now we can solve the new system as we normally would.
\newline
\begin{eqnarray*}
\begin{bmatrix}[rccc]
1&2&-4&1\\
2&3&8&0\\
-1&9&10&5
\end{bmatrix}
&\xrightarrow[]{r_1 + r_3}
\begin{bmatrix}[rccc]
1&2&-4&1\\
2&3&8&0\\
0&11&6&6
\end{bmatrix}\\
&\xrightarrow[]{2r_1 - r_2}
\begin{bmatrix}[rccc]
1&2&-4&1\\
0&1&-16&2\\
0&11&6&6
\end{bmatrix}\\
&\xrightarrow[]{11r_2 - r_3}
\begin{bmatrix}[rccc]
1&2&-4&1\\
0&1&-16&2\\
0&0&-182&16
\end{bmatrix}
\end{eqnarray*}
\newline
Now solve the system for $X, Y, Z$:
\begin{eqnarray*}
X&=&-\frac{7}{13} \\
Y&=&\frac{54}{91} \\
Z&=&-\frac{8}{91} 
\end{eqnarray*}

Now we solve for the original $x, y,$ and $z$:
\begin{eqnarray*}
x&=&-\frac{13}{7} \\
y&=&\frac{91}{54} \\
z&=&-\frac{91}{8} 
\end{eqnarray*}
\end{solution}
\ii
\noindent If $A$ is a matrix with three rows and five columns, then what is the maximum possible number of leading 1's in its reduced row echelon form?
\\
\begin{solution}
\noindent The maximum possible number of leading 1's in the reduced row echelon form of a matrix with 3 rows and 5 columns is 3. It is indeed possible to obtain this maximal number, as the matrix 
\[
\begin{bmatrix}
1&0&0&0&0\\
0&1&0&0&0\\
0&0&1&0&0
\end{bmatrix}
\]
illustrates. 
\end{solution}
\ii
\noindent If $B$ is a matrix with three rows and six columns, then what is the maximum possible number of parameters in the general solution of the linear system with augmented matrix $B$?
\\
\begin{solution}
\noindent The matrix $B$ corresponds to a linear system of 3 equations in 5 unknowns $x_1, x_2, \dots, x_5$. 
 
Let $U$ be a row echelon form of $B$, and let $k$ be the number of leading 1's {\em among the first five columns} of $U$. Then the number of parameters in the general solution to the system corresponding to $B$ is $5-k$. Thus we see, that the number of parameters is at most 5 (when $k=5$). 

This case is indeed possible, as the matrix $B=\underset{3\times 6}{\boldzero}$ illustrates.  
\end{solution}
\ii
\noindent If $C$ is a matrix with five rows and three columns, then what is the minimum possible number of rows of zeros in any row echelon form of $C$?
 \\
\begin{solution}
\noindent If a row echelon form of $A$ has $r$ zero rows, then all other rows have leading 1's. Thus there are $5-r$ leading 1's in this case. Since the number of leading 1's is at most 3 (the number of columns), we have $5-r\leq 3$. It follows that $2\leq r$, and thus there are at least $2$ zero rows in a row echelon form of $A$. It is indeed possible to achieve this minimum number of zero rows, as the matrix \[
U=\begin{bmatrix}
1&0&0\\
0&1&0\\
0&0&1\\
0&0&0\\
0&0&0
\end{bmatrix}
\]
illustrates. 
\end{solution}
\ii
\noindent True or False. If true, prove it; if false, give an explicit counterexample.
\\
(a) If a matrix is in reduced row echelon form, then it is also in row echelon form.
\\
(b) If an elementary row operation is applied to a matrix that is in row echelon form, the resulting matrix will still be in row echelon form.
\\
(c) Every matrix has a unique row echelon form.
\\
(d) A homogeneous linear system in $n$ unknowns whose corresponding augmented matrix has a reduced row echelon form with $r$ leading 1's has $n-r$ free variables.
\\
(e) All leading 1's in a matrix in row echelon form must occur in different columns.
\\
(f ) If every column of a matrix in row echelon form has a leading 1, then all entries that are not leading 1's are zero.
\\
(g) If a homogeneous linear system of $n$ equations in $n$ unknowns has a corresponding augmented matrix with a reduced row echelon form containing $n$ leading 1's, then the linear system has only the trivial solution.
\\
(h) If the reduced row echelon form of the augmented matrix for a linear system has a row of zeros, then the system must have infinitely many solutions.
\\
(i) If a linear system has more unknowns than equations, then it must have infinitely many solutions.
\ \\
\begin{solution} \ \\
(a) True. A matrix in reduced row echelon form satisfies all conditions for row echelon form, plus an additional condition. 
\\
(b) False. Consider \[
\begin{bmatrix}1&0\\0&1\end{bmatrix}\xrightarrow[]{r_2+r_1}\begin{bmatrix}
1&1\\ 1&1
\end{bmatrix}
\]
\noindent
(c) False. Consider $A=\begin{bmatrix}1&1\\0&1\end{bmatrix}$, which is already in row echelon form. It is further row equivalent to $B=\begin{bmatrix}1&0\\0&1\end{bmatrix}$.
\\
(d) True. Let $A$ be the augmented matrix corresponding to the system, let $U$ be a row echelon form of $A$, and let $r$ be the number of pivot columns of $U$. Both matrices have $n+1$ columns, and the last column of each is a zero column. This means the system is guaranteed to be consistent (i.e., that the $r$ pivot columns are among the first $n$ columns of $U$), and hence that the number of free variables is $n-r$. 
\\
(e) True. This is part of the definition of row echelon form. The leading 1's must form a staircase pattern. 
\\
(f) False. Consider $\begin{bmatrix}1&1\\0&1\end{bmatrix}$.
\\
(g) True. The system is consistent, since homogeneous systems are always consistent, and the number of free variables is $n-\#\text{(pivot columns)}=n-n=0$. Hence there are no free variables, which means the system has a unique solution. 
\\
(h) False. The system could be inconsistent, in which case there are 0 solutions! Consider the system corresponding to the matrix $A=\begin{bmatrix}0&1\\0&0\end{bmatrix}$. 
\\
(i) False. Consider the system 
\[
\begin{linsys}{3}
0x_1&+&0x_2&=&1
\end{linsys}
\]
The system is clearly inconsistent. Hence it has 0 solutions, not infinitely many. 
\end{solution}
\ee


