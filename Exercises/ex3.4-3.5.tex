\bb
\ii Let $V=\R^n$. Let $S=\{\boldv_1,\boldv_2,\dots ,\boldv_r\}$ be a set of $r$ vectors. Prove (using some Gaussian elimination theory) that if $r>n$, then $S$ is linearly dependent. 

In other words: in $\R^n$ you can have {\em at most} $n$ linearly independent vectors.  
\\
\begin{solution}
\noindent Write 
\[
\boldv_j=\begin{bmatrix}a_{1j} \\a_{2j} \\ \vdots \\ a_{nj}
\end{bmatrix}
\]
Let $A=[a_{ij}]$ be the matrix obtained by placing $\boldv_j$ as the $j$-th column. This is a $n\times r$ matrix, $r>n$. 

Now observe that 
\begin{align*}
c_1\boldv_1+c_2\boldv_2+\cdots +c_r\boldv_r=\boldzero
&\Leftrightarrow A\begin{bmatrix}
c_1\\c_2 \\ \vdots \\ c_r \end{bmatrix}
=\begin{bmatrix}0\\ 0 \\ \vdots \\ 0  \end{bmatrix}\\
&\Leftrightarrow (c_1,c_2,\dots, c_r) \text{ is a solution to } A\boldx=\boldzero
\end{align*}
But the linear system corresponding to $A\boldx=\boldzero$ has $n$ equations in $r$ unknowns. Since $r>n$, there are more unknowns than equations. Gaussian elimination now tells us that this system has a free variable, and hence infinitely many solutions. In particular, there is a nontrivial solution, which proves the vectors $\boldv_j$ are linearly dependent. 
\end{solution}
\ii Let $V=M_{22}$, and define $A_1=\begin{bmatrix}
1&1\\1&1
\end{bmatrix}$, $A_2=\begin{bmatrix}
0&1\\1&0
\end{bmatrix}$, $A_3=\begin{bmatrix}
1&1\\ 1&0
\end{bmatrix}$. 
\bb
\ii Compute $W=\Span(\{A_1,A_2,A_3\})$, identifying it as a certain {\em familiar} set of matrices. 
\ii Decide whether $S=\{A_1,A_2,A_3\}$ is independent. 
\ee
%\vfill
\begin{solution}
\ \\
(a) Using the definition of $\Span$, we compute 
\begin{align*}
\Span\{A_1,A_2,A_3\}&=\{c_1A_1+c_2A_2+c_3A_3\colon c_i\in\R\} \\
&=\left\{\begin{bmatrix} c_1+c_3&c_1+c_2+c_3\\ c_1+c_2+c_3 & c_1\end{bmatrix}\colon c_i\in\R\right\}.
\end{align*}
We claim this last set is none other than the set of all symmetric matrices. It is clear that each element in the set above is symmetric, so it remains only to show that if $A$ is symmetric, then $A$ can be written in the above form for some choice of $c_1,c_2,c_3$. 

Write $A=\begin{bmatrix}a&b\\b&c \end{bmatrix}$. We need to find $c_1,c_2,c_3$ such that 
\[
\begin{bmatrix} c_1+c_3&c_1+c_2+c_3\\ c_1+c_2+c_3 & c_1\end{bmatrix}=\begin{bmatrix}a&b\\b&c \end{bmatrix}
\]
The bottom-right entry tells us that we must pick $c_1=c$. Then the top-left entry tells us that $c_3=a-c$. Lastly, the off diagonal entries tells us that $c_2=b-a$. 

Thus we can write $A=cA_1+(b-a)A_2+(a-c)A_3$, showing $A\in\Span\{A_1,A_2,A_3\}$, and proving that $\Span\{A_1,A_2,A_3\}=(\text{set of symmetric matrices)}$. 

(b) From the above description of a general linear combination $c_1A_1+c_2A_2+c_3A_3$, it is clear that his will be the zero matrix iff $c_1=c_2=c_3=0$. Thus $S$ is linearly independent. 
\end{solution}

\ii Let $A=\begin{bmatrix}[rrrr]1&1&1&1\\ 1&1&-1&-2
\end{bmatrix}$. Find a basis for $W=\NS(A)$ and compute its dimension. You should first use GE to solve the defining matrix equation of $\NS(A)$. 
\\
\begin{solution}
\noindent
Using GE techniques we see that the set of solutions to $A\boldx=\boldzero$ is given by 
\[
W=\{(\frac{t}{2}-s ,s, -\frac{3t}{2},t)\colon s,t\in\R\}
\]
To find a basis for $W$ we express this as a linear combination of two vectors:
\begin{align*}
W&=\{(\frac{t}{2}-s ,s, -\frac{3t}{2},t)\colon s,t\in\R\}\\
&=\{s(-1,1,0,0)+t\left(\frac{1}{2},0,-\frac{3}{2},1\right)\colon s,t\in\R\}\\
&=\Span\left\{\left(-1,1,0,0\right),\left(\frac{1}{2},0,\frac{3}{2},1\right)\right\}
\end{align*}
This shows that $\boldv_1=(-1,1,0,0)$ and $\boldv_2=(1/2,0,-3/2,1)$ span $W=\NS(A)$. Since they are clearly linearly independent, they form a basis. 

Since $W=\NS(A)$ has a basis consisting of two vectors, we conclude $\dim(W)=2$. 
\end{solution}
\ii \label{ex:matrixbases} 
Let $V=M_{nn}$. For each of the following subspaces $W\subset M_{nn}$, give a basis $B$ of $W$ and compute $\dim(W)$. 
\\
Justify your answer.

(a) $W$ is the set of upper triangular $n\times n$ matrices.
 
(b) $W$ is the set of symmetric $n\times n$ matrices. 

(c) $W$ is the set of skew-symmetric $n\times n$ matrices ($A^T=-A$). 
\\
\begin{solution}
\noindent
In what follows, $E_{ij}$ is the matrix with a 1 in the $ij$-th entry, and 0's elsewhere. To come up with bases for the listed spaces, you should first write down what an arbitrary matrix of the given description looks like, then understand this description as a linear combination of some $E_{ij}$'s. 
\\ \\
(a) An upper triangular matrix $A$ can have arbitrary entries $a_{ij}$ for all pairs $i,j$ with $i\leq j$, and must have 0's elsewhere. That means we can write 
\[
A=\sum_{i\leq j}a_{ij}E_{ij}
\]
as a linear combination {\em only} of the $E_{ij}$ with $i\leq j$. This shows the set 
\[
B=\{E_{ij}\colon i\leq j\}
\]
spans $W$. Since it is a subset of the standard basis for $M_{nn}$, $B$ is automatically independent. Thus it is a basis. 

We have $\dim(W)=\#B=1+2+\cdots + n=\frac{n(n+1)}{2}$.
\\ \\
(b)  An symmetric matrix $A$ can have arbitrary diagonal entries $a_{ii}$, but once you pick an entry $a_{ij}$ with $i<j$, then the corresponding entry $a_{ji}$ is determined since $a_{ji}=a_{ij}$. 

Let
\[
S=\{E_{11},\dots ,E_{nn}\}\cup\{D_{ij}\}_{1\leq i<j\leq n}
\]
where $E_{ii}$ is exactly as above, and $D_{ij}$ is the matrix with a 1 in the $ij$-th entry, a 1 in the $ji$-th entry, and 0's elsewhere. I claim $S$ is a basis. 

$S$ {\em spans}. As noted above an arbitrary symmetric matrix $A$ has any coefficients $a_{ii}$ along the diagonal; if the $ij$-th entry is $a_{ij}$, then the $ji$-th entry is $a_{ji}$, and thus it is clear that 
\[
A=a_{11}E_{11}+\cdots a_{nn}E_{nn}+\sum_{1\leq i<j\leq n}a_{ij}D_{ij}.
\]
Thus $S$ spans. 

$S$ {\em is linearly independent}. Suppose 
\[
a_{11}E_{11}+\cdots a_{nn}E_{nn}+\sum_{1\leq i<j\leq n}a_{ij}D_{ij}=0_{nn}.
\]
From the definition of $E_{ii}$ and $D_{ij}$, if follows that the $ij$-th entry of the matrix on the LHS is just $a_{ij}$. Thus $a_{ij}=0$ for all $i,j$. This shows $S$ is independent. 

Finally, The number of elements in $S$ is equal to the number of positions in an $n\times n$ matrix on and above the diagonal. Counting from the bottom of the matrix up, we see there are 
\[
1+2+\cdots +n=\frac{n(n+1)}{2}
\]
of these. Thus $\dim(W)=\frac{n(n+1)}{2}$. 
\\ \\
(c) Very similar to (b), except a skew-symmetric matrix $A$ must $a_{ii}=0$ for all $i$, and $a_{ij}=-a_{ji}$ for all $i<j$. Setting $S_{ij}$ to be the matrix with a 1 in the $ij$-th position, a -1 in the $ji$-th position and 0's elsewhere, we see that 
\[
B=\{S_{ij}\colon 1\leq i<j\leq n\}
\]
is a basis for $W$ in this case. We count the basis in a similar manner to (b) to conclude 
\[
\dim(W)=1+2+\cdots +(n-1)=\frac{n(n-1)}{2}.
\]
\end{solution}

\ii Determine if the vectors are linearly independent  or not in $\R^3$
\bb
\ii  $(-3,0,4), (5,-1,2), (1,1,3)$

\ii $(-2,0,1), (3,2,5), (6,-1,1), (7,0,-2)$

\ee
\begin{solution}
\noindent The relevant system of linear equations has augmented matrix:
\begin{eqnarray*}
\begin{bmatrix}[rrrr]
-3&5&1&0\\
0&-1&1&0\\
4&2&3&0
\end{bmatrix}
&\xrightarrow[]{-1/3r_1}&
\begin{bmatrix}[rrrr]
1&-5/3&-1/3&0\\
0&-1&1&0\\
4&2&3&0
\end{bmatrix}\\
&\xrightarrow[]{4r_1 - r_3}&
\begin{bmatrix}[rrrr]
1&-5/3&-1/3&0\\
0&-1&1&0\\
0&-26/3&-13/3&0
\end{bmatrix}\\
&\xrightarrow[]{-r_2}&
\begin{bmatrix}[rrrr]
1&-5/3&-1/3&0\\
0&1&-1&0\\
0&-26/3&-13/3&0
\end{bmatrix}\\
&\xrightarrow[]{26/3r_2 +r_3}&
\begin{bmatrix}[rrrr]
1&-5/3&-1/3&0\\
0&1&-1&0\\
0&0&-13&0
\end{bmatrix}\\
\end{eqnarray*}
Thus the only solution to the corresponding system is the trivial one, which means the vectors are linearly independent.
\\
(b) 
Consider the matrix
$$
\begin{bmatrix}[rrrrr]
-2&3&6&7&0\\
0&2&-1&0&0\\
1&5&1&-2&0
\end{bmatrix}
$$
When you reduce it, you will have a free variable, which means we do not have just the trivial solution. Thus the vectors are linearly dependent.
\end{solution}
\ii Determine if the vectors are linearly independent  or not in $\R^4$
\bb
\ii $(3,8,7,-3), (1,5,3,-1), (2,-1,2,6), (4,2,6,4)$
\ii $(3,0,-3,6), (0,2,3,1), (0,-2,-2,0), (-2,1,2,1)$
\ee
\begin{solution}
\noindent 
(a)  Dependent. The relevant linear system has augmented matrix 
\[
\begin{bmatrix}[rrrr|r]
3&1&2&4&0\\
8&5&-1&2&0\\
7&3&2&6&0\\
-3&-1&6&4&0
\end{bmatrix}
\]
which row reduces to 
\[
\begin{bmatrix}[rrrrr]
1&1/3&2/3&4/3&0\\
0&7/3&-19/3&-26/3&0\\
0&0&3&3&0\\
0&0&0&0&0
\end{bmatrix}
\]
Since the fourth column has no leading 1, the corresponding variable is free, which means the linear system has nontrivial solutions. This in turn implies the vectors are dependent. 
\\ (b) Independent. The relevant linear system has augmented matrix 
\[
\begin{bmatrix}[rrrr|r]
3&0&0&-2&0\\
0&2&-2&1&0\\
-3&3&-2&2&0\\
6&1&0&1&0
\end{bmatrix}
\]
which row reduces to 
\[
\begin{bmatrix}[rrrr|r]
1&0&0&-2/3&0\\
0&1&0&5&0\\
0&0&1&15/2&0\\
0&0&0&1&0
\end{bmatrix}
\]
Since the first four columns are all pivot columns, the corresponding linear system has no free variables, and hence only the trivial solution. This in turn implies the vectors are independent.
\end{solution}
\ii Determine whether the vectors are linearly independent or are linearly dependent in $P_2$.
\bb
\ii $2-x+4x^2$, $3+6x+2x^2$, $2+10x-4x^2$
\ii $1+3x+3x^2$, $x+4x^2$, $5+6x+3x^2$, $7+2x-x^2$

\ee
\begin{solution}
\noindent Independent. The relevant system of equations arising from looking at linear combinations yielding the zero polynomial has augmented matrix:
\[
\begin{bmatrix}[rrr|r]
2&3&2&0\\
-1&6&10&0\\
4&2&-4&0
\end{bmatrix}
\]
which row reduces to 
\[
\begin{bmatrix}[rrr|r]
1&-6&-10&0\\
0&1&22/15&0\\
0&0&1&0
\end{bmatrix}
\]
Since the first three columns have leading 1's, the corresponding system has no free variables, and hence only the trivial solution. This implies the polynomials are independent.
\\ (b) Dependent. The relevant system of equations arising from looking at linear combinations yielding the zero polynomial has augmented matrix 
\[
\begin{bmatrix}[rrrr|r]
1&0&5&7&0\\
3&1&6&2&0\\
3&4&3&-1&0
\end{bmatrix}
\]
Gaussian elimination shows the system has a free variable, which means there is a nontrivial linear combination yielding the zero polynomial, and hence that the polynomials are linearly dependent.
\end{solution}
\ii Determine whether the matrices are linearly independent or dependent.
\bb
\ii  In $M_{22}$

\[
\begin{bmatrix}[rr]
1&0\\
1&2
\end{bmatrix}
;
\begin{bmatrix}[rr]
1&2\\
2&1
\end{bmatrix}
;
\begin{bmatrix}[rr]
0&1\\
2&1
\end{bmatrix}
\]
\ii In $M_{23}$
\[
\begin{bmatrix}[rrr]
1&0&0\\
0&0&0
\end{bmatrix}
;
\begin{bmatrix}[rrr]
0&0&1\\
0&0&0
\end{bmatrix}
;
\begin{bmatrix}[rrr]
0&0&0\\
0&1&0
\end{bmatrix}
\]

\ee
\begin{solution}
\noindent (a) Independent. Looking at linear combinations of the matrices equal to the zero matrix gives rise to the linear system with augmented matrix
\[
\begin{bmatrix}[rrr|r]
1&1&0&0\\
0&2&1&0\\
1&2&2&0\\
2&1&1&0
\end{bmatrix}
\]
This row reduces to 
\[
\begin{bmatrix}[rrr|r]
1&0&0&0\\
0&1&0&0\\
0&0&1&0\\
0&0&0&0
\end{bmatrix}
\]
Thus $c_1=c_2=c_3=0$ is the only linear combination of the matrices yielding $\boldzero$ and we can see that  the matrices are independent. 
\\
(b) Independent. Consider
$$k_1
\begin{bmatrix}[rrr]
1&0&0\\
0&0&0
\end{bmatrix}
+k_2
\begin{bmatrix}[rrr]
0&0&1\\
0&0&0
\end{bmatrix}
+k_3
\begin{bmatrix}[rrr]
0&0&0\\
0&1&0
\end{bmatrix}
=
\begin{bmatrix}[rrr]
0&0&0\\
0&0&0
\end{bmatrix}
$$
Since the first matrix is the only one to contribute to the zero in the $a_{11}$ slot, $k_1$ must be zero. Since the second matrix is the only one to contribute to the zero in the $a_{13}$ slot, $k_2$ must also be zero. Similarly, $k_3$ must also be zero. This means only the trivial solution exits, thus the 3 given matrices are independent.
\end{solution}
\ii Determine all values of $k$ for which the following matrices are linearly independent in $M_{22}$.
$$
\begin{bmatrix}[rr]
1&0\\
1&k
\end{bmatrix}
;
\begin{bmatrix}[rr]
-1&0\\
k&1
\end{bmatrix}
;
\begin{bmatrix}[rr]
2&0\\
1&3
\end{bmatrix}
$$
\begin{solution}
\noindent 
Looking at linear combinations of the matrices equal to the zero matrix gives rise to the linear system with augmented matrix
\[
\begin{bmatrix}[rrr|r]
1&-1&2&0\\
0&0&0&0\\
1&k&1&0\\
k&1&3&0
\end{bmatrix}
\]
This row reduces to 
\[
\begin{bmatrix}[rcc|r]
1&-1&2&0\\
0&-k-1&2k-3&0\\
0&0&2k-4&0\\
0&0&0&0
\end{bmatrix}
\]
The system has only the trivial solution iff the first three columns are pivot columns iff $k+1\ne 0$ and $2k-4\ne 0$ iff $k\ne -1$ and $k\ne 2$. Thus the matrices are independent for all values of $k$ except for $k=-1$ and $k=2$.
\end{solution}
\ii \label{ex:basis} Show that the following polynomials form a basis for $P_3$.
\[
p_1=1+x, p_2=1-x, p_3=1-x^2, p_4=1-x^3
\]
\begin{solution}
To prove that the $p_i$ form a basis, we must show that for any $p(x)=b_0+b_1x+b_2x^2+b_3x^3$, we there is a {\em unique} choice of $c_1,c_2,\dots, c_4$ satisfying $c_1p_1+c_2p_2+c_3p_3+c_4p_4=p(x)$. After combining like terms and equating like coefficients, this boils down to the matrix equation 
\[
A\colvec{c_1\\c_2\\c_3\\c_4}=\colvec{b_0\\ b_1\\ b_2\\ b_3}
\]
having a unique solution for any choice of $b_i$, where 
\[
A = 
\begin{bmatrix}[rrrr]
1&1&1&1\\
1&-1&0&0\\
0&0&-1&0\\
0&0&0&-1
\end{bmatrix}.
\]
The invertibility theorem tells us that this is the case if and only if $A$ is invertible if and only if $\det A\ne 0$. We compute 
\[
\det(A) = -1\det
\begin{bmatrix}[rrr]
1&1&1\\
1&-1&0\\
0&0&-1
\end{bmatrix}
= -1\left(-1\det
\begin{bmatrix}[rr]
1&1\\
1&-1
\end{bmatrix}\right)
=-1(-1(-1-1)) = -2 \neq 0
\]
Thus $A$ is invertible, and hence the $p_i$ form a basis for $P_3$. 
\end{solution}

\ii Show that the following matrices form a basis for $M_{22}$.
$$
A_1=\begin{bmatrix}[rr]
1&1\\
1&1
\end{bmatrix}
; \ 
A_2=\begin{bmatrix}[rr]
1&-1\\
0&0
\end{bmatrix}
; \ 
A_3=\begin{bmatrix}[rr]
0&-1\\
1&0
\end{bmatrix}
; \ 
A_4=\begin{bmatrix}[rr]
1&0\\
0&0
\end{bmatrix}
$$
\begin{solution} Reasoning as in Exercise \ref{ex:basis}, this boils down to the invertibility of the following matrix:
$$
B=
\begin{bmatrix}[rrrr]
1&1&0&1\\
1&-1&-1&0\\
1&0&1&0\\
1&0&0&0
\end{bmatrix}
$$
We test this using the determinant: 
\begin{eqnarray*}
\det(B) = -1\det
\begin{bmatrix}[rrr]
1&0&1\\
-1&-1&0\\
0&1&0
\end{bmatrix}
= -1\left(1\det
\begin{bmatrix}[rr]
1&0\\
-1&-1
\end{bmatrix}\right)
=-1(1(-1-0)) = 1 \neq 0
\end{eqnarray*}
Thus $B$ is not invertible, which implies the matrices $A_i$ do not form a basis of $M_{22}$. 
\end{solution}
\ii Show that the following vectors do not form a basis for $P_2$
$$
p_1=1-3x+2x^2, p_2=1+x+4x^2, p_3=1 - 7x
$$
\begin{solution} Reasoning as in Exercise \ref{ex:basis}, this amounts to showing 
$$A =
\begin{bmatrix}[rrr]
1&1&1\\
-3&1&-7\\
2&4&0
\end{bmatrix}
$$
is invertible. Again, we can test this using the determinant: 
\begin{eqnarray*}
\det(A) = 2\det
\begin{bmatrix}[rr]
1&1\\
1&-7
\end{bmatrix}
-4\det
\begin{bmatrix}[rr]
1&1\\
-3&-7
\end{bmatrix}
= 2(-7-1) -4(-7+3) = -16 + 16 = 0
\end{eqnarray*}
Thus $A$ is not invertible, and hence the polynomials do not form a basis. 
\end{solution}
\ii Let $V$ be the space spanned by $\boldv_1 = \cos^2x$, $\boldv_2 = \sin^2x$, $\boldv_3=\cos2x$.
\bb
\ii Show that $S = {\boldv_1,\boldv_2,\boldv_3}$ is not a basis for $V$.

\ii Find a basis for $V$.
\ee
\begin{solution}
\noindent The trig identity $\cos^2x - \sin^2x =\cos(2x)$ provides a linear relation between the elements of $S$. Thus $S$ is not linearly independent, and hence does not form a basis. 
\\
(b) 
Remove $\boldv_3$. The span remains the same and $\boldv_1$ and $\boldv_2$ are independent. So
$$
S'=\{\cos^2x,\sin^2x\}
$$
is a basis for $V$. 
\end{solution}

\ii Let $V=\R^n$, considered as column vectors. 
Prove: vectors $\boldv_1,\boldv_2,\dots, \boldv_n$ form a basis of $\R^n$ if and only if the matrix $A=\begin{bmatrix}
\vert &\vert &\cdots &\vert\\
\boldv_1&\boldv_2& &\boldv_n\\
\vert &\vert &\cdots &\vert
\end{bmatrix}$
is invertible. 
\\
\begin{solution}
\noindent
We prove the statement using a {\em chain of equivalences}:
\begin{align*}
&\{\boldv_1, \boldv_2,\dots, \boldv_n\} \text{ is a basis of $\R^n$}\\
&\Leftrightarrow \text{ for all } \boldb\in \R^n \text{ we can write $\boldb=c_1\boldv_1+c_2\boldv_2+\cdots +c_n\boldv_n$ in a {\em unique} way} &\text{(Theorem 3.15)}\\
&\Leftrightarrow \text{for all $\boldb\in \R^n$ the matrix equation $A\boldc=\boldb$ has a unique solution} &\text{(column method)}\\
&\Leftrightarrow \text{$A$ is invertible}&\text{(invertibility theorem)}
\end{align*}  
\end{solution}
\ii Let $\boldv_1,\boldv_2,\dots, \boldv_n$ be a basis for $\R^n$ (considered as column vectors), and let $Q$ be an invertible $n\times n$ matrix. 
\\ 
Show that the vectors 
\begin{align*}
\boldw_1&=Q\boldv_1\\ 
\boldw_2&=Q\boldv_2\\
\ \vdots \\
 \boldw_n&=Q\boldv_n
 \end{align*}
 form a basis for $\R^n$. 
\\
\begin{solution}
\noindent 
Let $A=\begin{bmatrix}
\vert &\vert &\cdots &\vert\\
\boldv_1&\boldv_2& &\boldv_n\\
\vert &\vert &\cdots &\vert
\end{bmatrix}$. \\
Then $A$ is invertible, by the previous exercise. Let $B=QA$. Using the column method of multiplication, we have 
\[
B=QA=\begin{bmatrix}
\vert &\vert &\cdots &\vert\\
Q\boldv_1&Q\boldv_2& &Q\boldv_n\\
\vert &\vert &\cdots &\vert
\end{bmatrix}
=\begin{bmatrix}
\vert &\vert &\cdots &\vert\\
\boldw_1&\boldw_2& &\boldw_n\\
\vert &\vert &\cdots &\vert
\end{bmatrix}
\]
Since $Q$ and $A$ are invertible, so is $B$. The previous exercise then implies that the $\boldw_i$ form a basis. 
\end{solution} 
\ii True or false. If true, provide a proof; if false, give an explicit counterexample. 
\bb
\ii If $V = \Span\{\boldv_1,\dots ,\boldv_n\}$, then $\{\boldv_1,\dots ,\boldv_n\}$ is a basis for $V$.
\ii Every linearly independent subset of a vector space $V$ is a basis for $V$.
\ii If $\{\boldv_1, \boldv_2, \dots , \boldv_n\}$ is a basis for a vector space $V$, then every vector in $V$ can be expressed as a linear combination of $\boldv_1, \boldv_2, \dots , \boldv_n$.
\ii Every basis of $P_4$ contains at least one polynomial of degree 3 or less.
\ii There is a basis for $M_{22}$ consisting of invertible matrices. 
\ee
\begin{solution}
\noindent (a) False. A basis must also be linearly independent. Counter example: $V = R^2$ and the vectors $\{(1,0),(0,1),(1,1)\}$. Then $span\{(1,0),(0,1),(1,1)\} = R^2$ but since $(1,0) + (0,1) = (1,1)$ the vector set is not independent and therefore not a basis.
\\
\\ 
(b) False. The vector set must span $V$. Consider $V = R^3$ and the vector set $\{(1,0,0),(0,1,0)\}$. Clearly the set is independent, but it cannot span $V$.
\\ \\ 
(c) True. A basis spans $V$ by definition. 
\\
(d) False. Consider the vector set
$$
1+x+x^2+x^3+x^4, x+x^2+x^3+x^4, x^2+x^3+x^4, x^3+x^4, x^4
$$
The degree of each polynomial is 4. To show this set forms a basis, we set up the usual linear combination equation $c_1p_1+c_2p_2+c_3p_3+c_4p_4=p$, where $p$ is an arbitrary polynomial, which boils down to a matrix equation of the form 
\[
A\colvec{c_1\\ c_2\\ c_3\\ c_4\\c_5}=\colvec{a_0\\ a_1\\ a_2\\ a_3\\a_4} \text{ ($A\boldc=\bolda$)}
\]
where 
$$
A = 
\begin{bmatrix}[rrrrr]
1&0&0&0&0\\
1&1&0&0&0\\
1&1&1&0&0\\
1&1&1&1&0\\
1&1&1&1&1
\end{bmatrix}.
$$
The set forms a basis if and only if this equation has a unique solution for each $\bolda=(a_0,a_1,a_2,a_3,a_4)$, if and only if $A$ is invertible. Since $\det A=1$ (lower triangular matrix), we conclude $A$ is invertible, and hence the given polynomials form a basis. 
\\
(e) True. You can check for yourself that the following matrices are invertible and form a basis:
\[
\begin{bmatrix}
1&0\\
0&1
\end{bmatrix}, 
\begin{bmatrix}
1&1\\
0&1
\end{bmatrix},
\begin{bmatrix}
1&0\\
1&1
\end{bmatrix},
\begin{bmatrix}
1&1\\
1&0
\end{bmatrix}
\]
\end{solution}

\ee
