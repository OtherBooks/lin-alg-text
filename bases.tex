\begin{frame}{\ref{s:vectorspace}.\ref{ss:basis}: executive summary}
\alert{Definitions:} basis.
\bspace
\alert{Procedures:} deciding whether a set is basis.
\bspace
\alert{Theorems:} $B$ is a basis for $V$ iff every vector in $V$ can be written as a linear combination of the vectors in $B$ in {\em a unique way}.

\end{frame}
\begin{frame}{\ref{s:vectorspace}.\ref{ss:basis}: definition of basis}
\begin{definition}
Let $V$ be a vector a space. A set $B=\{\boldv_1,\dots,\boldv_n\}$ is called a (finite) {\bf basis} for $V$ if 
\bb[(i)]
\ii $B$ spans $V$, and 
\ii $B$ is linearly independent.
\ee
\end{definition}
\pause
\begin{comment}
You should think of a basis $B$ as a ``minimal" set of vectors needed to generate the space $V$. 
\bpause
Condition (i) ensures all elements of $V$ can be written as a linear combination of the $\boldv_i$. Thus $B$ generates $V$. 
\bpause 
Condition (ii) ensures we have no redundant elements in our set $B$. Thus $B$ is minimal. 
\end{comment}
\end{frame}
\begin{frame}{Some standard bases}
\bb
\ii Let $V=\R^n$, and let $\bolde_i$ be the the $n$-tuple whose $i$-th entry is 1, and whose every other entry is 0: e.g, 
\[
\bolde_1=(1,0,\dots,0), \bolde_2=(0,1,0,\dots, 0),\text{ etc.}
\]
Then the set $B=\{\bolde_1,\bolde_2,\dots \bolde_n\}$ is a basis for $\R^n$, called the {\bf standard basis} for $\R^n$. 
\pause
\ii Let $V=P_n$. Then the set $B=\{1, x, x^2, \dots x^{n}\}$ is a basis for $V$, called the {\bf standard basis} for $P_n$. 
\pause
\ii Let $V=M_{mn}$, and let $E_{ij}$ be the $m\times n$ matrix whose $ij$-th entry is 1, and whose every other entry is 0. Then $ B=\{E_{ij}\colon 1\leq i\leq m, 1\leq j\leq n\}$ is a basis for $V$, called the {\bf standard basis} for $M_{mn}$.
\ee
\end{frame}
\begin{frame}{Some nonstandard bases}
For each of the following choices of $V$ and $B$, prove that $B$ is a basis for $V$. 
\bb
\ii $V=\R^4$, $B=\{(1,1,1,1), (1,0,0,-1), (1,0,-1,0), (0,1,-1,0)\}$.
\ii $V=P_2$, $B=\{1+x+x^2, -x+x^2, -1+x^2\}$.
\ii $V=M_{22}$, 
\[
B=\left\{
\begin{bmatrix}[rr]
3&6\\
3&-6
\end{bmatrix},
\begin{bmatrix}[rr]
0&-1\\
-1&0
\end{bmatrix},
\begin{bmatrix}[rr]
0&-8\\
-12&-4
\end{bmatrix},
\begin{bmatrix}[rr]
1&0\\
-1&2
\end{bmatrix}
\right\}.
\]
\ee
\end{frame}
\begin{frame}{Nonexistence of a finite basis}
A vector space need not have a finite basis. 
\bspace
For example, the space $P_\infty$ of all polynomials does not have a finite basis. 
\pause
\begin{proof}
Suppose by contradiction that $P_\infty$ did have a finite basis $B=\{p_1,p_2,\dots, p_n\}$ for some polynomials $p_i(x)$. 
\bpause 
Each polynomial $p_i$ has a degree $m_i=\deg(p_i)$. Let $m$ be the maximum of $m_1,m_2,\dots, m_n$. Then $\Span(S)\subset P_m$, the space of polynomials of degree at most $m$. 
\bpause 
But then we have 
\[
\Span(B)\subset P_m\subsetneq P_\infty.
\]
Thus $B$ doesn't span $P_\infty$. A contradiction. 
\end{proof}
\pause Since \[ P_\infty\subset C^\infty(a,b)\]
it follows that $C^\infty(a,b)$ also fails to have a finite basis. 
\end{frame}
\begin{frame}{Some funny spaces}
\bb
\ii Let $V=\{\boldzero\}$. The only subsets of $V$ are the empty set $S=\{ \}$, and the set $S=\{\boldzero\}$.
\bpause 
The first set doesn't span $V$; the second set is not linearly independent. So technically the zero space has no basis! 
\bpause 
This turns out to be a significant inconvenience, so by executive order we declare the empty set $B=\{ \}$ to be a basis of the zero space $V=\{\boldzero\}$. 
\pause\ii Let $V=\R_{>0}$. Recall that here vector addition is real number multiplication, and scalar multiplication is real number exponentiation. 

Let $r\ne 1$ be any element of $\R_{>0}$, not equal to 1. Claim: $B=\{r\}$ is a basis for $\R_{>0}$.  
\bspace
Proof: homework exercise! 
\ee
\end{frame}
\begin{frame}
\begin{theorem}\label{th:basisunique}
A set $B=\{\boldv_1,\dots, \boldv_n\}$ is a basis of $V$ if and only if every vector $\boldv\in V$ can be written as a linear combination of $\boldv_i$ in a \alert{unique way}: i.e., for every $\boldv\in V$ there is exactly one choice of scalars $c_1,c_2, \dots c_n$ such that 
\[
\boldv=c_1\boldv_1+c_2\boldv_2\cdots +c_n\boldv_n.
\]
\end{theorem}
\pause
\begin{comment}
The theorem allows us to prove a set $B$ is a basis in one shot. No need to prove $\Span(B)=V$ and linear independence in two separate steps. 
\bpause
Indeed, simply set up a linear combination of the given vectors equal to an arbitrary element of $V$, boil this vector equation down to a system of $n$ linear equations in the $n$ unknowns $c
_i$, and use GE to determine the set of solutions. 
\end{comment}
\end{frame}

%\begin{frame}
%Your professor will now address some sloppiness in our setup. 
%\bspace
%As an example, take $V=P_2$. what is the difference between the two bases $S=\{1,x,x^2\}$ and $S'=\{x, 1, x^2\}$? 
%\bspace 
%Shouldn't we really be talking about \alert{ordered bases}, as opposed to just bases? 
%\bpause
%We patch things up by agreeing that when dealing with bases \alert{order counts}. By way of another executive order applying only to bases, we declare that even though we notate bases as \alert{sets}, we really have in mind an \alert{ordered} list of elements. 
%\end{frame}