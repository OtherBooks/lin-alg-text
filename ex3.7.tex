\bb
\ii Give an explicit example of a matrix $A$ such that $\CS(A)\ne \CS(U)$. 
\\
\begin{solution}
Take $A=\begin{bmatrix}1&1\\1&1 \end{bmatrix}$, which row reduces to $U=\begin{bmatrix} 1&1\\0&0 \end{bmatrix}$. 

Then $\CS(A)=\Span(\{(1,1\})$, which is the line $y=x$, and $\CS(U)=\Span(\{(1,0)\})$, which is the $x$-axis. Clearly these two lines are not equal. 
\end{solution}
\ii Prove that $\CS(A)=\{\boldb\in\R^m\colon A\boldx=\boldb \text{ has a solution}\}$, as we claimed above.

This is a {\em set equality} of the form $S=T$. You can prove this either (a) by showing $S\subset T$ and $T\subset S$, or (b) by showing that $x\in S \Leftrightarrow x\in T$. 
%\vfill
\\
\begin{solution}
A complete proof of this is given in my 4.6-4.7 notes. 
\end{solution}
\ii Find bases and dimensions for all fundamental spaces of 
\[
A=\begin{bmatrix}[rrrr] 
1&-1&2&0\\
2&0&3&1\\
1&1&1&1
\end{bmatrix}
\]
%\vfill
\begin{solution}
Routine. Follow the usual algorithms. The matrix row reduces to 
\[
U=\begin{bmatrix}[rrrr]
1&-1&2&0\\ 0&1&-1/2&1/2\\ 0&0&0&0
\end{bmatrix}.
\]
From this we compute the following bases:
\begin{align*}
\NS(A)\colon B_1&=\{(1,1,0,-2), (-3,1,2,0) \} & \dim(\NS(A))&=2\\
\CS(A)\colon B_2&=\{ (1,2,1), (-1,0,1) \} & \dim(\CS(A))&=2\\
\RS(A)\colon B_3&=\{ (1,-1,2,0), (0,1,-\frac{1}{2},\frac{1}{2}) & \dim(RS(A))&=2 
\end{align*}
\end{solution}
%\ii Let $A$ be $m\times n$. Look closely at our description of the method of computing the fundamental spaces to do prove the following:
%\[
%\dim(\CS(A))=\dim(\RS(A))=(\# \text{ leading variables in system <m>A\boldx=\boldzero</m>})
%\]

%\vfill
\ii Let $S=\{p_1=1+x+2x^2,p_2=1-x,p_3=1+x^2,p_4=1+x-x^2\}$ and let $W=\Span(S)\subset P$.  (Recall $P$ is the space of all polynomials.) 
\bb
\ii Use ``street smarts" to decide whether $S$ is linearly independent. 
\ii Use coordinate vectors and an appropriate fundamental space algorithm to choose a basis of $W=\Span(S)$ {\em from among the elements of $S$}.
\ii Give a satisfying description of $W$. 
\ee
\begin{solution}
\noindent 
(a) In fact we see that $S$ lies within the smaller subspace $P_2$, which has dimension $3$. Since $S$ has 4 elements and lives in a 3-dimensional space, it is guaranteed to be dependent. Street smarts! 
\\ \\
(b) Let $B$ be the standard basis of $P_2$: i.e., $B=\{1,x,x^2\}$. Then we can translate this whole problem into $\R^3$ using coordinate vectors. Namely we let:
\[
\boldv_1=(p_1)_B=\begin{bmatrix}
1\\ 1\\ 2
\end{bmatrix}, \boldv_2=(p_2)_B=\begin{bmatrix}
1\\ -1\\ 0
\end{bmatrix}, \boldv_3=(p_3)_B=\begin{bmatrix}
1\\0 \\ 1
\end{bmatrix}, \boldv_4=(p_4)_B=\begin{bmatrix}
1\\ 1\\ -1
\end{bmatrix},
\]
and set $W'=\Span(\{\boldv_1,\boldv_2,\boldv_3,\boldv_4\}$. 

Putting the $\boldv_i$ into the columns of a matrix $A$ and applying our column space algorithm tells us that $\{\boldv_1,\boldv_2,\boldv_4\}$ form a basis for $W'$. (You do the work.) 

Translating back to $P_3$ we conclude that the corresponding polynomials $\{p_1,p_2,p_4\}$ form a basis for $W$. 
\\
(c) Since $W$ has a basis containing three elements, we see that $\dim(W)=3$. Since $W\subset P_2$ and $\dim(P_2)=\dim(W)=3$, we see by the dimension theorem compendium (subspace part)  that $W$ is in fact all of $P_2$. 
\end{solution}
\ii For each matrix below use the rank-nullity theorem to help determine the fundamental spaces ``by inspection"--i.e., without having to do Gaussian elimination. 

The various fundamental spaces will live either in $\R^2$ or $\R^3$. For each matrix give two sketches showing (a) $\NS(A)$ and $\RS(A)$ in one sketch, and (b) $\CS(A)$ in another. 
\bb
\ii $A=\begin{bmatrix}[rrr]
3&2&1\\
-6&-4&-2
\end{bmatrix}$
%\vfill

\ii $A=\begin{bmatrix}[rr]
2&1\\
8&4\\
6&3
\end{bmatrix}
$
\ee
\begin{solution}
\noindent (a) The columns of $A$ are all scalar multiples of each other, so clearly $B_1=\{(1,-2)\}$ is a basis of $\CS(A)$, and we have $\rank(A)=1$. When you sketch $\CS(A)$ you get the line in $\R^2$  connecting $(0,0)$ and $(1,-2)$. 

It follows that $\RS(A)$ is also 1-dimensional, and thus that any nonzero row of $A$ will do as a basis: e.g., $B_2=\{(3,2,1)\}$. When you sketch $\RS(A)$ you get the line in $\R^3$ passing through $(0,0,0)$ and $(3,2,1)$. 

Lastly rank-nullity implies $\dim(\NS(A))=3-1=2$. Furthermore, by the orthogonality relation $\NS(A)$ is the plane orthogonal to $\RS(A)$. Having already sketched $\RS(A)$ above, this is now easy to add to your sketch. You do it! 
%\vfill
\\
(b)
The reasoning is similar to the above. 

A basis for $\CS(A)$ is $B_1=\{(1,4,3)\}$. Thus $\CS(A)$ is the span of this vector, which defines a line in $\R^3$. 

A basis for $\RS(A)$ is $B_2=(2,1)$, and thus $\RS(A)$ is the line passing through the origin and $(2,1)$. Then $\NS(A)$ is just the orthogonal complement of this in $\R^2$, which now is the line perpendicular to this line. Sketch it!  
\end{solution}

%\newpage
\ii Let $A$ be $m\times n$ with $n<m$. Use fundamental spaces to show that there is a $\boldb\in\R^m$ such that the system $A\boldx=\boldb$ is inconsistent. 
%\vfill
\\
\begin{solution}
Using the fact that $\CS(A)=\{\boldb\in\R^m\colon A\boldx=\boldb \text{ has a solution}\}$, we see that we need only show that $\CS(A)\subsetneq R^m$. We do so using rank-nullity. 

We have $\rank(A)\leq\min\{m,n\}$. Since $n<m$, $\min\{m,n\}=n$. Thus $\rank(A)\leq n$. 

But $\rank(A)=\dim(\CS(A))$. Thus 
\[\dim(\CS(A))\leq n<m=\dim(\R^m).\] 
Thus we have $\CS(A)\subset \R^m$ and $\dim(\CS(A))<\dim(\R^m)$. 
By our theorem about dimensions of subspaces, it follows that $\CS(A)\ne \R^m$, and we are done. 

\end{solution}
\ii Let $A=\begin{bmatrix}[rrrr]1&1&1&1\\ 2&1&2&1\\ 1&1&1&1 \end{bmatrix}$. 
Use the rank-nullity theorem to find bases for all the fundamental spaces of $A$ ``by inspection". 
%\vfill
\\
\begin{solution}
More of the same. There are clearly only 2 linearly independent columns. So a basis for $\CS(A)$ is $B_1=\{(1,2,1),(1,1,1)\}$. 

Since $\dim(\RS(A))=\dim(\CS(A))=2$, we see easily that $B_2=\{(1,1,1,1),(2,1,2,1)\}$ is a basis for $\RS(A)$. 

Lastly, rank-nullity tells us that $\dim(\NS(A))=n-2=4-2=2$. Thus to come up with a basis for $\NS(A)$ I need only find two linearly independent elements of $\NS(A)$. 

I can come up with two such solutions to $A\boldx=\boldzero$ easily by inspection, thinking of $A\boldx$ as a linear combination of the columns of $A$. For example, since the first and third columns of $A$ are identical, I see that $(1,0,-1,0)$ is an element of $\NS(A)$. (Do the multiplication yourself!) Since the second and fourth columns are identical, I see that $(0,1,0,-1)$ is an element of $\NS(A)$. 

Since these two vectors are clearly linearly independent and $\NS(A)$ has dimension 2, we see that $B_3=\{(1,0,-1,0), (0,1,0,-1)\}$ is a basis for $\NS(A)$.    
\end{solution}
\ii Find bases for all the fundamental spaces of the following matrices.
\bb
\ii 
$$
A = 
\begin{bmatrix}[rrrr]
1&4&5&2\\
2&1&3&0\\
-1&3&2&2
\end{bmatrix}
$$
\ii 
$$
A = 
\begin{bmatrix}[rrrrr]
1&4&5&6&9\\
3&-2&1&4&-1\\
-1&0&-1&-2&-1\\
2&3&5&7&8
\end{bmatrix}
$$
\ee
\begin{solution}
\noindent (a) The matrix $A$ reduces to:
$$
\begin{bmatrix}[rrrc]
1&0&1&-2/7\\
0&1&1&4/7\\
0&0&0&0
\end{bmatrix}
$$
The non-zero rows of the reduced matrix form a basis for the row space:
$$\{
\begin{bmatrix}[rrrr]
1&0&1&-2/7
\end{bmatrix},
\begin{bmatrix}[rrrr]
0&1&1&4/7
\end{bmatrix}\}$$
Since the null space is the solution space of the homogeneous linear system $A\boldx = \textbf{0}$, we need to look at the first two rows of the reduced matrix.
\begin{eqnarray*}
x_1 + x_3 - 2/7x_4 &=& 0\\
x_2 + x_3 +4/7x_4 &=& 0
\end{eqnarray*}
Setting the free variables $x_3 = r$ and $x_4 = s$.
\begin{eqnarray*}
(x_1,x_2,x_3,x_4) &=& (-r + 2/7s, -r-4/7s,r,s)\\
&=& r(-1,-1,1,0) + s(2/7,-4/7,0,1)
\end{eqnarray*}
Thus the basis for the null space is:
$$
\left\{
\begin{bmatrix}[c]
-1\\
-1\\
1\\
0
\end{bmatrix}
,
\begin{bmatrix}[c]
2/7\\
-4/7\\
0\\
1
\end{bmatrix}
\right\}
$$
\\
(b) The 
matrix $A$ reduces to:
$$
\begin{bmatrix}[rrrrr]
1&0&1&2&1\\
0&1&1&1&2\\
0&0&0&0&0\\
0&0&0&0&0
\end{bmatrix}
$$
Thus the basis for the row space is:
$$
\{
\begin{bmatrix}[rrrrr]
1&0&1&2&1
\end{bmatrix},
\begin{bmatrix}[rrrrr]
0&1&1&1&2
\end{bmatrix}\}
$$
Setting up the equations for the null space basis:
\begin{eqnarray*}
x_1 + x_3+ 2x_4+x_5 &=& 0\\
x_2+x_3+x_4+2x_5 &=& 0
\end{eqnarray*}
Setting the free variables $x_3 = r$, $x_4 = s$, and $x_5 = t$.
\begin{eqnarray*}
(x_1,x_2,x_3,x_4,x_5,) &=& (-r-2s-t,-r-s-2t,r,s,t)\\
&=& r(-1,-1,1,0,0) + s(-2,-1,0,1,0) + t(-1,-2,0,0,1) 
\end{eqnarray*}
Thus the basis for the null space is:
$$
\left\{
\begin{bmatrix}[c]
-1\\
-1\\
1\\
0\\
0
\end{bmatrix},
\begin{bmatrix}[c]
-2\\
-1\\
0\\
1\\
0
\end{bmatrix},
\begin{bmatrix}[c]
-1\\
-2\\
0\\
0\\
1
\end{bmatrix}
\right\}
$$
\end{solution}

\ii Find bases for the row space and for the column space of the given matrix.
\bb
\ii 
$$A=
\begin{bmatrix}[rrrr]
1&2&4&5\\
0&1&-3&0\\
0&0&1&-3\\
0&0&0&0
\end{bmatrix}
$$
\ii 
$$A=
\begin{bmatrix}[rrrr]
1&2&-1&5\\
0&1&4&3\\
0&0&1&-7\\
0&0&0&1
\end{bmatrix}
$$
\ee
\begin{solution}
\noindent
(a) The non-zero rows of the matrix for a basis for the row space:
$$
\{
\begin{bmatrix}[rrrr]
1&2&4&5
\end{bmatrix},
\begin{bmatrix}[rrrr]
0&1&-3&0
\end{bmatrix},
\begin{bmatrix}[rrrr]
0&0&1&-3
\end{bmatrix}\}
$$
The columns that have the leading 1's of the row vectors form a basis for the column space:
$$
\left\{
\begin{bmatrix}[c]
1\\
0\\
0\\
0\\
0
\end{bmatrix},
\begin{bmatrix}[c]
2\\
1\\
0\\
0\\
0
\end{bmatrix},
\begin{bmatrix}[c]
4\\
-3\\
1\\
0\\
0
\end{bmatrix},
\begin{bmatrix}[c]
5\\
0\\
-3\\
1\\
0
\end{bmatrix}
\right\}
$$
\\
(b) The non-zero rows of the matrix for a basis for the row space:
$$
\{
\begin{bmatrix}[rrrr]
1&2&-1&5
\end{bmatrix},
\begin{bmatrix}[rrrr]
0&1&4&3
\end{bmatrix},
\begin{bmatrix}[rrrr]
0&0&1&-7
\end{bmatrix},
\begin{bmatrix}[rrrr]
0&0&0&1
\end{bmatrix}
\}
$$
The columns that have the leading 1's of the row vectors form a basis for the column space:
$$
\left\{
\begin{bmatrix}[c]
1\\
0\\
0\\
0\\
0
\end{bmatrix},
\begin{bmatrix}[c]
2\\
1\\
0\\
0\\
0
\end{bmatrix},
\begin{bmatrix}[c]
-1\\
4\\
1\\
0\\
0
\end{bmatrix},
\begin{bmatrix}[c]
5\\
3\\
-7\\
1\\
0
\end{bmatrix}
\right\}
$$
\end{solution}
\ii Let 
$\ds A=
\begin{bmatrix}[rrrrr]
1&-2&5&0&3\\
-2&5&-7&0&-6\\
-1&3&-2&1&-3\\
-3&8&-9&1&-9
\end{bmatrix}
$.
\bb
\ii 
Use the usual procedure to find bases for $\CS(A)$ and $\RS(A)$. 
\ii Now compute a basis for $\RS(A)$ consisting of a {\em subset of the rows}  of $A$. Hint: look at $A^T$. 
\ee
\begin{solution}
The given matrix reduces to:
$$
\begin{bmatrix}[rrrrr]
1&0&11&0&3\\
0&1&3&0&0\\
0&0&0&1&0\\
0&0&0&0&0
\end{bmatrix}
$$
The non-zero rows of the reduced matrix for a basis for the row space:
$$
\{
\begin{bmatrix}[rrrrr]
1&0&11&0&3
\end{bmatrix},
\begin{bmatrix}[rrrrr]
0&1&3&0&0
\end{bmatrix},
\begin{bmatrix}[rrrrr]
0&0&0&1&0
\end{bmatrix}
\}
$$
The columns of the original matrix that correspond the the columns of the reduced matrix that have the leading 1's of the row vectors form a basis for the column space:
$$
\left\{
\begin{bmatrix}[c]
1\\
-2\\
-1\\
-3
\end{bmatrix},
\begin{bmatrix}[c]
-2\\
5\\
3\\
8
\end{bmatrix},
\begin{bmatrix}[c]
0\\
0\\
1\\
1
\end{bmatrix}
\right\}
$$
\\
(b) 
We have $\RS(A)=\CS(A^T)$. Applying the usual procedure to compute a basis of $\CS(A^T)$ produces a basis for $\RS(A)=\CS(A^T)$ from among the columns of $A^T$, which are the rows of $A$. 
\\
We have 
$$A^T=
\begin{bmatrix}[rrrr]
1&-2&-1&-3\\
-2&5&3&8\\
5&-7&-2&-9\\
0&0&1&1\\
3&-6&-3&-9
\end{bmatrix}
$$
which reduces to:
$$
\begin{bmatrix}[rrrr]
1&0&0&0\\
0&1&0&1\\
0&0&1&1\\
0&0&0&0\\
0&0&0&0
\end{bmatrix}
$$
Since columns 1,2,and 3 have the leading 1's, the corresponding rows of the original matrix form a basis:
$$
\{
\begin{bmatrix}[rrrrr]
1&-2&5&0&3
\end{bmatrix},
\begin{bmatrix}[rrrrr]
-2&5&-7&0&-6
\end{bmatrix},
\begin{bmatrix}[rrrrr]
-1&3&-2&1&-3
\end{bmatrix}
\}
$$
\end{solution}
\ii Find a subset of the given vectors that forms a basis for the space spanned by those vectors, and then express each vector that is not in the basis as a linear combination of the basis vectors.
$$
\boldv_1 = (1,0,1,1), \boldv_2 = (-3,3,7,1), \boldv_3 = (-1,3,9,3), \boldv_4 = (-5,3,5,-1)
$$
\begin{solution}
Start by writing each vector as a column in a matrix.
$$
\begin{bmatrix}[rrrr]
1&-3&-1&-5\\
0&3&3&3\\
1&7&9&5\\
1&1&3&-1
\end{bmatrix}
$$
Like the previous problems we reduce the matrix.
$$
\begin{bmatrix}[rrrr]
1&0&2&-2\\
0&1&1&1\\
0&0&0&0\\
0&0&0&0
\end{bmatrix}
$$
Now the columns of the original matrix that correspond to the columns of the reduced matrix that have the leading 1's form a basis. Thus column 1 and 2 form the basis. Since our columns are vectors, the basis is: $$\{\boldv_1,\boldv_2\}$$
Now we find a set of dependency equations for each column in the reduced matrix that does not have a leading 1. Let $\boldw_i$ be the columns of the reduced matrix. Then
\begin{eqnarray*}
\boldw_3 &=& 2\boldw_1 + \boldw_2\\
\boldw_4 &=& -2\boldw_1 + \boldw_2
\end{eqnarray*}
Replacing each $\boldw_i$ with $\boldv_i$ gives the required result:
\begin{eqnarray*}
\boldv_3 &=& 2\boldv_1 + \boldv_2\\
\boldv_4 &=& -2\boldv_1 + \boldv_2
\end{eqnarray*}
\end{solution}

\ii Find the rank and nullity of each matrix by reducing it to row echelon form.
\bb
\ii
$$A =
\begin{bmatrix}[rrrrr]
1&0&-2&1&0\\
0&-1&-3&1&3\\
-2&-1&1&-1&3\\
0&1&3&0&-4
\end{bmatrix}
$$
\ii
$$A =
\begin{bmatrix}[rrrr]
1&3&1&3\\
0&1&1&0\\
-3&0&6&-1\\
3&4&-2&1\\
2&0&-4&-2
\end{bmatrix}
$$
\ee
\begin{solution}
\noindent (a) The matrix reduces to:
$$
\begin{bmatrix}[rrrrr]
1&0&-2&0&1\\
0&1&3&0&-4\\
0&0&0&1&-1\\
0&0&0&0&0
\end{bmatrix}
$$
Since the matrix has 3 leading 1's, $\rank(A)=3$. The rank-nullity theorem now tells us that $\nullity(A)=n-3=5-3=2$. 
\\
(b) 
$A$ reduces to:
$$
\begin{bmatrix}
1&0&-2&0\\
0&1&1&0\\
0&0&0&1\\
0&0&0&0\\
0&0&0&0
\end{bmatrix}
$$
This matrix has three leading 1's, which means $\rank(A)=3$ and $\nullity(A)=4-3=1$.
\end{solution} 
\ii Matrix $R$ is the reduced row echelon form of the matrix $A$.
$$
A=
\begin{bmatrix}[rrrr]
0&2&2&4\\
1&0&-1&-3\\
2&3&1&1\\
-2&1&3&-2
\end{bmatrix},
R = 
\begin{bmatrix}[rrrr]
1&0&-1&0\\
0&1&1&0\\
0&0&0&1\\
0&0&0&0
\end{bmatrix}
$$
\bb
\ii Compute the rank and nullity of A.
\ii Confirm that the rank an nullity satisfy the rank-nullity theorem.
\ee
\begin{solution}
\noindent 
(a) Since $R$ has three non-zero rows, $\rank(A) = 3$. There is one free variable, so $\nullity(A) = 1$.
\\
(b)
The matrix $A$ is $4\times 4$. Thus $n = 4$. In our example we have 
\begin{align*}
\rank(A) + \nullity(A) &=3+1 &\text{(from (a))}\\
&=4
\end{align*}
Thus $\rank(A) + \nullity(A)=n$ as predicted by the theorem. 
\end{solution}
\ii Give two explicit matrices $A$ and $B$ of the same size satisfying $\rank(A)=\rank(B)$, but $\rank(A^2)\ne \rank(B^2)$. 
\\
\begin{solution}
\noindent
Let $A=\begin{bmatrix}
1&1\\
-1&-1
\end{bmatrix}$ and $B=\begin{bmatrix}
1&1\\ 1&1
\end{bmatrix}$. Both clearly have rank 1. Since $A^2=\boldzero$, we have $\rank(A^2)=0$. On the other hand $B^2=\begin{bmatrix}
2&2\\ 2&2
\end{bmatrix}$, which has rank 1.    
\end{solution}
\ii Prove: an $n\times n$ matrix $A$ satisfies $\CS(A)\subseteq \NS(A)$ if and only if  $A^2=\boldzero$. 
\\
Produce an explicit nonzero example of such a matrix. 
\\
\begin{solution}
\noindent Write $A=\begin{bmatrix}
\vert &\vert & &\vert\\
\hspace{5pt} \boldv_1&\hspace{5pt}\boldv_2&\cdots &\hspace{5pt}\boldv_n\\
\vert &\vert & &\vert
\end{bmatrix}$. 
Then we have 
\begin{align*}
A^2=\boldzero 
&\Longleftrightarrow 
\begin{bmatrix}
\vert &\vert & &\vert\\
\hspace{5pt} A\boldv_1&\hspace{5pt}A\boldv_2&\cdots &\hspace{5pt}A\boldv_n\\
\vert &\vert & &\vert
\end{bmatrix}=\boldzero &\text{(column method)}\\
&\Longleftrightarrow A\boldv_j=\boldzero \text{ for all <m>1\leq j\leq n</m>} \\
&\Longleftrightarrow \boldv_j\in\NS(A) \text{ for all <m>1\leq j\leq n</m>}\\
&\Longleftrightarrow \Span\left(\{ \boldv_1,\boldv_2,\dots, \boldv_n\}\right)\subseteq\NS(A)\\
&\Longleftrightarrow \CS(A)\subseteq \NS(A)
\end{align*}  
The penultimate if and only only is a consequence of the general fact that a subspace $W$ contains $\Span(\{\boldw_1,\dots, \boldw_r\})$ if and only if it contains $\boldw_i$ for all $i$. 
\end{solution} 
\ii If $A$ is an $m$ by $n$ matrix, what is the largest possible value for its rank and the smallest possible value for its nullity?
\\
\begin{solution}
\noindent Since the row vectors of $A$ lie in $R^n$ and the column vectors in $R^m$, the row space of $A$ is at most $n$-dimensional and the column space is at most $m$-dimensional. Since the rank of $A$ is the common dimension of its row and column space, it follows that the rank is at most the smaller of $m$ and $n$. Thus:
$$
\rank(A)\leq \min(m,n)
$$
Using the rank-nullity theorem we can find a similar bound for the nullity.
\begin{align*}
\nullity(A) &= n - \rank(A) &\text{ (Thm. 4.8.2)}\\
&\geq n - \min(m,n) &\text{(since <m>\rank(A)\leq\min(m,n)</m>)}\\
&= \max(n-m,0) &
\end{align*}
The last equality follows from the more general rule that \[ r-\min\{m,n\}=\max\{r-m,r-n\}.\]
\end{solution}
\ii Complete the following statements. Justify your answer. 
\bb
\ii If $A$ is a  3 by 5 matrix, then the rank of $A$ is at most $\dots$

\ii If $A$ is a 3 by 5 matrix, then the nullity of $A$ is at most $\dots$
\ee
\begin{solution}
\noindent
(a) By the previous exercise  $\rank(A)$ is at most $\min(3,5) = 3$. 
\\
(b) 
The nullity is at most 5. Consider
$$A =
\begin{bmatrix}[rrrrr]
0&0&0&0&0\\
0&0&0&0&0\\
0&0&0&0&0
\end{bmatrix}
$$
The rank of this matrix is 0. Thus the nullity must be 5. Since the rank can never be negative, the largest the nullity can be is 5.
\end{solution}
\ii Let $A$ be a 5 by 7 matrix with rank 4.
\bb
\ii What is the dimension of the solution space of $A\boldx = \textbf{0}$?
\ii Is $A\boldx = \textbf{b}$ consistent for all vectors $\textbf{b}$ in $R^5$?
\ee
\begin{solution}
\noindent (a) The solution space of $A\boldx = \textbf{0}$ is $\NS(A)$. Thus:
$$
\nullity(A) =n- \rank(A) = 7 - 4 = 3
$$
\\
(b) No. Since $\rank(A)=4$, $\dim(\CS(A))=4$. Since $\CS(A)\subset\R^5$ and $\dim(\R^5)=5$ it follows that $\CS(A)\subsetneq \R^5$. Then there is $\boldb\in\R^5$ such that $\boldb\notin\CS(A)$. Since 
\[
\CS(A)=\{\boldb\in\R^5\colon A\boldx=\boldb \text{ is consistent}\}
\]
it follows that we cannot solve $A\boldx=\boldb$ for this $\boldb$. 
\end{solution}
\ii \label{ex:rowscolumns} Prove: If a matrix A is not a square, then either the row vectors or the column vectors of $A$ are linearly dependent.
\\
\begin{solution}
\noindent Suppose $A$ is a $m$ by $n$ matrix with $n>m$. Then we have $n$ columns in $A$ which we can think of as being vectors in $R^m$. The columns of $A$ cannot be linearly independent since $R^m$ has dimension $m$, so any collection of vectors from $R^m$ with more than $m$ vectors must be linearly dependent. So the columns are dependent. Similarly, if $n<m$, then you can think of the rows of $A$ as a collection of $m$ vectors in $R^n$. Using the same argument we can see that rows must be dependent.
\end{solution}
\ii True or false. If true, provide a proof; if false, give an explicit counterexample. 
\bb
\ii Either the row vectors or the column vectors of a square matrix are linearly independent.

\ii A matrix with linearly independent row vectors and linearly independent column vectors is square.
\ii The nullity of a nonzero $m$ by $n$ matrix is at most $m$.
\ii Adding one additional column to a matrix increases its rank by one.
\ii The nullity of a square matrix with linearly dependent rows is at least one.
\ii If $A$ is a square and $A\boldx = \textbf{b}$ is inconsistent for some vector $\textbf{b}$, the the nullity of $A$ is zero.
\ii If a matrix $A$ has more rows than columns, then the dimension of the row space is greater than the dimension of the column space.
\ii $\rank(A)=\rank(A^T)$. 
\ii If $\nullity(A^T)=\nullity(A)$, then $A$ is square. 
\ee
\begin{solution}
\noindent (a) 
False. Consider the matrix:
$$
\begin{bmatrix}[rrr]
1&1&1\\
1&1&1\\
1&1&1
\end{bmatrix}
$$
Both the row vectors and the column vectors are linearly dependent.
\\
(b) True. Let $A$ be an $m$ by $n$ matrix with linearly independent row vectors and linearly independent column vectors. Assume for the purpose of contradiction that $m \neq n$. Then by Exercise \ref{ex:rowscolumns} either the row vectors or the column vectors are linearly dependent. This contradicts the fact that $A$ has linearly independent row vectors and linearly independent column vectors. Thus the assumption is false and $m = n$, so $A$ is a square matrix.
\\
(c) False. The $m\times n$ zero matrix has nullity $n$. So if $n>m$ the nullity of $0_{mn}$ is greater than $n$.  
\\
(d) False. Consider:
$$
\begin{bmatrix}[rrr]
1&0&0\\
0&1&0\\
0&0&1
\end{bmatrix}
$$
Since the matrix has 3 leading 1's, the rank is 3. Now if we add a column of 1's
$$
\begin{bmatrix}[rrrr]
1&0&0&1\\
0&1&0&1\\
0&0&1&1
\end{bmatrix}
$$
Since the matrix has 3 leading 1's, the rank is 3.
\\
(e) True. Let $A$ be an $n$ by $n$ matrix. Since the rows are linearly dependent, the reduced form of $A$ has a row of zeros for the bottom row. Thus $$\rank(A) < n$$ And $$\nullity(A) = n - \rank(A) > 0$$
\\
(f) False. Use the invertibility theorem. Since $A$ is inconsistent for some vector $\textbf{b}$, part (e) is false. Thus part (o) is false. So $A$ does not have nullity 0.
\\
(g) 
False. By rank-nullity, the row space and the column space always have the same dimension.
\\
(h) True. We have $\rank(A)=\dim\CS(A)=\dim\RS(A^T)=\rank(A^T)$. 
\\
(i) True. Let $A$ be $m\times n$, and assume $\nullity(A)=\nullity(A^T)$.  Then by the rank-nullity theorem 
\begin{align*}
n&=\rank(A)+\nullity(A)&\text{(by rank-nullity theorem, since <m>A</m> is $m\times n$)}
\\
&=\rank(A^T)+\nullity(A) &\text{(since <m>\rank(A)=\rank(A^T)</m>)}\\
&=\rank(A^T)+\nullity(A^T) &\text{(by assumption)}\\
&=m &\text{(by rank-nullity theorem, since <m>A^T</m> is $n\times m$)}
\end{align*}

\end{solution}


\ee