\begin{frame}{Eigenvectors}
Let $T\colon V\rightarrow V$ be linear. We have seen that some choices of bases of $V$ are better than others for representing $T$ as a matrix. 
\bpause
The ideal situation is when $V$ happens to have a basis $B=\{\boldv_1,\dots, \boldv_n\}$ where $T(\boldv_i)=c_i\boldv_i$ for all $i$. The matrix $[T]_B$ in this case is diagonal!
\[
[T]_B=\begin{bmatrix}
c_1&0&\dots\\
0&c_2&0&\dots\\
\vdots &\\
0&0&\dots&c_n
\end{bmatrix}
\]  
\pause This motivates us to find as many vectors as we can satisfying $T(\boldv)=c\boldv$ for some $c$. Such vectors are called \alert{eigenvectors}.
\begin{definition}
Let $T\colon V\rightarrow V$ be linear. An {\bf eigenvector} of $T$ is a \alert{nonzero} vector $\boldv\ne\boldzero$ such that $T(\boldv)=\lambda\boldv$ for some constant $\lambda\in\R$.  
\end{definition}
\pause 
\alert{Comment:} why the switch to $\lambda$ all of a sudden? Tradition!  
\end{frame}
\begin{frame}{Examples by inspection}
Define $T\colon M_{22}\rightarrow M_{22}$ as $T(A)=A^T+A$. 

We have seen that 
\begin{eqnarray*}
T\left(\begin{bmatrix}[rr]
0&1\\
-1&0
\end{bmatrix}
\right)&=&\begin{bmatrix}[rr]
0&0\\
0&0
\end{bmatrix}=0\begin{bmatrix}[rr]
0&1\\
-1&0
\end{bmatrix}\\
T\left(\begin{bmatrix}[rr]
0&1\\
1&0
\end{bmatrix}
\right)&=&\begin{bmatrix}[rr]
0&2\\
2&0
\end{bmatrix}=2\begin{bmatrix}[rr]
0&1\\
1&0
\end{bmatrix}.
\end{eqnarray*}

\pause Thus $\begin{bmatrix}[rr]
0&1\\
-1&0
\end{bmatrix}$
is an eigenvector with eigenvalue 0, and $\begin{bmatrix}[rr]
0&1\\
1&0
\end{bmatrix}$ is an eigenvector with eigenvalue 2. 
\end{frame}
\begin{frame}{Examples by inspection}
Let $T_\theta\colon\R^2\rightarrow\R^2$ be rotation counterclockwise by $\theta$, $0<\theta<2\pi$.  

What are the eigenvectors/eigenvalues of $T_\theta$. (Think geometrically, do not use a matrix representation of $T_\theta$!)
\begin{bsolution}
Is there any vector $\boldv$ such that when we rotate it by $\theta$ we get a scalar multiple of $\boldv$?
\bspace
Our answer actually depends on exactly what $\theta$ is. 

If $\theta=\pi$, then $T_\pi$ is rotation by 180$^\circ$, and so $T_\pi(\boldv)=-\boldv$ for all $\boldv$. Thus everything is an eigenvector of $T_\pi$ with eigenvalue $-1$. 
\bspace
If $\theta\ne\pi$, then we see easily that $T_\theta$ has no eigenvectors. 
\end{bsolution}
\end{frame}
\begin{frame}{Examples by inspection}
Fix $\alpha$ with $0\leq \alpha<\pi$, let $\ell_\alpha$ be the line making an angle $\alpha$ with the positive $x$-axis, and let $T_\theta\colon\R^2\rightarrow\R^2$ be reflection through $\ell_\alpha$.

What are the eigenvectors/eigenvalues of $T_\alpha$. (Think geometrically, do not use a matrix representation of $T_\alpha$!)
\begin{bsolution}
If $\boldv$ lies along $\ell_\alpha$ to begin with, then its reflection through $\ell_\alpha$ is itself. This means $T_\alpha(\boldv)=\boldv$. Thus all vectors along $\ell_\alpha$ are eigenvectors with eigenvalue 1. 
\bspace
If $\boldv$ lies along the line perpendicular to $\ell_\alpha$, then its reflection is just $-\boldv$ (draw a picture). Thus all vectors along this line are eigenvectors with eigenvalue $-1$. 
\bspace 
Vectors pointing along any other line will not be eigenvectors. Draw a picture!

\end{bsolution}
\end{frame}
\begin{frame}{Finding eigenvalues of $T$ systematically}
Let $T\colon V\rightarrow V$ be linear, $\dim(V)=n$. 

Our first step for finding eigenvalues and eigenvectors of $T$ is to 
pick a basis $B$ for $V$ and represent $T$ with the matrix $A=[T]_B$.  
\bpause You get to choose the basis $B$ yourself, though typically we start with the standard basis of $V$. It is usually only after our eigenvector analysis that we can provide a cleverer choice of basis. 
\bpause
Once we have chosen an $A$ to represent $T$, we perform our analysis on $A$. As usual, anything we discover about $A$ will be true of $T$. For example:
\bb
\ii $A$ and $T$ have precisely the same eigenvalues.
\ii Eigenvectors of $A$ can be translated back to $V$ to get eigenvectors of $T$. 
\ee
\end{frame}
\begin{frame}{Finding eigenvalues of $A$}
\footnotesize
We now focus on finding eigenvalues of an $n\times n$ matrix $A$. Follow this chain of equivalences:
\begin{eqnarray*}
\text{$\lambda\in\R$ is an eigenvalue of $A$}&\Leftrightarrow&
\pause A\boldx=\lambda\boldx \text{ for some $\boldx\ne 0$}
\ \text{ (by def.)}\\
\pause&\Leftrightarrow&\lambda\boldx-A\boldx=\boldzero\\
\pause&\Leftrightarrow&(\lambda I_n)\boldx-A\boldx=\boldzero \ \text{ (since $\lambda\boldx=(\lambda I_n)\boldx$)}\\
\pause&\Leftrightarrow&(\lambda I_n-A)\boldx=\boldzero \text{ for some $\boldx\ne \boldzero$}\\
\pause&\Leftrightarrow&\lambda I_n-A \text{ is noninvertible}\\
\pause&\Leftrightarrow&\det(\lambda I_n-A)=0 \ \text{ (invertibility theorem)}
\end{eqnarray*}
\pause 
Thus we see that $\lambda$ is an eigenvalue if and only if $\det(\lambda I_n-A)=0$. 
\bpause 
Set $p(t)=\det(tI_n-A)$. This is polynomial function in $t$, as you will see shortly. We have just shown that the eigenvalues of $A$ are precisely the \alert{real roots} of $p(t)$. Better give this important object a name!
\pause
\begin{definition}
Let $A$ be $n\times n$. The {\bf characteristic polynomial} of $A$ is the polynomial 
\[
p(t)=\det(tI_n-A).
\]
\end{definition}
\end{frame}
\begin{frame}
\alert{Example}. Find all eigenvalues of $A=\begin{bmatrix}
1&1\\
2&-1
\end{bmatrix}$. 
\begin{bsolution}
Compute:

$
p(t)=\det(tI-A)=\val{\begin{array}{cc}t-1&-1\\ -2&t+1 \end{array}}=t^2-3.
$

\pause The eigenvalues are the roots of $p(t)$: $
\lambda_1=\sqrt{3}, \lambda_2=-\sqrt{3}$.
\end{bsolution}

\pause
\alert{Example}. Find all eigenvalues of $A=\begin{bmatrix}[rrr]
2&-1&-1\\
-1&2&-1\\
-1&-1&2
\end{bmatrix}$.
\begin{bsolution}
Compute:

$
p(t)=\det(tI-A)=\val{\begin{array}{ccc}t-2&1&1\\ 1&t-2&1\\ 1&1&t-2 \end{array}}=t^3-6t^2+9t=t(t-3)^2.
$

\pause The eigenvalues are the roots of $p(t)$: $\lambda_1=0$, $\lambda_2=3$.
\end{bsolution}
\end{frame}
\begin{frame}{Facts about the characteristic polynomial}
%\footnotesize
Let $A$ be $n\times n$ and let $p(t)=\det(tI-A)$ be the characteristic polynomial. 
\bb
\pause\ii $p(t)=t^n+a_{n-1}t^{n-1}+\cdots +a_1t+a_0$; in particular, $p(t)$ is {\em monic} (leading coefficient is 1), of degree $n$. 
\pause\ii  $p(t)$ has at most $n$ distinct roots. Some of these roots may be complex. Thus it is possible to have less than $n$ eigenvalues. Indeed, there may be no eigenvalues. 
\bpause 
What about the possible complex roots? Let's call these \alert{complex eigenvalues}. For now they do not count as eigenvalues for us, as it is impossible to find a nonzero $\boldx\in\R^n$ with a complex eigenvalue. 

\ee
\end{frame}
\begin{frame}{Fancy facts about $p(t)$}
\footnotesize
Let $p(t)=\det(tI-A)=t^n+a_{n-1}t^{n-1}+\cdots +a_1t+a_0$ be the characteristic polynomial. \\
Over $\C$ we can factor $p(t)=(t-\lambda_1)(t-\lambda_2)\cdots (t-\lambda_n)$, where some of the $\lambda_i$ may be complex numbers. 
\bpause Then we have: 
\begin{eqnarray*}
-(\lambda_1+\lambda_2+\cdots +\lambda_n)&=&a_{n-1}=-\tr(A)\\
(-1)^n\lambda_1\lambda_2\cdots\lambda_n&=&a_0=(-1)^n\det(A)
\end{eqnarray*}
\pause An easy consequence of these equalities is the following:
\begin{eqnarray*}
\tr(A)&=&\lambda_1+\lambda_2+\cdots +\lambda_n\\
\det(A)&=&\lambda_1\lambda_2\cdots\lambda_n
\end{eqnarray*}
In plain English: $\tr(A)$ is the sum of the eigenvalues; $\det(A)$ is the product of the eigenvalues. 
\bpause 
When $A$ is $2\times 2$, the above implies $p(t)=t^2-\tr(A)t+\det(A)$: a useful shortcut when having to compute eigenvalues of $2\times 2$ matrices! 
\end{frame}
\begin{frame}
 Let's make our list of facts official. 
 \begin{theorem}[Characteristic polynomial theorem]
 Let $A\in M_{nn}$, and let $p(t)=\det(tI_n-A)$ be its characteristic polynomial. 
 \bb[(a)]
 \ii $p(t)=t^n+a_{n-1}t^{n-1}+\cdots +a_1t+a_0$; in particular, $p(t)$ is {\em monic} (leading coefficient is 1), of degree $n$. 
 \ii The real roots of $p(t)$ are the (real) eigenvalues of $A$. Since $\deg p(t)=n$, there are at most $n$ distinct eigenvalues of $A$. 
 \ii Let $\lambda_1, \lambda_2, \dots, \lambda_n$ be the roots of $p(t)$. Then 
 \begin{align*}
 \tr(A)&=\sum_{i=1}^n\lambda_i=-a_{n-1}\\
 \det(A)&=\prod_{i=1}^n\lambda_i=(-1)^na_0
 \end{align*}
 \ee
 \end{theorem}
\end{frame}
\begin{frame}{Finding eigenvectors of $A$}
Now suppose we have found an eigenvalue $\lambda$ of $A$. 
\\
Our chain of equivalences from earlier told us that the eigenvectors of $A$ with eigenvalue $\lambda$ are precisely the \alert{nonzero} solutions to the matrix equation $(\lambda I_n-A)\boldx=\boldzero$. 
\bpause 
Thus to find all eigenvectors with eigenvalue $\lambda$ we simply compute $\NS(\lambda I_n-A)$ using Gaussian elimination.
\pause
\begin{definition}
Let $\lambda$ be an eigenvalue of $A$. We define the $\lambda$-eigenspace of $A$ to be the space 
\[
W_\lambda:=\NS(\lambda I_n-A).
\]
The nonzero elements of $W_\lambda$ are precisely the eigenvectors of $A$ with eigenvalue $\lambda$. 
\end{definition}
\end{frame}
\begin{frame}
Let's compute the eigenspaces of our previous examples. 

\alert{Example}. $A=\begin{bmatrix}
1&1\\
2&-1
\end{bmatrix}$, $\lambda_1=\sqrt{3}$, $\lambda_2=-\sqrt{3}$.
\begin{bsolution}
\pause Compute:

$W_{\sqrt{3}}=\NS(\sqrt{3}I-A)=\NS\left(\begin{bmatrix}\sqrt{3}-1&-1\\ -2&\sqrt{3}+1 \end{bmatrix}\right)\pause=\Span(\left\{ \begin{bmatrix}\sqrt{3}+1\\2\end{bmatrix} \right\})$

\pause
$W_{-\sqrt{3}}=\NS(-\sqrt{3}I-A)=\NS\left(\begin{bmatrix}-\sqrt{3}-1&-1\\ -2&-\sqrt{3}+1 \end{bmatrix}\right)\pause=\Span(\left\{ \begin{bmatrix}-\sqrt{3}+1\\ 2\end{bmatrix} \right\})$
\bpause Thus any multiple of $\boldv_1=(\sqrt{3}+1,2)$ is an eigenvector of $A$ with eigenvalue $\sqrt{3}$, and any multiple of $\boldv_2=(-\sqrt{3}+1,2)$ is an eigenvector of $A$ with eigenvalue $-\sqrt{3}$. 
\bpause
Check:
\bspace
$A\begin{bmatrix}\sqrt{3}+1\\ 2 \end{bmatrix}=\begin{bmatrix} 3\sqrt{3}+3\\ 2\sqrt{3} \end{bmatrix}=\sqrt{3}\begin{bmatrix}\sqrt{3}+1\\ 2 \end{bmatrix} \checkmark$. 
\bspace
$A\begin{bmatrix}-\sqrt{3}+1\\ 2 \end{bmatrix}=\begin{bmatrix} -3\sqrt{3}+3\\ -2\sqrt{3} \end{bmatrix}=-\sqrt{3}\begin{bmatrix}-\sqrt{3}+1\\ 2 \end{bmatrix} \checkmark$. 

\end{bsolution}
\end{frame}
\begin{frame}
Let's compute the eigenspaces of our previous examples. 

\alert{Example}. $A=\begin{bmatrix}[rrr]
2&-1&-1\\
-1&2&-1\\
-1&-1&2
\end{bmatrix}$, $\lambda_1=0$, $\lambda_2=3$. 
\begin{bsolution}
Compute:

$W_0=\NS(0I-A)=\NS\left(\begin{bmatrix}[rrr]
2&-1&-1\\
-1&2&-1\\
-1&-1&2
\end{bmatrix}
\right)=\Span\left(\left\{\boldv_1=\begin{bmatrix}1 \\ 1 \\ 1 \end{bmatrix}\right\}\right)$. 
\bpause
$W_3=\NS(3I-A)=\NS(\left( \begin{bmatrix} 1&1&1\\ 1&1&1\\ 1&1&1   \end{bmatrix} \right)
=\Span\left(\left\{\boldv_2= \begin{bmatrix}[r] 1 \\ -1 \\ 0 \end{bmatrix}, \boldv_3=\begin{bmatrix}[r]1 \\ 0 \\ -1 \end{bmatrix}\right\}\right)$. 
\bpause
Thus the eigenvectors of $A$ with eigenvalue 0 are vectors of the form $c\boldv_1$, and the eigenvectors of $A$ with eigenvalue 3 are vectors of the form $c\boldv_2+d\boldv_3$. 

\bpause\alert{Comment}. Note that in general $W_0=\NS(0I-A)=\NS(-A)=\NS(A)$. (Think about why the last equality is true.) 

Thus the eigenvectors of $A$ with eigenvalue 0 are precisely the nonzero vectors of $\NS(A)$, should there be any at all!  
\end{bsolution}
\end{frame}

