\begin{frame}{3.1-3.2: executive summary}
\alert{Definitions:} $\R^n$, $n$-vectors, linear combination, length (norm), dot product, distance, angle between vectors, 
\bspace
\alert{Procedures:} none really beyond computing with the various operations mentioned above. 
\bspace
\alert{Theorems:} Cauchy-Schwarz, Triangle Inequality.
\end{frame}
\begin{frame}{3.1-3.2: $\R^n$ and the dot product}
\footnotesize
\begin{definition}
We define $\R^n$ to be the set of {\color{red} all} $n$-tuples: i.e., 
\[
\R^n=\{(t_1,t_2,\dots t_n)\colon t_i\in\R \}.
\]
We call $\R^n$ {\bf $n$-space}, call elements of $\R^n$ {\bf $n$-vectors}, and denote elements of $\R^n$ with bold, lowercase letters: typically, $\boldv$, $\boldw$, and $\boldu$. 
\end{definition}
\pause
\begin{comment}
Technically an $n$-tuple is not the same thing as a matrix. An $n$-tuple $\boldv=(t_1,t_2,\dots ,t_n)$ is called a {\bf comma-delimited} form of a sequence, whereas matrices represent sequences as {\em rectangular arrays}. 
\bpause
Indeed there are two natural choices of matrices representing the $n$-tuple $\boldv$: the {\bf column-vector  form}
\[
\begin{bmatrix}
t_1\\t_2\\ \vdots \\ t_n
\end{bmatrix},
\]
or the {\bf row-vector form}
\[
\begin{bmatrix}t_1& t_2&\dots &t_n \end{bmatrix}.
\]
\end{comment}
\end{frame}
\begin{frame}{Equality and arithmetic in $\R^n$}
\footnotesize
We can transport all of the concepts/operations involving row and column vectors to elements of $\R^n$. 
\begin{definition}
Two tuples $\boldv=(v_1,v_2,\dots, v_m)$ and $\boldw=(w_1,w_2,\dots, w_n)$ are defined to be {\bf equal}, denoted $\boldv=\boldw$, if \alert{(i)} $m=n$, and 
\alert{(ii)} $v_i=w_i$ for all $1\leq i\leq n$. 
\end{definition}
\pause
\begin{definition}
Let $\boldv=(v_1,v_2,\dots,v_n)$ and $\boldw=(w_1,w_2,\dots w_n)$, and let $k\in\R$ be any scalar. We define
\bb
\ii {\bf zero vector}: $\boldzero=(0,0,\dots,0)$.
\ii {\bf sum}: $\boldv+\boldw=(v_1+w_1,v_2+w_2,\dots, v_n+w_n)$.
\ii {\bf scalar multiplication}: $k\boldv=(kv_1,kv_2,\dots, kv_n)$,
\ii {\bf additive inverse}: $-\boldv=(-1)\boldv=(-v_1,-v_2,\dots,-v_n)$.
\ii {\bf difference}: $\boldw-\boldv=\boldw+(-\boldv)=(w_1-v_1,w_2-v_2,\dots ,w_n-v_n)$. 
\ee
\pause A {\bf linear combination} of $n$-vectors $\boldv_1, \boldv_2,\dots, \boldv_r\in\R^n$ is an $n$-vector of the form 
\[
k_1\boldv_1+k_2\boldv_2\cdots +k_r\boldv_r,
\]
where $k_i\in\R$ are scalars. 
\end{definition}
\end{frame}
\begin{frame}
Your professor will now draw some pictures of arithmetic in $\R^2$. 
\bspace
Make sure your professor includes examples of $-\boldv$, $k\boldv$, $\boldv+\boldw$, and $\boldw-\boldv$. 
\end{frame}
\begin{frame}{3.2: length (or norm)}
\footnotesize
\begin{definition}
Given an element $\boldv=(v_1,v_2,\dots ,v_n)\in\R^n$, we define its {\bf length} (or {\bf norm}) as follows:
\[
\norm{\boldv}=\sqrt{v_1^2+v_2^2+\cdots +v_n^2}.
\]
\end{definition}
\pause The facts below follow immediately from the definition of norm. 
\begin{theorem}
Let $\boldv\in\R^n$. 
\bb[(1)]
\ii $\norm{\boldv}\geq 0$.
\ii $\norm{\boldv}=0$ if and only if $\boldv=\boldzero$.
\ii $\norm{k\boldv}=\val{k}\norm{\boldv}$.
\ee
\pause Note the subtle notational difference in (3): $\val{k}$ is the absolute value of the real number $k$; $\norm{\boldv}$ is the norm of the $n$-vector $\boldv$. 
\end{theorem}
\pause
\begin{definition}
A {\bf unit vector} is any $n$-vector $\boldu\in\R^n$ with $\norm{\boldu}=1$. 

In general, given any $\boldv\in\R^n$, the vector 
$
\frac{1}{\norm{\boldv}}\boldv=\frac{\boldv}{\norm{\boldv}}
$
is a unit vector. (Requires proof!)
\end{definition}


\end{frame}
\begin{frame}{Distance}
We use the notion of length to define a notion of distance between two vectors $\boldv, \boldw\in\R^n$. Specifically, we define the distance to be the length of their difference! 
\pause\begin{definition}
Given $\boldv,\boldw\in\R^n$ we define the {\bf distance between $\boldv$ and $\boldw$} to be 
\[
d(\boldv,\boldw)=\norm{\boldv-\boldw}=\sqrt{(v_1-w_1)^2+(v_2-w_2)^2+\cdots +(v_n-w_n)^2}.
\]
\end{definition}
\end{frame}
\begin{frame}{Dot product}
\begin{definition}
Given $\boldv=(v_1,\dots,v_n), \boldw=(w_1,\dots,w_n)\in\R^n$, we define the {\bf dot product} as 
\[
\boldv\cdot\boldw=v_1w_1+v_2w_2+\cdots +v_nw_n.
\]
\end{definition}
\pause Note the hybrid nature of the dot product operation. It takes as input a pair of $n$-vectors, $\boldv$ and $\boldw$, and hands back a real number:
\[
\pause (\boldv,\boldw)\pause\mapsto \boldv\cdot\boldw\in\R.
\]
\pause There are many obvious properties of the dot product. See your text for an exhaustive list. Here are a few:
\bb
\ii $\boldv\cdot\boldw=\boldw\cdot\boldv$,
\ii $\boldzero\cdot\boldv=0$,
\ii $\boldu\cdot(\boldv+\boldw)=\boldu\cdot\boldv+\boldu\cdot\boldw$.
\ee
\end{frame}
\begin{frame}{Dot product as matrix multiplication}
In fact, if we identify $n$-vectors with column vectors (or row vectors), then we can express the dot product as a certain matrix multiplication, in which case we get lots of properties for free. 
\bpause
\alert{Columns:} think of $\boldv$ and $\boldw$ as $n\times 1$ column vectors. Then 
\[
\boldv\cdot\boldw=\boldv^T\boldw.
\]
\bpause
\alert{Rows:} think of $\boldv$ and $\boldw$ as $1\times n$ row vectors. Then 
\[
\boldv\cdot\boldw=\boldv\boldw^T
\]
\end{frame}
\begin{frame}{Matrix multiplication via dot products}
The last observations also give us yet another way of looking at matrix multiplication! 
\bspace
Take $A=[a_{ij}]_{m\times n}$ and $B=[b_{ij}]_{n\times r}$. Think of $A=\begin{bmatrix} \bolda_1\\ \vdots \\ \bolda_m\end{bmatrix}$ as a collection of $m$ row vectors in $\R^n$, and $B=\begin{bmatrix}\boldb_1&\cdots &\boldb_r \end{bmatrix}$ as a collection of $r$ column vectors in $\R^n$. 
\bpause 
Set $AB=C=[c_{ij}]_{m\times r}$. It follows directly from the original definition of matrix multiplication that 
\[
c_{ij}=\bolda_i\cdot\boldb_j.
\] 
That is, the $ij$-th entry of $AB$ is simply the dot product of the $i$-th row of $A$ and the $j$-th column of $B$!
\end{frame}
\begin{frame}{Norm and distance via dot product}
It follows directly from the definitions that 
\[
\norm{\boldv}=\sqrt{\boldv\cdot\boldv},
\]
and hence that 
\begin{eqnarray*}
d(\boldv,\boldw)&=&\norm{\boldv-\boldw}\\
&=&\sqrt{(\boldv-\boldw)\cdot (\boldv-\boldw)}\\
&=&\sqrt{\boldv\cdot \boldv-2\boldv\cdot\boldw+\boldw\cdot\boldw}.
\end{eqnarray*}
\end{frame}

\begin{frame}{Angle between $n$-vectors}
We used the norm to define distance between $n$-vectors. We can use the dot product to define the angle between $n$-vectors.
\pause\begin{definition}
The {\bf angle between two vectors} $\boldv, \boldw\in\R^n$ is defined as the unique $\theta\in[0,\pi]$ satisfying 
\[
\cos(\theta)=\frac{\boldv\cdot\boldw}{\norm{\boldv}\norm{\boldw}}.
\]
\end{definition}
\pause
\alert{Comments.}
\bb
\ii For this definition to make any sense we need to be able to solve that equation for $\theta$. Is this possible? See the \alert{Cauchy-Schwarz inequality}!
\pause\ii Ask your professor to draw a picture in $\R^2$ showing how this definition conforms with our understanding of angles in the $xy$-plane. 
\ee
\end{frame}
\begin{frame}{Cauchy-Schwarz and the like}
%\footnotesize
\begin{theorem}[Cauchy-Schwarz inequality]
For any $\boldv,\boldw\in\R^n$ we have 
\[
\val{\boldv\cdot\boldw}\leq\norm{\boldv}\norm{\boldw}.
\]
\end{theorem}
\pause
\begin{proof}
Proof postponed until later!
\end{proof}
\pause
\begin{corollary}[Triangle inequalities]
For any $\boldu, \boldv, \boldw\in\R^n$ we have 
\bb[(1)]
\ii $\norm{\boldu+\boldv}\leq\norm{\boldu}+\norm{\boldv}$,
\ii $d(\boldu,\boldv)\leq d(\boldu,\boldw)+d(\boldw,\boldv)$.
\ee
\end{corollary}
\pause Ask your professor to draw a picture illustrating the triangle inequalities in $\R^2$. 
\end{frame}