\begin{frame}{\ref{s:linear systems}.\ref{ss:algebraic}: executive summary}\footnotesize
\alert{Definitions}: zero matrix $\boldzero_{m\times n}$, identity matrix $I_n$, inverse matrix $A^{-1}$, invertibility, $A^r$ and $A^{-r}$ for nonnegative integer $r\geq 0$.  
\bspace
\alert{Procedures}: none!
\bspace
\alert{Theorems}: Many! Lots of familiar rules of real number algebra (associativity, distributivity, etc.) also hold for matrix algebra. However some important properties fail: e.g., commutativity and multiplicative cancellation. We also learn some properties about taking the transpose ($A\mapsto A^T$) and its relation with taking the inverse ($A\mapsto A^{-1}$).  
\end{frame}
\begin{frame}{1.4: algebraic properties of matrices}\footnotesize
For the most part, the matrix operations of $\cdot$ and $+$ behave in much the same manner as their corresponding real number operations. \begin{theorem}[Properties of matrix arithmetic]\label{th:matrix-arith}
The following properties hold for all matrices $A, B, C$ and scalars $a, b, c$ for which the given expression makes sense. 
\begin{multicols}{2}
\bb[(a)]
\ii $A+B=B+A$
\ii $A+(B+C)=(A+B)+C$
\ii $A(BC)=(AB)C$
\ii $A(B+C)=AB+AC$
\ii $(B+C)A=BA+CA$
\ii $A(B-C)=AB-AC$
\ii $(B-C)A=BA-CA$
\ii $a(B+C)=aB+aC$
\ii $a(B-C)=aB-aC$
\ii $(a+b)C=aC+bC$
\ii $(a-b)C=aC-bC$
\ii $a(bC)=(ab)C$
\ii $a(BC)=(aB)C=B(aC)$. 
\ee 
\end{multicols}
\end{theorem} 
How does one actually prove one of these properties? 
\bpause
These are all \alert{matrix equalities}. Using the definition of matrix equality, we must show (1) the LHS and RHS matrices are of the same size, and (2) that for each $(i,j)$, the $(i,j)$-th entry of the LHS is equal to the $(i,j)$-th entry of the RHS. Let's see how this works on an example. 

\end{frame}
\begin{frame}
\begin{proof}[Proof of Theorem \ref{th:matrix-arith} (c)]
Let $A=[a_{ij}]_{m\times r}$, $B=[b_{ij}]_{r\times s}$, $C=[c_{ij}]_{s\times n}$. Then 
\[
A(BC)=(AB)C.
\]
\bpause
(1) One checks easily that the matrices on the LHS and RHS are both of size $m\times n$, so we move straight to (2) using our index notation:
 \scriptsize
\pause \begin{eqnarray*}
& &\Bigl(A(BC)\Bigr)_{ij}=\sum_{\ell=1}^ra_{i\ell}(BC)_{\ell j}
\pause=\sum_{\ell=1}^ra_{i\ell}\left(\sum_{k=1}^sb_{\ell k}c_{kj}\right)\\
\pause&=&\sum_{\ell=1}^r\sum_{k=1}^sa_{i\ell}(b_{\ell k}c_{kj}) \ \text{ (dist. prop.)}\\
\pause&=&\sum_{k=1}^s\sum_{\ell=1}^r(a_{i\ell}b_{\ell k})c_{kj} \ \text{ (comm.+assoc. prop.)}\\
\pause&=&\sum_{k=1}^s\left(\sum_{\ell=1}^ra_{i\ell}b_{\ell k}\right)c_{kj} \ \text{ (dist. prop.) }\\
\pause&=&\sum_{k=1}^s(AB)_{ik}c_{kj}
\pause=\Bigl((AB)C\Bigr)_{ij}
\end{eqnarray*}
\pause This proves their $ij$-th entries are equal, and hence $A(BC)=(AB)C$. 
\end{proof}
\end{frame}
\begin{frame}{Zero matrix and additive inverses}
Another similarity between matrix algebra and real number algebra is the existence of \alert{additive inverses}.
\bpause
\alert{Definition:} the {\bf $m\times n$ zero matrix} is the matrix $\boldzero_{m\times n}$ all of whose entries are 0. 

Note we have a different zero matrix for every possible size of matrix. 
\pause
\begin{theorem}
Let $A=[a_{ij}]$ be $m\times n$. Then the matrix $-A:=(-1)A=[-a_{ij}]$ is an {\bf additive inverse} of $A$ in the sense that 
\[
A+(-A)=(-A)+A=\boldzero_{m\times n}.
\]
\end{theorem}
\pause
\alert{Comment:}
thus every $m\times n$ matrix has an additive inverse. This allows us to ``cancel" matrices \alert{additively}. Example: solve the following equation for $B$.  
\begin{eqnarray*}
A+B&=&3A\\
\pause (-A)+A+B&=&(-A)+3A\\
\pause \boldzero_{m\times n}+B&=&(-1)A+3A\\
\pause B&=&2A
\end{eqnarray*} 
I give all the gory details, but now the point should be clear; we can cancel a multiple of $A$ on both sides, just as we do in normal algebra!  
\end{frame}
\begin{frame}{Abnormalities}
There are two very important properties that matrix algebra fails to enjoy. 
\begin{theorem}
Matrix multiplication is NOT \alert{commutative}, and there are nontrivial \alert{zero divisors}. That is: 
\bb
\pause\ii Fix $n$. There are (many) $n\times n$ matrices $A, B$ such that 
\[
AB\alert{\ne}BA.
\]
\alert{Example:} verify that 
$\begin{bmatrix}
1&0\\
0&0
\end{bmatrix}
\begin{bmatrix}
0&1\\
0&0
\end{bmatrix}
\ne
\begin{bmatrix}
0&1\\
0&0
\end{bmatrix}
\begin{bmatrix}
1&0\\
0&0
\end{bmatrix}.
$
\pause\ii Given matrices $A_{m\times n}, B_{n\times r}$, 
\[
AB=0_{m\times r}\alert{\not\Rightarrow} A=0_{m\times n} \text{ or } B=0_{n\times r}.
\]
In English: if the product of two matrices is the zero matrix, we CANNOT conclude that one of them is the zero matrix. 
\bpause
\alert{Example:} verify that 
$
\begin{bmatrix}
0&1\\
0&0
\end{bmatrix}
\begin{bmatrix}
1&0\\
0&0
\end{bmatrix}
=\begin{bmatrix}
0&0\\ 0&0
\end{bmatrix}.
$
\ee 
\end{theorem}
\end{frame}
\begin{frame}
It is worth pointing out how important the second failed property is to matrix algebra. 
\bpause 
In normal real number algebra, if $a\ne 0$ and $ab=ac$, we may ``cancel" the $a$ and conclude $b=c$. Not so in matrix algebra!
\pause \begin{theorem}[Failure of cancellation]
\begin{eqnarray*}
\text{Suppose $A\ne 0_{m\times n}$, then  } AB=AC &\alert{\not\Rightarrow}& B=C\\
\pause \text{Suppose $D\ne 0_{s\times t}$,  then } BD=CD &\alert{\not\Rightarrow}& B=D
\end{eqnarray*}
\end{theorem}
\bpause Let that sink in. Cancellation is one of the most used properties in normal algebra. It is in general no longer available to us when doing matrix algebra. 
\bpause \alert{Examples:} (you should verify). 
\[
\begin{bmatrix}
1&1&0\\
0&0&0
\end{bmatrix}\begin{bmatrix}
1&0\\
0&0\\
0&0
\end{bmatrix}
=
\begin{bmatrix}
1&1&0\\
0&0&0
\end{bmatrix}\begin{bmatrix}
0&0\\
1&0\\
0&0
\end{bmatrix}
\]
\[
\begin{bmatrix}
1&0&0\\
0&0&0
\end{bmatrix}\begin{bmatrix}
1&0\\
1&0\\
0&0
\end{bmatrix}
=
\begin{bmatrix}
0&1&0\\
0&0&0
\end{bmatrix}\begin{bmatrix}
1&0\\
1&0\\
0&0
\end{bmatrix}
\]
\end{frame}
\begin{frame}{Invertibility}\footnotesize
What allows cancellation of a nonzero $a$ in the reals is the existence of its \alert{multiplicative inverse}, the element $a^{-1}=\frac{1}{a}$, which satisfies $a^{-1}a=1$. 

\bpause Not all nonzero matrices $A$ have a multiplicative inverse. The ones that do are called \alert{invertible}. In order to define this properly, we need to identify a matrix that acts as the number one does: i.e., a \alert{multiplicative identity}.  
\pause
\begin{definition}
The {\bf identity matrix of size $n\times n$} is the $n\times n$ matrix $I_n$ with 1's along the diagonal (i.e., $a_{ii}=1$) and 0's everywhere else (i.e., $a_{ij}=0$ for $i\ne j$). For example:
\[
I_1=\begin{bmatrix} 1\end{bmatrix}, I_2=\begin{bmatrix} 1&0\\
0&1\end{bmatrix}, I_3=\begin{bmatrix}1&0&0\\
0&1&0\\ 0&0&1\end{bmatrix}, \text{ etc.}
\]

\end{definition} 
\pause\begin{theorem}[Multiplicative identity]
For any $A_{m\times n}$ we have
\begin{eqnarray*}
I_mA&=&A\\
\pause AI_n&=&A.
\end{eqnarray*}
\end{theorem}
\pause\begin{proof}
Use either column or row method to perform the multiplications involved! 
\end{proof}
\end{frame}
\begin{frame}{Invertibility}\footnotesize
\begin{definition}
A square matrix $A_{n\times n}$ is {\bf invertible} if there is a matrix $B_{n\times n}$ such that 
\begin{eqnarray*}
AB&=&I_n, \text{ and}\\
BA&=&I_n
\end{eqnarray*} 
In this case we call $B$ an {\bf inverse} of $A$, and we say that $A$ and $B$ are {\bf inverses} of one another. 
\end{definition}
\bpause
\alert{Theorem (uniqueness of inverses):} 
an invertible matrix $A$ has exactly one inverse. That is, if 
\[
AB=BA=I_n
\]
and 
\[
AC=CA=I_n,
\]
then $B=C$. 

We denote by $A^{-1}$ the inverse of $A$. 

\pause
\begin{proof}
Suppose we have such matrices $B$ and $C$. 

Then $I_n=AB=AC\pause\Rightarrow BAB=BAC\pause\Rightarrow I_nB=I_nC\pause\Rightarrow B=C$.
\end{proof}
\end{frame}
\begin{frame}{$2\times 2$ matrices}
When is a $2\times 2$ matrix $A=\abcdmatrix{a}{b}{c}{d}$ invertible?
\bpause 
If $ad-cb\ne 0$, then we can easily show that 
\[
A^{-1}=\frac{1}{ad-bc}\abcdmatrix{d}{-b}{-c}{a}
\]
is the inverse. 
\bpause 
Thus 
\[ ad-bc\ne 0\Rightarrow  A \text{ invertible} .\]
\bpause It turns out that the converse is also true, that is 
\[ ad-bc\ne 0\Leftrightarrow  A \text{ invertible} .\]
We will prove this a littler further down the line. 
\end{frame}
\begin{frame}\footnotesize
\begin{theorem}
Let $A,B$ be $n\times n$ matrices.
\[
A \text{ and } B  \text{ invertible}\Rightarrow AB \text{ invertible},
\]
and in fact $(AB)^{-1}=B^{-1}A^{-1}$. 
\end{theorem}
\pause
\begin{proof}
In the absence of more theory, we can only prove invertibility by providing an inverse. The last statement offers up a candidate: viz., $C=B^{-1}A^{-1}$. We need only show that $C$ satisfies the relevant properties: 
\begin{eqnarray*}
C(AB)&=&(B^{-1}A^{-1})AB=B^{-1}A^{-1}AB=B^{-1}IB=B^{-1}B=I\\
\pause
(AB)C&=&(AB)B^{-1}A^{-1}=ABB^{-1}A^{-1}=AIA^{-1}=AA^{-1}=I.
\end{eqnarray*}
\end{proof}
\pause
\begin{corollary}
More generally, 
\[
A_1,A_2,\dots A_r \text{ invertible}\Rightarrow (A_1A_2\cdots A_r)\text{ invertible},
\]
and in fact $(A_1A_2\cdots A_r)^{-1}=A_r^{-1}A_{r-1}^{-1}\cdots A_1^{-1}$. 
\end{corollary}
\pause \alert{Question:} do the implication arrows above go the other way?\\ \pause \alert{Answer:} we shall see. 
\end{frame}
\begin{frame}{Powers of matrices}
\footnotesize
\begin{definition}
Let $A$ be any $n\times n$ matrix. For any integer $r\geq 0$ we define the $r$-th power of $A$, $A^{r}$, as follows: 
\bb[i]
\pause\ii $A^0=I_n$. 
\pause\ii $\ds A^r=\underbrace{A\cdot A\cdots A}_{r \text{ times}}$ for $r>0$.
\ee
\pause Furthermore, if $A$ is \alert{invertible}, we define 
\[
A^{-r}=\left(A^{-1}\right)^r,
\] 
for any positive integer $r>0$. 
\end{definition}
\pause
\begin{theorem}[Power rules]
\begin{multicols}{2}
\bb[(a)]
\ii $A^{r}A^s=A^{r+s}$
\ii $(A^r)^s=A^{rs}$
\ii $\left(A^{-1}\right)^{-1}=A$
\ii $\left(A^n\right)^{-1}=A^{-n}$
\ii $(kA)^n=k^nA^n$ for any scalar $k$. 
\ee
\end{multicols}
\end{theorem}
\pause\alert{Question:} $A^rB^r\stackrel{\large{\color{red}?}}{=}(AB)^r$
\end{frame}
\begin{frame}{Transposes and inverses}
\begin{theorem}
\begin{multicols}{2}
\bb[(a)]
\ii $\left(A^T\right)^T=A$
\ii $(A+B)^T=A^T+B^T$
\ii $(A-B)^T=A^T-B^T$
\ii $(kA)^T=kA^T$
\ii $(AB)^T=B^TA^T$
\ee
\end{multicols}
\end{theorem}
\pause\begin{theorem}
Let $A_{n\times n}$ be invertible. Then $A^T$ is invertible, and $\left(A^T\right)^{-1}=\left(A^{-1}\right)^T$. 
\end{theorem}
\end{frame}
\begin{frame}{More sample proofs}
Let's prove some of the last two theorems. 
\pause
\begin{proof}[Proof that $(A+B)^T=A^T+B^T$]
\begin{eqnarray*}
\left((A+B)^T\right)_{ij}&=&(A+B)_{ji} \ \text{ (def. of transp.)}\\
\pause &=&A_{ji}+B_{ji} \ \text{ (def. of +)}\\
\pause &=&(A^T)_{ij}+(B^T)_{ij} \ \text{ (def. of transp.)}\\
\pause &=&(A^T+B^T)_{ij} \ \text{ (def. of +)}
\end{eqnarray*}
Since $ij$-entry of LHS and RHS is same for each $(i,j)$, it follows that $(A+B)^T=A^T+B^T$.  
\end{proof}
\end{frame}
\begin{frame}{More sample proofs}
\begin{proof}[Proof that $\left(A^T\right)^{-1}=\left(A^{-1}\right)^T$]
\pause Suppose $A$ is invertible, and let $A^{-1}$ be its inverse. The claim above is that $B=\left(A^{-1}\right)^T$ is the inverse of $A^T$.  To prove this claim we need only show that $BA^T=A^TB=I_n$: 
\pause
\begin{eqnarray*}
BA^T=\left(A^{-1}\right)^TA^T\pause &=&\left(AA^{-1}\right)^T \ \text{ (since $(CD)^T=D^TC^T$)}\\
\pause &=&I_n^T=I_n \ \checkmark
\end{eqnarray*}
\pause
\begin{eqnarray*}
A^TB=A^T\left(A^{-1}\right)^T\pause &=&\left(A^{-1}A\right)^T \ \text{ (since $(CD)^T=D^TC^T$)}\\
\pause &=&I_n^T=I_n \ \checkmark
\end{eqnarray*}

\end{proof}
\end{frame}