\begin{frame}{\ref{s:vectorspace}.\ref{ss:subspace}: executive summary}
\alert{Definitions:} subspace, linear combination, span, polynomial spaces $P$ and $P_n$, $\NS T$, $\range T$. 
\bspace
\alert{Procedures:} proving something is/isn't a subspace; computing the span of a set of vectors, computing $\NS T$ and $\range T$ for a linear transformation. 
\bspace
\alert{Theorems:} intersection of subspaces is a subspace, $\Span(\{\boldv_1,\boldv_2,\dots\})$ is a subspace, $\NS T$ is a subspace, $\range T$ is a subspace.
\end{frame}
\begin{frame}{\ref{s:vectorspace}.\ref{ss:subspace}: subspaces}\footnotesize
\begin{definition}
Let $V$ be a vector space. A subset $W\subseteq V$ is a {\bf subspace} of $V$ if
\bb[(i)]
\ii $\boldzero\in W$,
\ii $\boldw_1, \boldw_2\in W\Rightarrow \boldw_1+\boldw_2\in W$, (i.e., $W$ is {\bf closed under addition}),
\ii $\boldw\in W\Rightarrow k\boldw\in W$ for all $k\in\R$ (i.e., $W$ is {\bf closed under scalar multiplication}). 
\ee 
\end{definition}
\pause
\alert{Comments}\\
(1) You may see slightly different definitions for subspace in the literature: e.g., some define a subspace to be a subset $W\subseteq  V$ which, under the operations of addition and scalar multiplication of $V$, is itself a vector space. I prefer mine as it gives you a hands-on method of deciding whether a given subset of $W\subseteq V$ is in fact a sub{\em space} of $V$: namely, determine whether each of properties (i)-(iii) hold for $W$.
\bpause
(2) Each of properties (ii) and (iii) is stated as an \alert{implication}. Thus when verifying these, as always we assume the antecedent and prove the consequent.     
\end{frame}
\begin{frame}{Example}
\footnotesize
Let $V=\R^2$ and let $W\subseteq V$ be the subset $W=\{(t,t)\in\R^2
\colon t\in\R\}$. We claim $W$ is a subspace. 
\pause \begin{proof}
We must show properties (i)-(iii) hold for $W$. 
\bpause
(i) The zero element of $V$ is $\boldzero=(0,0)$, which is certainly of the form $(t,t)$. Thus $\boldzero\in W$. 
\bpause
(ii) We must prove the implication $\boldw_1, \boldw_2\in W\Rightarrow \boldw_1+\boldw_2\in W$. 
\begin{eqnarray*}
\pause\boldw_1,\boldw_2\in W&\Rightarrow& \boldw_1=(t,t), \boldw_2=(s,s) \text{ for some $t,s\in\R$}\\
\pause&\Rightarrow&\boldw_1+\boldw_2=(t+s,t+s) \\
\pause&\Rightarrow&\boldw_1+\boldw_2\in W.
\end{eqnarray*}
\pause
(iii) We must prove the implication $\boldw\in W\Rightarrow k\boldw\in W$. 
\begin{eqnarray*}
\pause \boldw\in W&\Rightarrow& \boldw=(t,t)\\
\pause&\Rightarrow& k\boldw=(kt,kt)\\
\pause&\Rightarrow& k\boldw\in W
\end{eqnarray*}
\end{proof}
\end{frame}
\begin{frame}{Example}
\footnotesize
Let $V=\R^n$, and treat elements of $V$ as \alert{column vectors}. Fix a vector $\boldv_0\in V$. Let $W\subseteq V$ be the subset $W=\{\boldw\in \R^n\colon \boldv_0^T\boldw=0\}$. We claim $W$ is a subspace. 
\pause \begin{proof}
We must show properties (i)-(iii) hold for $W$. 
\bpause
(i) The zero element of $V$ is $\boldzero=\colvec{0\\ \vdots\\ 0}$. Clearly $\boldv_0^T\boldzero=0$. Thus $\boldzero\in W$. 
\bpause
(ii) We must prove the implication $\boldw_1, \boldw_2\in W\Rightarrow \boldw_1+\boldw_2\in W$. 
\begin{eqnarray*}
\pause\boldw_1,\boldw_2\in W&\Rightarrow& \boldv_0^T\boldw_1=0,\boldv_0^T\boldw_2=0 \\
\pause&\Rightarrow&\boldv_0^T(\boldw_1+\boldw_2)=\boldv_0^T\boldw_1+\boldv_0^T\boldw_2=0+0=0\\
\pause&\Rightarrow& \boldw_1+\boldw_2\in W.
\end{eqnarray*}
\pause
(iii) We must prove the implication $\boldw\in W\Rightarrow k\boldw\in W$. 
\begin{eqnarray*}
\pause\boldw\in W&\Rightarrow& \boldv_0^T\boldw=0\\
\pause&\Rightarrow& \boldv_0^T(k\boldw)=k\boldv_0^T\boldw=k0=0\\
\pause&\Rightarrow& k\boldw\in W
\end{eqnarray*}
\end{proof}
\end{frame}
\begin{frame}{Lines and planes}
Recall that a line in $\R^2$ \alert{containing the origin $(0,0)$} can be expressed as the set of solutions $(x_1,x_2)\in\R^2$ to an equation of the form
\[
ax_1+bx_2=0, \text{ or } \colvec{a\\b}^T\colvec{x_1\\ x_2}=0
\]

Similarly, a plane in $\R^3$ \alert{containing the origin $(0,0,0)$} can be expressed as the the set of solutions $(x_1,x_2,x_3)$ to an equation of the form 
\[
ax_1+bx_2+cx_3=0, \text{ or } \colvec{a\\b\\c}^T\colvec{x_1\\x_2\\x_3}=0.
\]
\pause By the previous example we see that lines in $\R^2$ containing the origin are subspaces, as are planes in $\R^3$ containing the origin! Both can be expressed in the form $W=\left\{\boldx\colon \boldv_0^T\boldx=0\right\}$: take $\boldv_0=(a,b)$ in the first case, and $\boldv_0=(a,b,c)$ in the second. 
\bpause
On the other hand, a line or place that does \alert{not} contain the origin is not a subspace, since it does not contain $\boldzero$. 
\bpause
We will revisit this example after defining the dot product on $\R^n$. 
\end{frame}
%\begin{frame}{Subspaces of $F(-\infty,\infty)$}
%\footnotesize
%In the following you can replace $(-\infty,\infty)$ with $X$ for any subinterval of $\R$. 
%\bspace
%Let $V=F(-\infty,\infty)$, the vector space of all functions on the real line. We can define the following natural subsets of $V$. 
%\[
%\begin{array}{ccl}
%V&=&F(-\infty,\infty)=(\text{set of all functions } f\colon (-\infty,\infty)\rightarrow\R)\\
%\rotatebox{90}{$\subseteq$}& \\
%C(-\infty,\infty)&=&\{f\in V\colon f \text{ continuous}\}\\
%\rotatebox{90}{$\subseteq$}& \\
%C^1(-\infty,\infty)&=&\{f\in V\colon f' \text{ exists and is cont.}\}\\
%\rotatebox{90}{$\subseteq$}& \\
%C^2(-\infty,\infty)&=&\{f\in V\colon f', f'' \text{ exist and are cont.}\}\\
%\rotatebox{90}{$\subseteq$}& \\
%\vdots \\
%C^\infty(-\infty,\infty)&=&\{f\in V \colon f^{(n)} \text{ exists for all $n$}\}
%\end{array}
%\]
%\pause In fact these are all sub{\em spaces} of $V$. Proof: (i) the zero function, $h(x)=0$ for all $x$, is $\infty$-differentiable, and thus is an element of each of these sets; (ii)-(iii) these sets being closed under addition and scalar mult. is a consequence of calculus results about sums and scalar multiples of continuous/differentiable functions.  
%\end{frame}
\begin{frame}{Polynomials}\footnotesize
Recall that a {\bf polynomial} is a function that can be written in the form $f(x)=\anpoly$, where $a_i\in\R$ are fixed constants and $a_n\ne 0$. We call $n$ the {\bf degree} of such a function, denoted $\deg f=n$. 
\bpause
Let $P$ be the set of all polynomial functions, and let $P_n=\{f\colon f(x)=\anpoly, a_i\in\R \}$ be the set of all polynomials of degree \alert{at most} $n$. 

We have the set inclusions $P_n\subset P\subset C^\infty(X)$, where $X$ is any nontrivial interval. (The second inclusion holds since any polynomial is infinitely differentiable on any interval. )
\bpause
It is easy to see that in fact $P_n$ is a \alert{subspace} of $P$, and $P$ is a \alert{subspace} of $C^\infty(X)$. 
Indeed $P_n$ and $P$ both contain the zero polynomial $f(x)=0=0x^n+0x^{n-1}+\cdots 0x+0$, which is the same thing as the zero function $0_X=\boldzero$. Furthermore, sums and scalar multiples of polynomials (resp. of polynomials of at most degree $n$) are polynomials (resp. polynomials of at most degree $n$). 
\bpause
\alert{Important:} a fact we will make use of all the time is that for two polynomials 
\[\begin{array}{l}
f(x)=\anpoly, \ a_n\ne 0 \\
g(x)=\bmpoly, \ b_m\ne 0
\end{array}
\] 
we have $f(x)=g(x)$ if and only if (1) n=m, and (2) $a_i=b_i$ for all $0\leq i\leq n$. 
\end{frame}
\begin{frame}{Intersections and unions}
\begin{theorem}
Let $V$ be a vector space, and suppose $W_1,W_2,\dots, W_r$ are each subspaces of $V$. Then the intersection $W=W_1\cap W_2\cdots \cap W_r$ is a subspace of $V$. 
\end{theorem}
\pause
\alert{Comment}. Thus intersections of subspaces are subspaces. The same is not true of \alert{unions}. 
\bpause
For example, take $V=\R^2$, $W_1=\{(t,t)\colon t\in\R\}$ and $W_2=\{(t,-t)\colon t\in\R\}$. Then each $W_i$ is a subspace, but their union $W_1\cup W_2$ is not. Why?
\bpause
We have $\boldw_1=(1,1)\in W_1\subset W_1\cup W_2$ and $\boldw_2=(1,-1)\in W_2\subset W_1\cup W_2$, but $\boldw_1+\boldw_2=(2,0)\notin W_1\cup W_2$.  
\bpause
Note that $W_1\cup W_2$ is in fact closed under scalar multiplication. 
\end{frame}
\subsubsection*{Span}
\begin{frame}{Linear combinations and span}
Recall that a \alert{linear combination} in a vector space $V$ is a vector of the form 
\[
\boldv=k_1\boldv_1+k_2\boldv_2\cdots +k_r\boldv_r,
\] 
where $k_i\in \R$ are scalars. 
\bpause We use this notion to define the \alert{span} of a set of vectors. 
\begin{definition}
Let $V$ be a vector space, and let $S=\{\boldv_1,\boldv_2,\dots,\boldv_r\}$ be a set of vectors of $V$. The {\bf span} of $S$ is the set 
\begin{eqnarray*}
\Span(\{\boldv_1,\boldv_2,\dots,\boldv_r\})&:=&\left(\begin{array}{c} \text{the set of all linear } \\ \text{combinations of the $\boldv_i$}\end{array}\right)\\
&=&\{\boldv\in V\colon \boldv=k_1\boldv_1+k_2\boldv_2\cdots +k_r\boldv_r, \text{ for some } k_i\in\R \}.
\end{eqnarray*}
\end{definition}
\end{frame}
\begin{frame}
\begin{theorem}
Let $V$ be a vector space,  $S=\{\boldv_1,\boldv_2,\dots,\boldv_r\}$ a set of vectors of $V$, and $W=\Span(S)$. Then 
\bb[(i)]
\ii $W$ is a subspace of $V$;
\ii if $W'$ is a subspace containing all the vectors $\boldv_i$, then $W\subseteq W'$.
\ee
We paraphrase (ii) by saying $W=\Span(S)$ is the ``smallest" subspace of $V$ containing all the $\boldv_i$. 
\end{theorem}
\pause
\alert{More terminology:} in the spirit of the last theorem, given a set of vectors $S=\{\boldv_1,\dots, \boldv_r\}$, we call $W=\Span(S)$ the subspace of $V$ {\bf generated by} the vectors $\boldv_i$. Similarly, given a subspace $W$, a set $S=\{\boldv_1,\dots, \boldv_r\}$ for which $W=\Span(S)$ is called a {\bf spanning set} of $W$. 
\end{frame}
\begin{frame}{Examples}
\alert{$M_{mn}$}. Define $E_{ij}$ to be the matrix whose $(i,j)$-th entry is 1, and whose every other entry is 0. Then the set 
\[
\{ E_{ij}\colon 1\leq i\leq m, 1\leq j\leq n\}
\]
is a spanning set for $M_{mn}$. 
\bpause
\alert{$\R^n$}. In a similar vein, define $\bolde_i$ to be the $n$-tuple whose $i$-th entry is 1, and whose every other entry is 0.

Then $\{\bolde_1, \bolde_2, \dots, \bolde_n\}$ is a spanning set for $\R^n$. 
\bpause 
\alert{$P$ and $P_n$}. The set $\{1, x, x^2, \dots, \}$ is a spanning set for $P$. The set $\{1, x, x^2, \dots, x^n\}$ is a spanning set for $P_n$. 
\bpause
\alert{$\R^\infty$}. As above we can define $\bolde_i\in \R^\infty$ to be the infinite sequence whose $i$-th entry is 1, and whose every other entry is $0$. 

Note, however, that the set $\{\bolde_1, \bolde_2, \bolde_3, \dots\}$ is \alert{not} a spanning set for $\R^\infty$.
\bpause
Indeed, the sequence $(1,1,1,\dots)$ is not a (finite!) linear combination of the $\bolde_i$. 
\bpause
\alert{$\R_{>0}$} Any $a\ne 1\in \R_{>0}$ forms a spanning set for $\R_{>0}$ as a vector space. This is because scalar multiplication by $r$ in $\R_{>0}$ is defined as exponentiation. Thus $\Span(\{a\})=\{a^r\colon r\in\R\}=\R_{>0}$. The last equality holds since the exponential function $f(x)=a^x$ has range all positive reals for any base $a\ne 1$. 
\end{frame}
\begin{frame}{Example}
Let $V=P_2$ and let $S=\{p_1, p_2\}$, where $p_1(x)=x^2-1$ and $p_2(x)=x^2-x$. Show that $W=\Span(S)$ is the subspace of all polynomials $p(x)=a_2x^2+a_1x+a_0$ for which $p(1)=0$. That is:
\[
\Span(S)=\{p(x)\in P_2\colon p(1)=0\}
\]
\pause
\begin{proof}
We wish to prove a \alert{set equality}. We do so by showing the $\subseteq$ and $\supseteq$ relations separately.  (See my proof technique guide!)
\bpause 
\alert{$\subseteq$}. Note first that $p_1(1)=p_2(1)=0$. Given an element $q(x)\in \Span\left(\{p_1(x), p_2(x)\}\right)$, we have $q(x)=ap_1(x)+bp_2(x)$ for some $a, b\in \R$. But then $q(1)=ap_1(1)+bp_2(1)=0+0=0$. Thus $q(x)\in \{p(x)\colon p(1)=0\}$. 
\bpause
\alert{$\supseteq$}. Now take $p(x)=a_2x^2+a_1x+a_0\in \{p(x)\in P_2\colon p(1)=0\}$. We must find $a, b\in \R$ such that $p(x)=ap_1+bp_2$. 
\bpause Since $p(1)=0$, we have $a_2+a_1+a_0=0$. I claim $p(x)=(-a_0)p_1+(-a_1)p_2$. Indeed we have 
\[
-a_0p_1-a_1p_2=(-a_0-a_1)x^2+a_1x+a_0=a_2x^2+a_1x+a_0,
\]
since $a_2+a_1+a_0=0$. This shows that $p(x)\in\Span\left(\{p_1, p_2\}\right)$, as desired. 
\end{proof}
\end{frame}
\begin{frame}{Example}
Let $V=P_2$ and let $S=\{p_1, p_2, p_3\}$ where 
\[
p_1(x)=x^2+x+1, p_2(x)=x^2+x, p_3(x)=x^2+1.
\]
Show that $\Span(S)=P_2$. 
\pause
\begin{proof}
Again, we are tasked with showing a \alert{set equality}. 
\bspace
It is clear that $\Span(S)=\{rp_1+sp_2+tp_3\colon r,s,t\in\R\}\subseteq P_2$. 
\bspace
The harder direction is showing $P_2\subseteq \Span(S)$: i.e., given {\em any} $p(x)=a_2x^2+a_1x+a_0$ we must show there are $r,s,t\in\R$ such that $p(x)=rp_1+sp_2+tp_3$. 
\bpause 
We do so by setting up a system of equations. Combining like terms and equating coefficients in the polynomial expression $p(x)=rp_1+sp_2+tp_3$ yields the linear system 
\[
\begin{linsys}{3}
r&+&s&+&t&=&a_2\\
r&+&s&&&=&a_1\\
r&&&+&t&=&a_0
\end{linsys}
\]
\pause GE shows that the system has a solution for {\em any} choice of $a_2, a_1, a_0$: namely, $r=-a_2+a_1+a_0$, $s=a_2-a_0$, $t=a_2-a_1$. Thus given any $p(x)=a_2x^2+a_1x+a_0$, we can find $r,s,t$ such that $p=rp_1+sp_2+tp_3$, showing $P_2\subseteq \Span(S)$. 
\end{proof}
\end{frame}
\subsubsection*{Null space}

\begin{frame}{Null space of a linear transformation}
The notion of span is a useful tool for constructing subspaces inside a space $V$: given any collection $\{\boldv_1, \boldv_2, \dots, \boldv_r\}$, we now know that the set  $W=\Span(\{\boldv_1,\boldv_2,\dots, \boldv_r\})$ is guaranteed to be a subspace of $V$. 
\bpause 
The notion of a \alert{null space} of a linear transformation $T\colon V\rightarrow W$ provides another useful tool for constructing subspaces. 
\begin{definition}
The {\bf null space} of a linear transformation $T\colon V\rightarrow W$ is the set 
\[
\NS T\colon =\{\boldv\colon T(\boldv)=\boldzero_W \}.
\]
\pause 
In the special case where $T\colon \R^n\rightarrow \R^m$ and $T=T_A$, we may write $\NS A$ for $\NS T_A$: by definition this is the set 
\[
\NS A=\NS T_A=\{\boldx\in \R^n\colon A\boldx=\underset{m\times 1}{\boldzero}\}.
\]
In other words, $\NS A$ is the set of solutions to the homogenous matrix equation $A\boldx=\boldzero$, or equivalently, its associated homogenous system of equations. 
\end{definition}
\pause
As we see in the next theorem, given linear $T\colon V\rightarrow W$, its null space $\NS T$ is always a \alert{subspace} !!
\end{frame}
\begin{frame}
 \begin{theorem}[Null space theorem]
 Let $T\colon V\rightarrow W$. Then $\NS T$ is a subspace of $V$. 
 \end{theorem}
 \pause
 \begin{proof}
 \ 
 \bb[(i)]
 \ii Since $T(\boldzero_V)=\boldzero_W$, we see that $\boldzero_V\in \NS T$. 
 \pause
 \ii Suppose $\boldv_1, \boldv_2\in \NS T$. We have 
 \begin{align*}
 T(\boldv_1+\boldv_2)&=T(\boldv_1)+T(\boldv_2) &\text{($T$ is linear)}\\
 &=\boldzero_W+\boldzero_W &\text{(since $\boldv_1,\boldv_2\in\NS T$)}\\
 &=\boldzero_W.
 \end{align*}
 Thus $\boldv_1+\boldv_2\in \NS T$, proving the implication $\boldv_1,\boldv_2\in\NS T\Rightarrow \boldv_1+\boldv_2\in\NS T$. 
 \pause
 \ii Suppose $\boldv\in\NS T$. We have 
 \begin{align*}
 T(c\boldv)&=cT(\boldv) &\text{($T$ is linear)}\\
 &=c\boldzero_W &\text{(since $\boldv\in\NS T$)}\\
 &=\boldzero_W.
 \end{align*}
 This shows $c\boldv\in\NS T$, proving the implication $\boldv\in\NS T\Rightarrow c\boldv\in\NS T$. 
 \ee
  \pause Since $\NS T$ satisfies the conditions (i)-(iii), we see that it is a subspace of $V$. 

 \end{proof}
\end{frame}
\begin{frame}{Examples}
The theorem gives us a clever, indirect way of proving a subset $V'\subseteq V$ is a subspace: namely, find a linear transformation $T\colon V\rightarrow W$ for which $V'=\NS T$ !! 
\bpause
\begin{example}
The set $W\subseteq\R^3$ of all vectors $(x,y,z)$ satisfying $x+2y+3z=x-y-z=0$ is a subspace of $\R^3$. 

Indeed we have $W=\NS A$ where $A=\begin{bmatrix}
1&2&3\\
1&-1&-1
\end{bmatrix}
$.
\end{example} 
\pause
\begin{example}
The set $W=\{A\in M_{nn}\colon A^T=A\}$, consisting of all symmetric $n\times n$ matrices, is a subspace of $M_{nn}$. 

Indeed, we have $W=\NS T$, where $T\colon M_{nn}\rightarrow M_{nn}$ is the linear transformation defined as $T(A)=A^T-A$. 

(I leave it to you to show $T$ is linear.) 
\end{example}
\pause
\begin{example}
The set $W$ of all infinitely differentiable functions $f$ satisfying the differential equation $f''(x)+xf'(x)=3f(x)$ is a subspace of $C^\infty(\R)$. 

Indeed $W$ is $\NS T$, where $T\colon C^\infty(\R)\rightarrow C^\infty(\R)$ is the linear transformation defined as $T(f)=f''+xf'-3f$.

(I leave it to you to show $T$ is linear.) 

\end{example}
\end{frame}
\subsubsection*{Range of a linear transformation}
\begin{frame}{Range}
\begin{definition}
In general given a function $f\colon A\rightarrow B$ with domain $A$ and codomain $B$, its {\bf range} is the set 
\[
\range f=\{b\in B\colon b=f(a) \text{ for some $a\in A$}\}=f(A),
\]
where the last equality makes use of our ``image of a set" notation $f(A)$. 
\end{definition}
\pause
We will use the same notation for a linear transformation $T\colon V\rightarrow W$. The range of $T$ is a subset of the codomain $W$. Not surprisingly, it is in fact a \alert{subspace} of $W$. 
\end{frame}
\begin{frame}{Range}
\begin{theorem}[Range is a subspace]
Let $T\colon V\rightarrow W$ be a linear transformation. Then $\range T$ is a subspace of $W$. 
\end{theorem}
\pause
\begin{proof}
\bb[(i)]
\pause\ii We must show $\boldzero_W\in \range(T)$. But $\boldzero_W=T(\boldzero_V)$. Thus $\boldzero_W\in\range(T)$. 
\pause\ii Suppose $\boldw_1,\boldw_2\in \range(T)$. This means there are $\boldv_1, \boldv_2\in V$ such that $T(\boldv_i)=\boldw_i$ for $i=1,2$. We must show $\boldw=\boldw_1+\boldw_2\in\range(T)$.  
\bpause 
Set $\boldv=\boldv_1+\boldv_2$. Then
\begin{eqnarray*}
T(\boldv)&=&T(\boldv_1+\boldv_2)\\
&=&T(\boldv_1)+T(\boldv_2) \ \text{ (since $T$ is a linear transformation)}\\
&=&\boldw_1+\boldw_2=\boldw.
\end{eqnarray*} 
Since we have provided a $\boldv$ with $T(\boldv)=\boldw_1+\boldw_2$, we see that $\boldw_1+\boldw_2\in\range(T)$. 
\pause\ii Suppose $\boldw\in\range(T)$. Then there is a $\boldv\in V$ with $T(\boldv)=\boldw$. Then $T(k\boldv)=kT(\boldv)=k\boldw$, showing $k\boldw\in\range(T)$.  
\ee

\end{proof}
 
\end{frame}
\begin{frame}{Example}
Let $T=T_A\colon \R^2\rightarrow\R^3$, where $A=\begin{bmatrix}
1&1\\
2&1\\
3&5
\end{bmatrix}$. According to the theorem, $\range T_A$ is a subspace of $\R^3$. Can we identify this subspace as a familiar geometric object?
\bpause
By definition $\range T_A$ is the set 
\[
\{\boldy\in\R^3\colon \boldy=T_A(\boldx) \text{ for some $\boldx\in \R^3$}\}=\left\{\boldy=\begin{bmatrix}
a\\ b\\ c
\end{bmatrix}\colon \boldy=A\boldx \text{ for some $\boldx\in\R^2$}\right\}.\]
\pause 
Thus to compute $\range T_A$ we must determine which choice of $\boldy=(a,b,c)$ makes the system $A\boldx=\boldy$ consistent.
We answer this using our good, old friend Gaussian elimination! 
\pause
\[
\begin{bmatrix}[rr|r]
1&1&a\\
2&1&b\\
3&5&c
\end{bmatrix}
\xrightarrow{\text{row reduction}}\begin{bmatrix}[rr|r]
1&1&a\\
0&1&2a-b\\
0&0&-7a+2b+c
\end{bmatrix} \]
\pause
To be consistent we need $-7a+2b+c=0$. We conclude that $\range T$ is the set of all $(a,b,c)$ satisfying $-7a+2b+c=0$. Geometrically this is the plane passing through $O$ with normal vector $\boldn=(-7,2,1)$. 
\end{frame}
\begin{frame}{Example}
Consider again the linear transformation $T\colon M_{nn}\rightarrow M_{nn}$, $T(A)=A^T-A$. We saw that $\NS T$ was the space of symmetric matrices. The theorem above tells us that $\range T$ is also a subspace of $M_{nn}$. What is it?
\bpause
Take $B\in \range T$. By definition this means $B=T(A)=A^T-A$ for some $A$. So one, somewhat unsatisfying way of describing $\range T$ is as the set of all matrices of the form $A^T-A$. 
\bpause
Let's investigate further. Notice that if $B=A^T-A$, then $B^T=(A^T-A)^T=(A^T)^T-A^T=A-A^T=-B$. Thus every element $B\in \range T$ satisfies $B^T=-B$. Such matrices are called {\bf skew-symmetric}. 
\bpause I claim further that in fact $\range T$ is the the set of \alert{all} skew-symmetric matrices. To prove this, I need to show that given a skew-symmetric matrix $B$, there is a matrix $A$ such that $T(A)=B$. 
\bpause Suppose I have a $B$ such that $B^T=-B$. Let $A=-\frac{1}{2}B$. Then
\[
T(A)=T(-\frac{1}{2}B)=-\frac{1}{2}(B^T-B)=-\frac{1}{2}(-B-B)=B.
\] 
This shows that $B\in \range T$, and concludes the proof that $\range T$ is the set of all skew-symmetric matrices. 
 
\end{frame}
