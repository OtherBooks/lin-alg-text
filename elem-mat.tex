\begin{frame}{\ref{s:linear systems}:\ref{ss:elementary} executive summary}\footnotesize
\alert{Definitions}:elementary matrices
\bspace
\alert{Procedures}: finding $A^{-1}$ using row reduction, writing $A$ as a product of elementary matrices.
\bspace
\alert{Theorems}: elementary matrices are invertible, and their inverses are themselves elementary; the invertibility theorem.\end{frame}
\begin{frame}{\ref{s:linear systems}:\ref{ss:elementary}: computing inverses with row reduction}
Surprisingly row reduction provides a tool for both deciding whether a matrix is invertible, and computing this inverse if it exists. 
\bpause 
First we will construe row operations as certain matrix multiplications. 
\begin{definition}
A square matrix $E_{m\times m}$ is an {\bf elementary matrix} if multiplying any matrix $A_{m\times n}$ on the left by $E$ performs one of our row operations on $A$. 
\end{definition}
\pause
\begin{comment}
Equivalently, a matrix $E_{m\times m}$ is elementary if it is the result of applying a single row operation on $I_m$. 
\end{comment}
\end{frame}
\begin{frame}
The \alert{row method} of multiplication is the key to matching up a row operation with a particular elementary matrix. 

Suppose the multiplication 
\[
\underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
\]
performs a single row operation on $A$. What must $E$ be? In what follows let $\boldr_i$ be the $i$-th row of $A$ for $1\leq i\leq m$. 
\bspace
\alert{Scale.} Suppose $E$ replaces $\boldr_i$ of $A$ with $c\cdot\boldr_i$ for $c\ne 0$.  Then $E$ is the identity matrix except for the $i$-th row, where the 1 is replaced with $c$.  
\[
\underset{cr_i}{E}=\begin{bmatrix}
1&0&0&\cdots& 0&0\\
0&1&0&\cdots&0\\
\vdots& &\vdots & & \vdots \\
0&\cdots&0&c&0&\cdots\\
\vdots& &\vdots & & \vdots \\
0&0&\cdots&0&0&1
\end{bmatrix}
\]
\end{frame}
\begin{frame}
The \alert{row method} of multiplication is the key to matching up a row operation with a particular elementary matrix. 

Suppose the multiplication 
\[
\underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
\]
performs a single row operation on $A$. What must $E$ be? In what follows let $\boldr_i$ be the $i$-th row of $A$ for $1\leq i\leq m$. 
\bspace
\alert{Swap.} Suppose $E$ swaps rows $i$ and $j$ of $A$. Then $E$ is the identity matrix with the $i$-th row and $j$-th rows swapped. 
\[
\underset{r_i\leftrightarrow r_j}{E}=\begin{bmatrix}
1&0&0&\cdots& 0&0\\
\vdots& &\vdots & & \vdots \\
0&0&0&0&1&\cdots\\
\vdots& &\vdots & & \vdots \\
0&\cdots&1&0&\cdots&0\\
\vdots& &\vdots & & \vdots \\
0&0&\cdots&0&0&1
\end{bmatrix}
\]
\end{frame}
\begin{frame}
The \alert{row method} of multiplication is the key to matching up a row operation with a particular elementary matrix. 

Suppose the multiplication 
\[
\underset{m\times m}{E}\cdot\underset{m\times n}{A}=\underset{m\times n}{A'}
\]
performs a single row operation on $A$. What must $E$ be? In what follows let $\boldr_i$ be the $i$-th row of $A$ for $1\leq i\leq m$. 
\bspace
\alert{Row addition.} Suppose $E$ replaces $\boldr_i$ of $A$ with $\boldr_i+c\boldr_j$. Then $E$ is the identity matrix except for its $i$-th row. 
\[
\underset{r_i+cr_j}{E}=\begin{bmatrix}
1&0&0&\cdots& 0&0\\
\vdots& &\vdots & & \vdots \\
\cdots&c&\cdots&1&\cdots&0\\
\vdots& &\vdots & & \vdots \\
0&0&\cdots&0&0&1
\end{bmatrix}
\]
  
\end{frame}
\begin{frame}{Row reduction as matrix multiplication}
\footnotesize
Suppose we perform a sequence of row operations on a matrix $A$. Denote the $i$-th row operation $\rho_i$, and denote by $\rho_i(B)$ the result of applying $\rho_i$ to a matrix $B$. 
\bpause 
Our sequence of operations $\rho_i$ would produce the following sequence of matrices:
\[
\begin{array}{c}
A\\
\pause\rho_1(A)\\
\pause\rho_2(\rho_1(A))\\
\pause \vdots \\
\rho_r(\rho_{r-1}(\dots \rho_2(\rho_1(A))))
\end{array}
\]
\bpause Let $\rho_i$ corresponds to the elementary matrix $E_i$. Then we represent this same sequence using matrix multiplication:
\[
\begin{array}{c}
A\\
\pause E_1A\\
\pause E_2E_1A\\
\pause \vdots \\
E_rE_{r-1}\cdots E_2E_1A
\end{array}
\]
\end{frame}
\begin{frame}{Elementary matrices are invertible}
We will use our row operation notation to denote the different types of elementary matrices:
\[
\underset{cr_i}{E}, \underset{r_i\leftrightarrow r_j}{E}, \underset{r_i+cr_j}E.
\] 
\pause
\begin{theorem}[Elementary matrix theorem]
Fix $n$. All elementary matrices are invertible, and their inverses are elementary matrices. In fact:
\begin{eqnarray*}
\underset{cr_i}{E}^{-1}&=&\underset{\frac{1}{c}r_i}{E}\\
\underset{r_i\leftrightarrow r_j}{E}^{-1}&=&\underset{r_i\leftrightarrow r_j}{E}\\
\underset{r_i+cr_j}E^{-1}&=&\underset{r_i-cr_j}E
\end{eqnarray*}
\end{theorem}
\pause
\begin{proof}
These are formulas that you can easily check for each type of elementary matrix directly. 
\end{proof}
\end{frame}
\begin{frame}{Examples}
Fix $n=3$. Verify that the pairs below are indeed inverses. 
\bspace
We have $\underset{r_1+3r_3}{E}=\begin{bmatrix}[rrr] 1&0&3\\0&1&0\\ 0&0&1 \end{bmatrix}$ and thus 
$\left(\underset{r_1+3r_3}{E}\right)^{-1}=\begin{bmatrix}[rrr] 1&0&-3\\0&1&0\\ 0&0&1 \end{bmatrix}$
\bspace
We have $\underset{2r_3}{E}=\begin{bmatrix}[rrr] 1&0&0\\0&1&0\\ 0&0&2 \end{bmatrix}$ and thus 
$\left(\underset{2r_3}{E}\right)^{-1}=\begin{bmatrix}[rrr] 1&0&0\\0&1&0\\ 0&0&\frac{1}{2} \end{bmatrix}$ \bspace
We have $\underset{r_1\leftrightarrow r_3}{E}=\begin{bmatrix}[rrr] 0&0&1\\0&1&0\\ 1&0&0 \end{bmatrix}$ and thus 
$\left(\underset{r_1\leftrightarrow r_3}{E}\right)^{-1}=\begin{bmatrix}[rrr] 0&0&1\\0&1&0\\ 1&0&0 \end{bmatrix}$ 
\end{frame}
\begin{frame}{Interlude on matrix equations}
We take a moment to make the following simple, somewhat overdo observation. Namely, we can represent a \alert{system of linear equations} 
\[
\eqsys\tag{$*$}
\]
as a \alert{single matrix equation}
\[
\genmatrix\begin{bmatrix}
x_1\\x_2\\ \vdots \\ x_n
\end{bmatrix}
=\begin{bmatrix}
b_1\\ b_2 \\ \vdots \\ b_m
\end{bmatrix},\tag{$**$} 
\]
or $A\boldx=\boldb$,where $A=[a_{ij}]$, $\boldx=[x_i]$, $\boldb=[b_i]$.
\bpause 
Indeed if you expand out the left-hand side of $(**)$ into an $m\times 1$ column vector, using the definition of matrix multiplication,  and then invoke the definition of matrix equality, then you obtain the linear system $(*)$. 
\bpause 
By the same token, an $n$-tuple $(s_1,s_2,\dots, s_n)$ is a solution to the \alert{system of equations} $(*)$ if and only if its corresponding column vector $\underset{n\times 1}{\bolds}=[s_i]$ is a solution to the \alert{matrix equation} $A\boldx=\boldb$.  
\end{frame}
\begin{frame}{Interlude on matrix equations}
In particular, a homogeneous linear system 
\[
\homsys
\]
can be represented as the single matrix equation 
\[
A\boldx=\underset{m\times 1}{\boldzero},
\]
where $A=[a_{ij}]$ and $\boldx=[x_i]$; and $(s_1,s_2,\dots, s_n)$ is a solution to the \alert{homogenous system} if and only if its corresponding column vector $\underset{n\times 1}{\bolds}=[s_i]$ is a solution to the \alert{matrix equation} $A\boldx=\underset{m\times 1}{\boldzero}$. 
\end{frame}

\begin{frame}
We are now ready to state a major theorem about invertibility. 
\begin{theorem}[Invertibility theorem]
Let $A=[a_{ij}]$ be a square $n\times n$ matrix. The following statements are equivalent.
\bb[(a)]
\ii $A$ is invertible.
\pause\ii $A\boldx=\boldzero$ has a unique solution (the trivial one). 
\pause\ii $A$ is row equivalent to $I_n$, the $n\times n$ identity matrix.
\pause\ii $A$ is a product of elementary matrices. 	
\ee
\end{theorem}
\pause\begin{proof}
\footnotesize
Recall that two show two statements $P$ and $Q$ are equivalent, we must show two implications: $P\Rightarrow Q$, and $Q\Rightarrow P$. 
\bpause We must show this for each pair of statements above. That would be 6 different pairs and a total of 12 different implications to show! 
\bspace We ease our work load by showing the following \alert{cycle} of implications:
\[
(a)\Rightarrow(b)\Rightarrow(c)\Rightarrow(d)\Rightarrow(a)
\]
and using the fact that implication is transitive! 
\bspace 
We will complete the proof in class. 
\end{proof}
\end{frame}
\begin{frame}{Algorithm for inverses}
The proof of the invertibility theorem (IT) provides an algorithm for (1) deciding whether $A$ is invertible, and (2) computing $A^{-1}$ if it exists. 
\bpause
The algorithm:
\bb
\ii Row reduce $A$ to row echelon form $U$ (keeping track of the elementary matrices you use). The theorem tells us that $A$ is invertible if and only if $U$ has $n$ leading 1's. 
\pause\ii If this is so, we can keep row reducing to the identity matrix. In terms of matrices we have 
\[
E_rE_{r-1}\cdots E_1A=I_n.
\] 
This means $A^{-1}=E_rE_{r-1}\cdots E_1$.
\pause\ii As an added bonus we also have $A=E_1^{-1}E_2^{-1}\cdots E_{r}^{-1}$, expressing $A$ as a product of elementary matrices. 
\ee
\end{frame}
\begin{frame}{Inverses with augmented matrix}
\footnotesize
It can be bothersome to keep track of those $E_i$, and the order of their multiplication. Instead, we use an augmented matrix method, beginning with the augmented matrix 
\[
\begin{bmatrix}[c|c]
A&I_n\end{bmatrix},
\]
and simply applying row reductions until $A$ on the left becomes $I_n$, at which point $I_n$ on the right has become $A^{-1}$. 

\pause Why does this work? In terms of the $E_i$, we have the sequence:
\[
\begin{array}{c}
\begin{bmatrix}[c|c]
A&I_n\end{bmatrix}
\\
\pause\begin{bmatrix}[c|c]
E_1A&E_1\end{bmatrix}
\\
\pause\begin{bmatrix}[c|c]
E_2E_1A&E_2E_1\end{bmatrix}
\\
\pause\vdots\\
\begin{bmatrix}[c|c]
E_rE_{r-1}\cdots E_1A&E_rE_{r-1}\cdots E_1\end{bmatrix}\pause=\begin{bmatrix}[c|c]I_n&A^{-1}\end{bmatrix}
\end{array}
\]

\pause Thus when we are done, the RHS of our augmented matrix magically becomes $A^{-1}$.
\end{frame}
\begin{frame}\scriptsize
\alert{Example:} take $A=\begin{bmatrix}[rrr]
1&2&0\\
1&2&1\\
-1&-1&3
\end{bmatrix}. $
\begin{eqnarray*}
\begin{bmatrix}[rrr|rrr]
1&2&0&1&0&0\\
1&2&1&0&1&0\\
-1&-1&3&0&0&1
\end{bmatrix}
\pause&\xrightarrow{r_2-r_1}&\begin{bmatrix}[rrr|rrr]
1&2&0&1&0&0\\
0&0&1&-1&1&0\\
-1&-1&3&0&0&1
\end{bmatrix} \\
\pause&\xrightarrow{r_3+r_1}&\begin{bmatrix}[rrr|rrr]
1&2&0&1&0&0\\
0&0&1&-1&1&0\\
0&1&3&1&0&1
\end{bmatrix} \\
\pause\alert{\text{(3 leading 1's, thus invertible!)}}&\xrightarrow{r_2\leftrightarrow r_3}&\begin{bmatrix}[rrr|rrr]
1&2&0&1&0&0\\
0&1&3&1&0&1\\
0&0&1&-1&1&0
\end{bmatrix} \\
\pause&\xrightarrow{r_2-3r_3}&\begin{bmatrix}[rrr|rrr]
1&2&0&1&0&0\\
0&1&0&4&-3&1\\
0&0&1&-1&1&0
\end{bmatrix} \\
\pause&\xrightarrow{r_1-2r_2}&\begin{bmatrix}[rrr|rrr]
1&0&0&-7&6&-2\\
0&1&0&4&-3&1\\
0&0&1&-1&1&0
\end{bmatrix}
\end{eqnarray*}
\pause We conclude that $A^{-1}=\begin{bmatrix}[rrr]
-7&6&-2\\
4&-3&1\\
-1&1&0
\end{bmatrix}$. On the next slide I show how $A^{-1}$ and $A$ can be written as products of elementary matrices. 
\end{frame}
\begin{frame}
\alert{Example (contd):} in terms of elementary matrices we have 
\[
E_5E_4E_3E_2E_1A=I,
\]
where 
\begin{align*}\scriptsize
E_1&=\begin{bmatrix}[rrr] 1&0&0\\-1&1&0\\ 0&0&1\end{bmatrix}  &  E_2&=\begin{bmatrix}[rrr]1&0&0\\ 0&1&0\\ 1&0&1\end{bmatrix}\\
E_3&=\begin{bmatrix}[rrr]1&0&0\\0&0&1\\0&1&0
\end{bmatrix} &  E_4&=\begin{bmatrix}[rrr]
1&0&0\\ 0&1&-3\\ 0&0&1\end{bmatrix}\\
E_5&=\begin{bmatrix}[rrr]1&-2&0\\0&1&0\\ 0&0&1\end{bmatrix}
\end{align*}
\pause 
Thus we have $A^{-1}=E_5E_4E_3E_2E_1$ and $A=E_1^{-1}E_2^{-1}E_3^{-1}E_4^{-1}E_5^{-1}$.
\bspace
This shows how both of these matrices can be written as products of elementary matrices (recall that each $E_i^{-1}$ is also elementary). 
\end{frame}